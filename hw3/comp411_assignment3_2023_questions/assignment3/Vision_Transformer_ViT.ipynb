{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "dRs1KgnbCZDm"
      },
      "source": [
        "# Attentional Networks in Computer Vision\n",
        "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
        "\n",
        "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
        "\n",
        "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
      ],
      "id": "dRs1KgnbCZDm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xapNI_YECZDp"
      },
      "source": [
        "# Part I. Preparation\n",
        "\n",
        "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
        "\n",
        "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
      ],
      "id": "xapNI_YECZDp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhEvHXZbCZDp"
      },
      "outputs": [],
      "source": [],
      "id": "JhEvHXZbCZDp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore"
        ],
        "id": "xPqPxFlWCZDq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np"
      ],
      "id": "xPqPxFlWCZDq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hP96ZH9CZDq"
      },
      "outputs": [],
      "source": [
        "PYTORCH_ENABLE_MPS_FALLBACK=1"
      ],
      "id": "0hP96ZH9CZDq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore"
        ],
        "id": "omLkkS43CZDr"
      },
      "outputs": [],
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "# The torchvision.transforms package provides tools for preprocessing data\n",
        "# and for performing data augmentation; here we set up a transform to\n",
        "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
        "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
        "transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
        "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
        "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
        "# training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
        "                             transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64,\n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64,\n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=False,\n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ],
      "id": "omLkkS43CZDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "1n6_A2_ACZDr"
      },
      "source": [
        "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
        "\n",
        "The global variables `dtype` and `device` will control the data types throughout this assignment."
      ],
      "id": "1n6_A2_ACZDr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "pma2QfPXCZDr",
        "outputId": "b26ddf48-f07d-4e48-c6e5-a5db5814137d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ],
      "id": "pma2QfPXCZDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_sG8YlKCZDs"
      },
      "source": [
        "# Part II. Barebones Transformers: Self-Attentional Layer\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
        "\n",
        "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
        "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
        "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
        "\n",
        "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
        "\n",
        "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
        "\n",
        "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
        "\n",
        "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
        "\n",
        "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
        "\n",
        "5. Reassemble heads into one flat vector and return the output.\n",
        "\n",
        "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf"
      ],
      "id": "F_sG8YlKCZDs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfB4LB_MCZDs"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        ## initialize module's instance variables\n",
        "        self.input_dims = input_dims\n",
        "        self.head_dims = head_dims\n",
        "        self.num_heads = num_heads\n",
        "        self.proj_dims = head_dims * num_heads\n",
        "\n",
        "        ## Declare module's parameters\n",
        "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Input of shape, [B, N, D] where:\n",
        "        ## - B denotes the batch size\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token\n",
        "        ## - D corresponds to model dimensionality\n",
        "        b,n,d = x.shape\n",
        "\n",
        "        ## Construct queries,keys,values\n",
        "        q_ = self.W_Q(x)\n",
        "        k_ = self.W_K(x)\n",
        "        v_ = self.W_V(x)\n",
        "\n",
        "        ## Seperate q,k,v into their corresponding heads,\n",
        "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
        "        ## - B denotes the batch size\n",
        "        ## - H denotes number of heads\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token\n",
        "        ## - D//H corresponds to per head dimensionality\n",
        "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
        "\n",
        "        #########################################################################################\n",
        "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
        "        #########################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        ## Compute attention logits. Note that this operation is conducted as a\n",
        "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
        "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
        "        ## Output Attention logits should have the size: [B,H,N,N]\n",
        "#         print(f\"input shape: {q.shape}\")\n",
        "#         print(f\"output shape:{b} {q.shape[1]} {n} {n}\")\n",
        "#         print(f\"{k.transpose(-2, -1).shape}\")\n",
        "        attention_logits = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dims ** 0.5)\n",
        "\n",
        "        ## Compute attention Weights. Note that this operation is conducted as a\n",
        "        ## Softmax Normalization across the keys dimension.\n",
        "        ## Hint: You can apply the Softmax operation across the final dimension\n",
        "\n",
        "        attention_weights = F.softmax(attention_logits, dim=-1)\n",
        "\n",
        "\n",
        "        ## Compute output values. Note that this operation is conducted as a\n",
        "        ## batched matrix multiplication between the Attention Weights matrix and\n",
        "        ## the values tensor. After computing output values, the output should be reshaped\n",
        "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
        "        ## Output should be of size [B, N, D]\n",
        "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
        "\n",
        "        attn_out = torch.matmul(attention_weights, v).permute(0,2,1,3).reshape(b,n,self.proj_dims)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE\n",
        "        ################################################################################\n",
        "\n",
        "        return attn_out"
      ],
      "id": "HfB4LB_MCZDs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLmnbAizCZDt"
      },
      "source": [
        "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ],
      "id": "MLmnbAizCZDt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "MrCWRlolCZDt",
        "outputId": "06decf04-746c-48a4-acb5-67cb5dd47372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 16, 256])\n"
          ]
        }
      ],
      "source": [
        "def test_self_attn_layer():\n",
        "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
        "    layer = SelfAttention(32,64,4)\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,256]\n",
        "test_self_attn_layer()"
      ],
      "id": "MrCWRlolCZDt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guzBvvy6CZDt"
      },
      "source": [
        "# Part III. Barebones Transformers: Transformer Encoder Block\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
        "\n"
      ],
      "id": "guzBvvy6CZDt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkgmmdvUCZDt"
      },
      "outputs": [],
      "source": [
        "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
        "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = F.relu(self.fc_1(x))\n",
        "        o = self.fc_2(o)\n",
        "        return o"
      ],
      "id": "kkgmmdvUCZDt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bb27-wECZDt"
      },
      "outputs": [],
      "source": [
        "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
        "## module follows a simple computational pipeline:\n",
        "## input --> layernorm --> SelfAttention --> skip connection\n",
        "##       --> layernorm --> MLP ---> skip connection ---> output\n",
        "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
        "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
        "## to the SelfAttention block.\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "###############################################################\n",
        "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
        "###############################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dims)\n",
        "        self.self_attention = SelfAttention(input_dims=hidden_dims, head_dims=(hidden_dims//num_heads), num_heads=num_heads, bias=False)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dims)\n",
        "        self.mlp = MLP(hidden_dims, hidden_dims*4, hidden_dims, bias=True)\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "###################################################################\n",
        "#                                 END OF YOUR CODE                #\n",
        "###################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "##############################################################\n",
        "# TODO: Complete the forward of TransformerBlock module      #\n",
        "##############################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "        input1 = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.self_attention(x)\n",
        "        x = self.mlp(x + input1)\n",
        "\n",
        "        input2 = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = x + input2\n",
        "        return x\n",
        "\n",
        " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "###################################################################\n",
        "#                                 END OF YOUR CODE                #\n",
        "###################################################################"
      ],
      "id": "4Bb27-wECZDt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_1o1zsmCZDu"
      },
      "source": [
        "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 128)."
      ],
      "id": "w_1o1zsmCZDu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "wDVrTgs0CZDu",
        "outputId": "6e83413a-0a0f-498d-fcaa-ab491afad5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 16, 128])\n"
          ]
        }
      ],
      "source": [
        "def test_transfomerblock_layer():\n",
        "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
        "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
        "    out = layer(x)\n",
        "    print(out.size())\n",
        "test_transfomerblock_layer()"
      ],
      "id": "wDVrTgs0CZDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ozMqU4CZDu"
      },
      "source": [
        "# Part IV The Vision Transformer (ViT)\n",
        "\n",
        "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
      ],
      "id": "O4ozMqU4CZDu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMbYZaZmCZDu"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        ## initialize module's instance variables\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.input_dims = input_dims\n",
        "        self.output_dims = output_dims\n",
        "        self.num_trans_layers = num_trans_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.image_k = image_k\n",
        "        self.patch_k = patch_k\n",
        "\n",
        "        self.image_height = self.image_width = image_k\n",
        "        self.patch_height = self.patch_width = patch_k\n",
        "\n",
        "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
        "                'Image size must be divisible by the patch size.'\n",
        "\n",
        "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
        "        self.patch_flat_len = self.patch_height * self.patch_width\n",
        "\n",
        "        ## Declare module's parameters\n",
        "\n",
        "        ## ViT's flattened patch embedding projection:\n",
        "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
        "\n",
        "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
        "\n",
        "        ## Learnable classt token and its index among attention sequence elements.\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
        "        self.cls_index = torch.LongTensor([0])\n",
        "\n",
        "        ## Declare cascaded Transformer blocks:\n",
        "        transformer_encoder_list = []\n",
        "        for _ in range(self.num_trans_layers):\n",
        "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
        "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
        "\n",
        "        ## Declare the output mlp:\n",
        "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
        "\n",
        "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
        "        ## Create sliding window pathes using nn.Functional.unfold\n",
        "        ## Input dimensions: [B,D,H,W] where\n",
        "        ## --B : input batch size\n",
        "        ## --D : input channels\n",
        "        ## --H, W: input height and width\n",
        "        ## Output dimensions: [B,N,H*W,D]\n",
        "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
        "        ##      sliding window stride and padding.\n",
        "        b,d,h,w = x.shape\n",
        "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)\n",
        "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
        "        n = x_unf.size(1)\n",
        "        return x_unf,n\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.size(0)\n",
        "        ## create sliding window patches from the input image\n",
        "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
        "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
        "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
        "        ## linearly embed each flattened patch\n",
        "        x_embed = self.linear_embed(x_patch_flat)\n",
        "\n",
        "        ## retrieve class token\n",
        "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
        "        ## concatanate class token to input patches\n",
        "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
        "\n",
        "        ## add positional embedding to input patches + class token\n",
        "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
        "\n",
        "        ## pass through the transformer encoder\n",
        "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
        "\n",
        "        ## select the class token\n",
        "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
        "\n",
        "        ## create output\n",
        "        out = self.out_mlp(out_cls_token)\n",
        "\n",
        "        return out.squeeze(-2)"
      ],
      "id": "zMbYZaZmCZDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gfFmwB0CZDu"
      },
      "source": [
        "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 10)."
      ],
      "id": "4gfFmwB0CZDu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "7eTsRYQnCZDu",
        "outputId": "9dd4a422-43ab-447f-a2bf-4683a7e83792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "def test_vit():\n",
        "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
        "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
        "    out = model(x)\n",
        "    print(out.size())\n",
        "test_vit()"
      ],
      "id": "7eTsRYQnCZDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYG2uGBVCZDu"
      },
      "source": [
        "# Part V. Train the ViT"
      ],
      "id": "JYG2uGBVCZDu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC_DLQU8CZDv"
      },
      "source": [
        "### Check Accuracy\n",
        "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network.\n",
        "\n",
        "The check_batch_accuracy function is provided for you below:"
      ],
      "id": "kC_DLQU8CZDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "v2rCZpPYCZDv"
      },
      "outputs": [],
      "source": [
        "def check_batch_accuracy(out, target,eps=1e-7):\n",
        "    b, c = out.shape\n",
        "    with torch.no_grad():\n",
        "        _, pred = out.max(-1)\n",
        "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
        "    return correct, float(correct) / (b)"
      ],
      "id": "v2rCZpPYCZDv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMWhll96CZDv"
      },
      "source": [
        "### Training Loop\n",
        "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
      ],
      "id": "WMWhll96CZDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "w1QBILJECZDv"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer, trainloader):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "\n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
        "\n",
        "    Returns: overall training accuracy for the epoch\n",
        "    \"\"\"\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    network.train()  # put model to training mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
        "\n",
        "        outputs = network(inputs)\n",
        "        loss =  F.cross_entropy(outputs, targets)\n",
        "\n",
        "        # Zero out all of the gradients for the variables which the optimizer\n",
        "        # will update.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # This is the backwards pass: compute the gradient of the loss with\n",
        "        # respect to each  parameter of the model.\n",
        "        loss.backward()\n",
        "\n",
        "        # Actually update the parameters of the model using the gradients\n",
        "        # computed by the backwards pass.\n",
        "        optimizer.step()\n",
        "\n",
        "        loss = loss.detach()\n",
        "        train_loss += loss.item()\n",
        "        correct_p, _ = check_batch_accuracy(outputs, targets)\n",
        "        correct += correct_p\n",
        "        total += targets.size(0)\n",
        "\n",
        "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    return 100.*correct/total"
      ],
      "id": "w1QBILJECZDv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM4DNt7ECZDv"
      },
      "source": [
        "### Evaluation Loop\n",
        "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
      ],
      "id": "gM4DNt7ECZDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9A9k9RzCZDv"
      },
      "outputs": [],
      "source": [
        "def evaluate(network, evalloader):\n",
        "    \"\"\"\n",
        "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "\n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
        "\n",
        "    Returns: overall evaluation accuracy for the epoch\n",
        "    \"\"\"\n",
        "    network.eval() # put model to evaluation mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n---- Evaluation in process ----')\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
        "            outputs = network(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
        "            correct += correct_p\n",
        "            total += targets.size(0)\n",
        "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return 100.*correct/total"
      ],
      "id": "v9A9k9RzCZDv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dja_Ve3ECZDv"
      },
      "source": [
        "### Overfit a ViT\n",
        "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free.\n",
        "\n",
        "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`.\n",
        "\n",
        "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
        "\n",
        "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
      ],
      "id": "dja_Ve3ECZDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk88myaLCZDv",
        "outputId": "faabc3ff-9985-463d-aada-99df5c725e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
            "==> Data ready, batchsize = 25\n"
          ]
        }
      ],
      "source": [
        "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
        "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
        "\n",
        "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
        "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
        "\n",
        "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
        "\n",
        "batch_size_sub = 25\n",
        "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
        "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
        "\n",
        "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
      ],
      "id": "Gk88myaLCZDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "l_s8XApBCZDw",
        "outputId": "b612dd6d-e5d0-425c-fb6b-82fae9b8175f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Loss: 3.247 | Acc: 4.000% (1/25)\n",
            "Loss: 4.329 | Acc: 2.000% (1/50)\n",
            "Loss: 4.994 | Acc: 8.000% (6/75)\n",
            "Loss: 5.377 | Acc: 7.000% (7/100)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 7.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 3.790 | Acc: 20.000% (5/25)\n",
            "Loss: 3.942 | Acc: 14.000% (7/50)\n",
            "Loss: 4.192 | Acc: 10.667% (8/75)\n",
            "Loss: 4.266 | Acc: 11.000% (11/100)\n",
            "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 11.0\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 4.333 | Acc: 20.000% (5/25)\n",
            "Loss: 4.028 | Acc: 16.000% (8/50)\n",
            "Loss: 3.921 | Acc: 16.000% (12/75)\n",
            "Loss: 3.731 | Acc: 16.000% (16/100)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 16.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 3.128 | Acc: 12.000% (3/25)\n",
            "Loss: 3.080 | Acc: 18.000% (9/50)\n",
            "Loss: 3.164 | Acc: 13.333% (10/75)\n",
            "Loss: 3.122 | Acc: 12.000% (12/100)\n",
            "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 12.0\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 3.077 | Acc: 12.000% (3/25)\n",
            "Loss: 2.959 | Acc: 14.000% (7/50)\n",
            "Loss: 2.958 | Acc: 14.667% (11/75)\n",
            "Loss: 2.894 | Acc: 13.000% (13/100)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 13.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 3.016 | Acc: 4.000% (1/25)\n",
            "Loss: 2.971 | Acc: 4.000% (2/50)\n",
            "Loss: 2.961 | Acc: 5.333% (4/75)\n",
            "Loss: 2.880 | Acc: 7.000% (7/100)\n",
            "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 7.0\n",
            "\n",
            "Epoch: 3\n",
            "Loss: 2.541 | Acc: 4.000% (1/25)\n",
            "Loss: 2.551 | Acc: 4.000% (2/50)\n",
            "Loss: 2.913 | Acc: 6.667% (5/75)\n",
            "Loss: 2.881 | Acc: 6.000% (6/100)\n",
            "Epoch 3 of training is completed, Training accuracy for this epoch is 6.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.560 | Acc: 12.000% (3/25)\n",
            "Loss: 2.424 | Acc: 18.000% (9/50)\n",
            "Loss: 2.417 | Acc: 13.333% (10/75)\n",
            "Loss: 2.442 | Acc: 12.000% (12/100)\n",
            "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 12.0\n",
            "\n",
            "Epoch: 4\n",
            "Loss: 2.272 | Acc: 12.000% (3/25)\n",
            "Loss: 2.423 | Acc: 10.000% (5/50)\n",
            "Loss: 2.489 | Acc: 9.333% (7/75)\n",
            "Loss: 2.452 | Acc: 9.000% (9/100)\n",
            "Epoch 4 of training is completed, Training accuracy for this epoch is 9.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.708 | Acc: 4.000% (1/25)\n",
            "Loss: 2.664 | Acc: 4.000% (2/50)\n",
            "Loss: 2.546 | Acc: 9.333% (7/75)\n",
            "Loss: 2.578 | Acc: 9.000% (9/100)\n",
            "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 9.0\n",
            "\n",
            "Epoch: 5\n",
            "Loss: 2.049 | Acc: 24.000% (6/25)\n",
            "Loss: 2.295 | Acc: 20.000% (10/50)\n",
            "Loss: 2.405 | Acc: 14.667% (11/75)\n",
            "Loss: 2.419 | Acc: 14.000% (14/100)\n",
            "Epoch 5 of training is completed, Training accuracy for this epoch is 14.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.478 | Acc: 24.000% (6/25)\n",
            "Loss: 2.399 | Acc: 22.000% (11/50)\n",
            "Loss: 2.471 | Acc: 16.000% (12/75)\n",
            "Loss: 2.486 | Acc: 14.000% (14/100)\n",
            "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 14.0\n",
            "\n",
            "Epoch: 6\n",
            "Loss: 2.567 | Acc: 12.000% (3/25)\n",
            "Loss: 2.370 | Acc: 20.000% (10/50)\n",
            "Loss: 2.395 | Acc: 18.667% (14/75)\n",
            "Loss: 2.445 | Acc: 16.000% (16/100)\n",
            "Epoch 6 of training is completed, Training accuracy for this epoch is 16.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.528 | Acc: 4.000% (1/25)\n",
            "Loss: 2.395 | Acc: 8.000% (4/50)\n",
            "Loss: 2.364 | Acc: 9.333% (7/75)\n",
            "Loss: 2.397 | Acc: 10.000% (10/100)\n",
            "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 10.0\n",
            "\n",
            "Epoch: 7\n",
            "Loss: 2.483 | Acc: 4.000% (1/25)\n",
            "Loss: 2.354 | Acc: 8.000% (4/50)\n",
            "Loss: 2.345 | Acc: 9.333% (7/75)\n",
            "Loss: 2.357 | Acc: 8.000% (8/100)\n",
            "Epoch 7 of training is completed, Training accuracy for this epoch is 8.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.444 | Acc: 20.000% (5/25)\n",
            "Loss: 2.420 | Acc: 14.000% (7/50)\n",
            "Loss: 2.384 | Acc: 10.667% (8/75)\n",
            "Loss: 2.399 | Acc: 11.000% (11/100)\n",
            "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 11.0\n",
            "\n",
            "Epoch: 8\n",
            "Loss: 2.330 | Acc: 4.000% (1/25)\n",
            "Loss: 2.279 | Acc: 8.000% (4/50)\n",
            "Loss: 2.249 | Acc: 8.000% (6/75)\n",
            "Loss: 2.236 | Acc: 10.000% (10/100)\n",
            "Epoch 8 of training is completed, Training accuracy for this epoch is 10.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.696 | Acc: 8.000% (2/25)\n",
            "Loss: 2.529 | Acc: 10.000% (5/50)\n",
            "Loss: 2.440 | Acc: 14.667% (11/75)\n",
            "Loss: 2.524 | Acc: 13.000% (13/100)\n",
            "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 13.0\n",
            "\n",
            "Epoch: 9\n",
            "Loss: 2.182 | Acc: 20.000% (5/25)\n",
            "Loss: 2.239 | Acc: 18.000% (9/50)\n",
            "Loss: 2.181 | Acc: 18.667% (14/75)\n",
            "Loss: 2.191 | Acc: 20.000% (20/100)\n",
            "Epoch 9 of training is completed, Training accuracy for this epoch is 20.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.530 | Acc: 20.000% (5/25)\n",
            "Loss: 2.406 | Acc: 22.000% (11/50)\n",
            "Loss: 2.362 | Acc: 20.000% (15/75)\n",
            "Loss: 2.427 | Acc: 20.000% (20/100)\n",
            "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 20.0\n",
            "\n",
            "Epoch: 10\n",
            "Loss: 1.879 | Acc: 28.000% (7/25)\n",
            "Loss: 2.024 | Acc: 22.000% (11/50)\n",
            "Loss: 2.068 | Acc: 21.333% (16/75)\n",
            "Loss: 2.109 | Acc: 20.000% (20/100)\n",
            "Epoch 10 of training is completed, Training accuracy for this epoch is 20.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.553 | Acc: 4.000% (1/25)\n",
            "Loss: 2.421 | Acc: 8.000% (4/50)\n",
            "Loss: 2.368 | Acc: 16.000% (12/75)\n",
            "Loss: 2.488 | Acc: 16.000% (16/100)\n",
            "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 16.0\n",
            "\n",
            "Epoch: 11\n",
            "Loss: 1.892 | Acc: 24.000% (6/25)\n",
            "Loss: 1.910 | Acc: 24.000% (12/50)\n",
            "Loss: 1.987 | Acc: 22.667% (17/75)\n",
            "Loss: 2.023 | Acc: 24.000% (24/100)\n",
            "Epoch 11 of training is completed, Training accuracy for this epoch is 24.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.567 | Acc: 8.000% (2/25)\n",
            "Loss: 2.302 | Acc: 20.000% (10/50)\n",
            "Loss: 2.299 | Acc: 17.333% (13/75)\n",
            "Loss: 2.407 | Acc: 15.000% (15/100)\n",
            "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 15.0\n",
            "\n",
            "Epoch: 12\n",
            "Loss: 1.964 | Acc: 24.000% (6/25)\n",
            "Loss: 1.840 | Acc: 40.000% (20/50)\n",
            "Loss: 1.781 | Acc: 40.000% (30/75)\n",
            "Loss: 1.918 | Acc: 35.000% (35/100)\n",
            "Epoch 12 of training is completed, Training accuracy for this epoch is 35.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.642 | Acc: 8.000% (2/25)\n",
            "Loss: 2.505 | Acc: 14.000% (7/50)\n",
            "Loss: 2.520 | Acc: 13.333% (10/75)\n",
            "Loss: 2.627 | Acc: 11.000% (11/100)\n",
            "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 11.0\n",
            "\n",
            "Epoch: 13\n",
            "Loss: 2.058 | Acc: 24.000% (6/25)\n",
            "Loss: 1.863 | Acc: 26.000% (13/50)\n",
            "Loss: 1.856 | Acc: 26.667% (20/75)\n",
            "Loss: 1.906 | Acc: 25.000% (25/100)\n",
            "Epoch 13 of training is completed, Training accuracy for this epoch is 25.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.838 | Acc: 12.000% (3/25)\n",
            "Loss: 2.491 | Acc: 16.000% (8/50)\n",
            "Loss: 2.456 | Acc: 14.667% (11/75)\n",
            "Loss: 2.656 | Acc: 13.000% (13/100)\n",
            "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 13.0\n",
            "\n",
            "Epoch: 14\n",
            "Loss: 1.766 | Acc: 40.000% (10/25)\n",
            "Loss: 1.772 | Acc: 32.000% (16/50)\n",
            "Loss: 1.879 | Acc: 29.333% (22/75)\n",
            "Loss: 1.830 | Acc: 30.000% (30/100)\n",
            "Epoch 14 of training is completed, Training accuracy for this epoch is 30.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.927 | Acc: 12.000% (3/25)\n",
            "Loss: 2.692 | Acc: 16.000% (8/50)\n",
            "Loss: 2.707 | Acc: 14.667% (11/75)\n",
            "Loss: 2.870 | Acc: 12.000% (12/100)\n",
            "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 12.0\n",
            "\n",
            "Final train set accuracy is 30.0\n",
            "Final val set accuracy is 12.0\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "regularization_val = 1e-6\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims=10\n",
        "num_trans_layers = 4\n",
        "num_heads=4\n",
        "image_k=32\n",
        "patch_k=4\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "network = ViT(hidden_dims=hidden_dims,\n",
        "            input_dims=input_dims,\n",
        "            output_dims=output_dims,\n",
        "            num_trans_layers=num_trans_layers,\n",
        "            num_heads=num_heads,\n",
        "            image_k=image_k,\n",
        "            patch_k=patch_k,\n",
        "            bias=False).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0)\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE\n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "eval_accs=[]\n",
        "for epoch in range(15):\n",
        "    tr_acc = train(network, optimizer, trainloader_sub)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))\n",
        "\n",
        "    eval_acc = evaluate(network, valloader_sub)\n",
        "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
        "              .format(epoch, eval_acc))\n",
        "    tr_accs.append(tr_acc)\n",
        "    eval_accs.append(eval_acc)\n",
        "\n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
      ],
      "id": "l_s8XApBCZDw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKcEJDYICZDw"
      },
      "source": [
        "## Train the net\n",
        "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
      ],
      "id": "tKcEJDYICZDw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yY2Rz9JJCZDw",
        "outputId": "8b144988-6cc6-4ec7-be9c-dba759fe2383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Loss: 3.203 | Acc: 6.250% (4/64)\n",
            "Loss: 3.842 | Acc: 10.156% (13/128)\n",
            "Loss: 3.785 | Acc: 9.896% (19/192)\n",
            "Loss: 5.399 | Acc: 8.984% (23/256)\n",
            "Loss: 5.111 | Acc: 8.750% (28/320)\n",
            "Loss: 4.906 | Acc: 9.635% (37/384)\n",
            "Loss: 4.679 | Acc: 10.268% (46/448)\n",
            "Loss: 4.514 | Acc: 9.570% (49/512)\n",
            "Loss: 4.535 | Acc: 9.375% (54/576)\n",
            "Loss: 4.548 | Acc: 9.219% (59/640)\n",
            "Loss: 4.430 | Acc: 9.233% (65/704)\n",
            "Loss: 4.282 | Acc: 8.984% (69/768)\n",
            "Loss: 4.153 | Acc: 9.255% (77/832)\n",
            "Loss: 4.049 | Acc: 9.598% (86/896)\n",
            "Loss: 4.020 | Acc: 9.375% (90/960)\n",
            "Loss: 3.949 | Acc: 9.473% (97/1024)\n",
            "Loss: 3.902 | Acc: 9.651% (105/1088)\n",
            "Loss: 3.856 | Acc: 9.896% (114/1152)\n",
            "Loss: 3.813 | Acc: 10.033% (122/1216)\n",
            "Loss: 3.749 | Acc: 10.156% (130/1280)\n",
            "Loss: 3.691 | Acc: 10.045% (135/1344)\n",
            "Loss: 3.627 | Acc: 10.369% (146/1408)\n",
            "Loss: 3.573 | Acc: 10.462% (154/1472)\n",
            "Loss: 3.531 | Acc: 10.482% (161/1536)\n",
            "Loss: 3.489 | Acc: 10.625% (170/1600)\n",
            "Loss: 3.446 | Acc: 10.637% (177/1664)\n",
            "Loss: 3.410 | Acc: 10.532% (182/1728)\n",
            "Loss: 3.377 | Acc: 10.491% (188/1792)\n",
            "Loss: 3.348 | Acc: 10.399% (193/1856)\n",
            "Loss: 3.318 | Acc: 10.260% (197/1920)\n",
            "Loss: 3.291 | Acc: 10.181% (202/1984)\n",
            "Loss: 3.263 | Acc: 10.254% (210/2048)\n",
            "Loss: 3.237 | Acc: 10.322% (218/2112)\n",
            "Loss: 3.210 | Acc: 10.340% (225/2176)\n",
            "Loss: 3.188 | Acc: 10.357% (232/2240)\n",
            "Loss: 3.168 | Acc: 10.243% (236/2304)\n",
            "Loss: 3.146 | Acc: 10.220% (242/2368)\n",
            "Loss: 3.125 | Acc: 10.238% (249/2432)\n",
            "Loss: 3.104 | Acc: 10.337% (258/2496)\n",
            "Loss: 3.089 | Acc: 10.430% (267/2560)\n",
            "Loss: 3.069 | Acc: 10.595% (278/2624)\n",
            "Loss: 3.050 | Acc: 10.677% (287/2688)\n",
            "Loss: 3.030 | Acc: 10.756% (296/2752)\n",
            "Loss: 3.014 | Acc: 10.795% (304/2816)\n",
            "Loss: 2.999 | Acc: 10.972% (316/2880)\n",
            "Loss: 2.984 | Acc: 11.175% (329/2944)\n",
            "Loss: 2.966 | Acc: 11.436% (344/3008)\n",
            "Loss: 2.953 | Acc: 11.654% (358/3072)\n",
            "Loss: 2.942 | Acc: 11.671% (366/3136)\n",
            "Loss: 2.927 | Acc: 11.719% (375/3200)\n",
            "Loss: 2.910 | Acc: 11.949% (390/3264)\n",
            "Loss: 2.894 | Acc: 12.139% (404/3328)\n",
            "Loss: 2.881 | Acc: 12.264% (416/3392)\n",
            "Loss: 2.873 | Acc: 12.269% (424/3456)\n",
            "Loss: 2.860 | Acc: 12.528% (441/3520)\n",
            "Loss: 2.845 | Acc: 12.640% (453/3584)\n",
            "Loss: 2.835 | Acc: 12.664% (462/3648)\n",
            "Loss: 2.827 | Acc: 12.662% (470/3712)\n",
            "Loss: 2.815 | Acc: 12.871% (486/3776)\n",
            "Loss: 2.803 | Acc: 13.073% (502/3840)\n",
            "Loss: 2.792 | Acc: 13.217% (516/3904)\n",
            "Loss: 2.782 | Acc: 13.256% (526/3968)\n",
            "Loss: 2.771 | Acc: 13.343% (538/4032)\n",
            "Loss: 2.762 | Acc: 13.379% (548/4096)\n",
            "Loss: 2.750 | Acc: 13.534% (563/4160)\n",
            "Loss: 2.739 | Acc: 13.684% (578/4224)\n",
            "Loss: 2.730 | Acc: 13.783% (591/4288)\n",
            "Loss: 2.721 | Acc: 13.879% (604/4352)\n",
            "Loss: 2.712 | Acc: 13.904% (614/4416)\n",
            "Loss: 2.704 | Acc: 13.973% (626/4480)\n",
            "Loss: 2.698 | Acc: 13.996% (636/4544)\n",
            "Loss: 2.691 | Acc: 14.084% (649/4608)\n",
            "Loss: 2.688 | Acc: 14.084% (658/4672)\n",
            "Loss: 2.683 | Acc: 14.105% (668/4736)\n",
            "Loss: 2.675 | Acc: 14.250% (684/4800)\n",
            "Loss: 2.669 | Acc: 14.350% (698/4864)\n",
            "Loss: 2.664 | Acc: 14.265% (703/4928)\n",
            "Loss: 2.656 | Acc: 14.423% (720/4992)\n",
            "Loss: 2.649 | Acc: 14.517% (734/5056)\n",
            "Loss: 2.645 | Acc: 14.512% (743/5120)\n",
            "Loss: 2.641 | Acc: 14.583% (756/5184)\n",
            "Loss: 2.636 | Acc: 14.710% (772/5248)\n",
            "Loss: 2.632 | Acc: 14.759% (784/5312)\n",
            "Loss: 2.627 | Acc: 14.807% (796/5376)\n",
            "Loss: 2.621 | Acc: 14.926% (812/5440)\n",
            "Loss: 2.617 | Acc: 14.953% (823/5504)\n",
            "Loss: 2.612 | Acc: 14.925% (831/5568)\n",
            "Loss: 2.606 | Acc: 15.057% (848/5632)\n",
            "Loss: 2.602 | Acc: 15.063% (858/5696)\n",
            "Loss: 2.597 | Acc: 15.174% (874/5760)\n",
            "Loss: 2.592 | Acc: 15.264% (889/5824)\n",
            "Loss: 2.587 | Acc: 15.370% (905/5888)\n",
            "Loss: 2.582 | Acc: 15.407% (917/5952)\n",
            "Loss: 2.579 | Acc: 15.392% (926/6016)\n",
            "Loss: 2.573 | Acc: 15.461% (940/6080)\n",
            "Loss: 2.570 | Acc: 15.479% (951/6144)\n",
            "Loss: 2.565 | Acc: 15.512% (963/6208)\n",
            "Loss: 2.561 | Acc: 15.593% (978/6272)\n",
            "Loss: 2.557 | Acc: 15.609% (989/6336)\n",
            "Loss: 2.554 | Acc: 15.609% (999/6400)\n",
            "Loss: 2.549 | Acc: 15.671% (1013/6464)\n",
            "Loss: 2.544 | Acc: 15.717% (1026/6528)\n",
            "Loss: 2.541 | Acc: 15.762% (1039/6592)\n",
            "Loss: 2.538 | Acc: 15.835% (1054/6656)\n",
            "Loss: 2.534 | Acc: 15.878% (1067/6720)\n",
            "Loss: 2.528 | Acc: 16.008% (1086/6784)\n",
            "Loss: 2.523 | Acc: 16.078% (1101/6848)\n",
            "Loss: 2.519 | Acc: 16.059% (1110/6912)\n",
            "Loss: 2.514 | Acc: 16.184% (1129/6976)\n",
            "Loss: 2.510 | Acc: 16.193% (1140/7040)\n",
            "Loss: 2.508 | Acc: 16.188% (1150/7104)\n",
            "Loss: 2.504 | Acc: 16.211% (1162/7168)\n",
            "Loss: 2.503 | Acc: 16.261% (1176/7232)\n",
            "Loss: 2.500 | Acc: 16.283% (1188/7296)\n",
            "Loss: 2.496 | Acc: 16.427% (1209/7360)\n",
            "Loss: 2.494 | Acc: 16.447% (1221/7424)\n",
            "Loss: 2.491 | Acc: 16.453% (1232/7488)\n",
            "Loss: 2.489 | Acc: 16.512% (1247/7552)\n",
            "Loss: 2.487 | Acc: 16.570% (1262/7616)\n",
            "Loss: 2.484 | Acc: 16.667% (1280/7680)\n",
            "Loss: 2.480 | Acc: 16.697% (1293/7744)\n",
            "Loss: 2.478 | Acc: 16.765% (1309/7808)\n",
            "Loss: 2.475 | Acc: 16.819% (1324/7872)\n",
            "Loss: 2.472 | Acc: 16.860% (1338/7936)\n",
            "Loss: 2.470 | Acc: 16.900% (1352/8000)\n",
            "Loss: 2.467 | Acc: 16.902% (1363/8064)\n",
            "Loss: 2.464 | Acc: 16.966% (1379/8128)\n",
            "Loss: 2.461 | Acc: 17.029% (1395/8192)\n",
            "Loss: 2.458 | Acc: 17.103% (1412/8256)\n",
            "Loss: 2.455 | Acc: 17.079% (1421/8320)\n",
            "Loss: 2.452 | Acc: 17.116% (1435/8384)\n",
            "Loss: 2.448 | Acc: 17.211% (1454/8448)\n",
            "Loss: 2.445 | Acc: 17.293% (1472/8512)\n",
            "Loss: 2.442 | Acc: 17.281% (1482/8576)\n",
            "Loss: 2.439 | Acc: 17.373% (1501/8640)\n",
            "Loss: 2.436 | Acc: 17.406% (1515/8704)\n",
            "Loss: 2.433 | Acc: 17.404% (1526/8768)\n",
            "Loss: 2.431 | Acc: 17.414% (1538/8832)\n",
            "Loss: 2.428 | Acc: 17.390% (1547/8896)\n",
            "Loss: 2.425 | Acc: 17.422% (1561/8960)\n",
            "Loss: 2.423 | Acc: 17.465% (1576/9024)\n",
            "Loss: 2.420 | Acc: 17.540% (1594/9088)\n",
            "Loss: 2.418 | Acc: 17.559% (1607/9152)\n",
            "Loss: 2.415 | Acc: 17.600% (1622/9216)\n",
            "Loss: 2.412 | Acc: 17.608% (1634/9280)\n",
            "Loss: 2.409 | Acc: 17.626% (1647/9344)\n",
            "Loss: 2.406 | Acc: 17.740% (1669/9408)\n",
            "Loss: 2.402 | Acc: 17.810% (1687/9472)\n",
            "Loss: 2.399 | Acc: 17.848% (1702/9536)\n",
            "Loss: 2.397 | Acc: 17.896% (1718/9600)\n",
            "Loss: 2.395 | Acc: 17.922% (1732/9664)\n",
            "Loss: 2.392 | Acc: 17.876% (1739/9728)\n",
            "Loss: 2.390 | Acc: 17.943% (1757/9792)\n",
            "Loss: 2.387 | Acc: 18.009% (1775/9856)\n",
            "Loss: 2.385 | Acc: 17.964% (1782/9920)\n",
            "Loss: 2.383 | Acc: 17.959% (1793/9984)\n",
            "Loss: 2.380 | Acc: 17.954% (1804/10048)\n",
            "Loss: 2.379 | Acc: 17.969% (1817/10112)\n",
            "Loss: 2.376 | Acc: 18.033% (1835/10176)\n",
            "Loss: 2.373 | Acc: 18.115% (1855/10240)\n",
            "Loss: 2.370 | Acc: 18.226% (1878/10304)\n",
            "Loss: 2.369 | Acc: 18.248% (1892/10368)\n",
            "Loss: 2.367 | Acc: 18.280% (1907/10432)\n",
            "Loss: 2.364 | Acc: 18.312% (1922/10496)\n",
            "Loss: 2.362 | Acc: 18.333% (1936/10560)\n",
            "Loss: 2.360 | Acc: 18.373% (1952/10624)\n",
            "Loss: 2.358 | Acc: 18.394% (1966/10688)\n",
            "Loss: 2.356 | Acc: 18.443% (1983/10752)\n",
            "Loss: 2.353 | Acc: 18.500% (2001/10816)\n",
            "Loss: 2.351 | Acc: 18.539% (2017/10880)\n",
            "Loss: 2.349 | Acc: 18.604% (2036/10944)\n",
            "Loss: 2.347 | Acc: 18.632% (2051/11008)\n",
            "Loss: 2.346 | Acc: 18.624% (2062/11072)\n",
            "Loss: 2.344 | Acc: 18.651% (2077/11136)\n",
            "Loss: 2.343 | Acc: 18.652% (2089/11200)\n",
            "Loss: 2.341 | Acc: 18.635% (2099/11264)\n",
            "Loss: 2.339 | Acc: 18.706% (2119/11328)\n",
            "Loss: 2.338 | Acc: 18.689% (2129/11392)\n",
            "Loss: 2.336 | Acc: 18.733% (2146/11456)\n",
            "Loss: 2.334 | Acc: 18.741% (2159/11520)\n",
            "Loss: 2.333 | Acc: 18.759% (2173/11584)\n",
            "Loss: 2.331 | Acc: 18.759% (2185/11648)\n",
            "Loss: 2.330 | Acc: 18.750% (2196/11712)\n",
            "Loss: 2.329 | Acc: 18.733% (2206/11776)\n",
            "Loss: 2.327 | Acc: 18.784% (2224/11840)\n",
            "Loss: 2.326 | Acc: 18.800% (2238/11904)\n",
            "Loss: 2.324 | Acc: 18.842% (2255/11968)\n",
            "Loss: 2.322 | Acc: 18.908% (2275/12032)\n",
            "Loss: 2.320 | Acc: 18.948% (2292/12096)\n",
            "Loss: 2.318 | Acc: 19.030% (2314/12160)\n",
            "Loss: 2.316 | Acc: 19.053% (2329/12224)\n",
            "Loss: 2.314 | Acc: 19.149% (2353/12288)\n",
            "Loss: 2.312 | Acc: 19.211% (2373/12352)\n",
            "Loss: 2.311 | Acc: 19.257% (2391/12416)\n",
            "Loss: 2.309 | Acc: 19.295% (2408/12480)\n",
            "Loss: 2.308 | Acc: 19.300% (2421/12544)\n",
            "Loss: 2.307 | Acc: 19.305% (2434/12608)\n",
            "Loss: 2.306 | Acc: 19.287% (2444/12672)\n",
            "Loss: 2.304 | Acc: 19.355% (2465/12736)\n",
            "Loss: 2.302 | Acc: 19.422% (2486/12800)\n",
            "Loss: 2.301 | Acc: 19.426% (2499/12864)\n",
            "Loss: 2.299 | Acc: 19.446% (2514/12928)\n",
            "Loss: 2.298 | Acc: 19.474% (2530/12992)\n",
            "Loss: 2.297 | Acc: 19.462% (2541/13056)\n",
            "Loss: 2.296 | Acc: 19.520% (2561/13120)\n",
            "Loss: 2.294 | Acc: 19.645% (2590/13184)\n",
            "Loss: 2.292 | Acc: 19.694% (2609/13248)\n",
            "Loss: 2.290 | Acc: 19.689% (2621/13312)\n",
            "Loss: 2.289 | Acc: 19.737% (2640/13376)\n",
            "Loss: 2.288 | Acc: 19.792% (2660/13440)\n",
            "Loss: 2.286 | Acc: 19.794% (2673/13504)\n",
            "Loss: 2.285 | Acc: 19.833% (2691/13568)\n",
            "Loss: 2.283 | Acc: 19.828% (2703/13632)\n",
            "Loss: 2.281 | Acc: 19.853% (2719/13696)\n",
            "Loss: 2.280 | Acc: 19.862% (2733/13760)\n",
            "Loss: 2.278 | Acc: 19.871% (2747/13824)\n",
            "Loss: 2.277 | Acc: 19.888% (2762/13888)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.275 | Acc: 19.940% (2782/13952)\n",
            "Loss: 2.274 | Acc: 19.991% (2802/14016)\n",
            "Loss: 2.272 | Acc: 20.014% (2818/14080)\n",
            "Loss: 2.271 | Acc: 20.044% (2835/14144)\n",
            "Loss: 2.270 | Acc: 20.080% (2853/14208)\n",
            "Loss: 2.269 | Acc: 20.116% (2871/14272)\n",
            "Loss: 2.269 | Acc: 20.096% (2881/14336)\n",
            "Loss: 2.268 | Acc: 20.111% (2896/14400)\n",
            "Loss: 2.267 | Acc: 20.147% (2914/14464)\n",
            "Loss: 2.266 | Acc: 20.168% (2930/14528)\n",
            "Loss: 2.265 | Acc: 20.169% (2943/14592)\n",
            "Loss: 2.264 | Acc: 20.183% (2958/14656)\n",
            "Loss: 2.263 | Acc: 20.224% (2977/14720)\n",
            "Loss: 2.261 | Acc: 20.258% (2995/14784)\n",
            "Loss: 2.259 | Acc: 20.299% (3014/14848)\n",
            "Loss: 2.259 | Acc: 20.312% (3029/14912)\n",
            "Loss: 2.257 | Acc: 20.346% (3047/14976)\n",
            "Loss: 2.256 | Acc: 20.399% (3068/15040)\n",
            "Loss: 2.255 | Acc: 20.399% (3081/15104)\n",
            "Loss: 2.254 | Acc: 20.425% (3098/15168)\n",
            "Loss: 2.253 | Acc: 20.424% (3111/15232)\n",
            "Loss: 2.252 | Acc: 20.437% (3126/15296)\n",
            "Loss: 2.251 | Acc: 20.482% (3146/15360)\n",
            "Loss: 2.249 | Acc: 20.539% (3168/15424)\n",
            "Loss: 2.248 | Acc: 20.558% (3184/15488)\n",
            "Loss: 2.247 | Acc: 20.595% (3203/15552)\n",
            "Loss: 2.246 | Acc: 20.671% (3228/15616)\n",
            "Loss: 2.244 | Acc: 20.682% (3243/15680)\n",
            "Loss: 2.242 | Acc: 20.719% (3262/15744)\n",
            "Loss: 2.241 | Acc: 20.743% (3279/15808)\n",
            "Loss: 2.241 | Acc: 20.735% (3291/15872)\n",
            "Loss: 2.239 | Acc: 20.739% (3305/15936)\n",
            "Loss: 2.238 | Acc: 20.775% (3324/16000)\n",
            "Loss: 2.237 | Acc: 20.804% (3342/16064)\n",
            "Loss: 2.235 | Acc: 20.809% (3356/16128)\n",
            "Loss: 2.234 | Acc: 20.868% (3379/16192)\n",
            "Loss: 2.233 | Acc: 20.872% (3393/16256)\n",
            "Loss: 2.232 | Acc: 20.882% (3408/16320)\n",
            "Loss: 2.231 | Acc: 20.898% (3424/16384)\n",
            "Loss: 2.229 | Acc: 20.939% (3444/16448)\n",
            "Loss: 2.228 | Acc: 20.979% (3464/16512)\n",
            "Loss: 2.227 | Acc: 20.970% (3476/16576)\n",
            "Loss: 2.227 | Acc: 20.986% (3492/16640)\n",
            "Loss: 2.226 | Acc: 21.001% (3508/16704)\n",
            "Loss: 2.225 | Acc: 20.986% (3519/16768)\n",
            "Loss: 2.223 | Acc: 21.025% (3539/16832)\n",
            "Loss: 2.223 | Acc: 21.023% (3552/16896)\n",
            "Loss: 2.222 | Acc: 21.050% (3570/16960)\n",
            "Loss: 2.221 | Acc: 21.053% (3584/17024)\n",
            "Loss: 2.221 | Acc: 21.067% (3600/17088)\n",
            "Loss: 2.220 | Acc: 21.100% (3619/17152)\n",
            "Loss: 2.219 | Acc: 21.108% (3634/17216)\n",
            "Loss: 2.218 | Acc: 21.111% (3648/17280)\n",
            "Loss: 2.217 | Acc: 21.149% (3668/17344)\n",
            "Loss: 2.216 | Acc: 21.163% (3684/17408)\n",
            "Loss: 2.216 | Acc: 21.148% (3695/17472)\n",
            "Loss: 2.214 | Acc: 21.179% (3714/17536)\n",
            "Loss: 2.214 | Acc: 21.193% (3730/17600)\n",
            "Loss: 2.213 | Acc: 21.207% (3746/17664)\n",
            "Loss: 2.212 | Acc: 21.238% (3765/17728)\n",
            "Loss: 2.212 | Acc: 21.240% (3779/17792)\n",
            "Loss: 2.211 | Acc: 21.248% (3794/17856)\n",
            "Loss: 2.210 | Acc: 21.261% (3810/17920)\n",
            "Loss: 2.209 | Acc: 21.269% (3825/17984)\n",
            "Loss: 2.208 | Acc: 21.288% (3842/18048)\n",
            "Loss: 2.207 | Acc: 21.306% (3859/18112)\n",
            "Loss: 2.206 | Acc: 21.319% (3875/18176)\n",
            "Loss: 2.205 | Acc: 21.371% (3898/18240)\n",
            "Loss: 2.204 | Acc: 21.351% (3908/18304)\n",
            "Loss: 2.203 | Acc: 21.369% (3925/18368)\n",
            "Loss: 2.202 | Acc: 21.392% (3943/18432)\n",
            "Loss: 2.201 | Acc: 21.415% (3961/18496)\n",
            "Loss: 2.200 | Acc: 21.433% (3978/18560)\n",
            "Loss: 2.199 | Acc: 21.429% (3991/18624)\n",
            "Loss: 2.198 | Acc: 21.431% (4005/18688)\n",
            "Loss: 2.197 | Acc: 21.464% (4025/18752)\n",
            "Loss: 2.196 | Acc: 21.482% (4042/18816)\n",
            "Loss: 2.195 | Acc: 21.499% (4059/18880)\n",
            "Loss: 2.194 | Acc: 21.548% (4082/18944)\n",
            "Loss: 2.193 | Acc: 21.559% (4098/19008)\n",
            "Loss: 2.192 | Acc: 21.576% (4115/19072)\n",
            "Loss: 2.192 | Acc: 21.582% (4130/19136)\n",
            "Loss: 2.191 | Acc: 21.573% (4142/19200)\n",
            "Loss: 2.190 | Acc: 21.579% (4157/19264)\n",
            "Loss: 2.190 | Acc: 21.570% (4169/19328)\n",
            "Loss: 2.189 | Acc: 21.581% (4185/19392)\n",
            "Loss: 2.189 | Acc: 21.618% (4206/19456)\n",
            "Loss: 2.188 | Acc: 21.639% (4224/19520)\n",
            "Loss: 2.187 | Acc: 21.635% (4237/19584)\n",
            "Loss: 2.186 | Acc: 21.677% (4259/19648)\n",
            "Loss: 2.185 | Acc: 21.697% (4277/19712)\n",
            "Loss: 2.185 | Acc: 21.703% (4292/19776)\n",
            "Loss: 2.184 | Acc: 21.709% (4307/19840)\n",
            "Loss: 2.184 | Acc: 21.684% (4316/19904)\n",
            "Loss: 2.183 | Acc: 21.725% (4338/19968)\n",
            "Loss: 2.183 | Acc: 21.740% (4355/20032)\n",
            "Loss: 2.182 | Acc: 21.790% (4379/20096)\n",
            "Loss: 2.181 | Acc: 21.830% (4401/20160)\n",
            "Loss: 2.180 | Acc: 21.880% (4425/20224)\n",
            "Loss: 2.179 | Acc: 21.865% (4436/20288)\n",
            "Loss: 2.178 | Acc: 21.875% (4452/20352)\n",
            "Loss: 2.177 | Acc: 21.890% (4469/20416)\n",
            "Loss: 2.177 | Acc: 21.909% (4487/20480)\n",
            "Loss: 2.175 | Acc: 21.933% (4506/20544)\n",
            "Loss: 2.175 | Acc: 21.953% (4524/20608)\n",
            "Loss: 2.174 | Acc: 21.986% (4545/20672)\n",
            "Loss: 2.173 | Acc: 21.996% (4561/20736)\n",
            "Loss: 2.173 | Acc: 22.005% (4577/20800)\n",
            "Loss: 2.172 | Acc: 22.038% (4598/20864)\n",
            "Loss: 2.171 | Acc: 22.047% (4614/20928)\n",
            "Loss: 2.171 | Acc: 22.075% (4634/20992)\n",
            "Loss: 2.170 | Acc: 22.084% (4650/21056)\n",
            "Loss: 2.170 | Acc: 22.074% (4662/21120)\n",
            "Loss: 2.169 | Acc: 22.087% (4679/21184)\n",
            "Loss: 2.168 | Acc: 22.106% (4697/21248)\n",
            "Loss: 2.167 | Acc: 22.100% (4710/21312)\n",
            "Loss: 2.166 | Acc: 22.114% (4727/21376)\n",
            "Loss: 2.166 | Acc: 22.122% (4743/21440)\n",
            "Loss: 2.165 | Acc: 22.140% (4761/21504)\n",
            "Loss: 2.165 | Acc: 22.144% (4776/21568)\n",
            "Loss: 2.164 | Acc: 22.162% (4794/21632)\n",
            "Loss: 2.164 | Acc: 22.193% (4815/21696)\n",
            "Loss: 2.163 | Acc: 22.206% (4832/21760)\n",
            "Loss: 2.162 | Acc: 22.242% (4854/21824)\n",
            "Loss: 2.161 | Acc: 22.277% (4876/21888)\n",
            "Loss: 2.160 | Acc: 22.290% (4893/21952)\n",
            "Loss: 2.160 | Acc: 22.307% (4911/22016)\n",
            "Loss: 2.159 | Acc: 22.355% (4936/22080)\n",
            "Loss: 2.158 | Acc: 22.394% (4959/22144)\n",
            "Loss: 2.158 | Acc: 22.397% (4974/22208)\n",
            "Loss: 2.156 | Acc: 22.445% (4999/22272)\n",
            "Loss: 2.155 | Acc: 22.484% (5022/22336)\n",
            "Loss: 2.155 | Acc: 22.500% (5040/22400)\n",
            "Loss: 2.155 | Acc: 22.520% (5059/22464)\n",
            "Loss: 2.154 | Acc: 22.541% (5078/22528)\n",
            "Loss: 2.153 | Acc: 22.557% (5096/22592)\n",
            "Loss: 2.152 | Acc: 22.594% (5119/22656)\n",
            "Loss: 2.152 | Acc: 22.575% (5129/22720)\n",
            "Loss: 2.151 | Acc: 22.586% (5146/22784)\n",
            "Loss: 2.151 | Acc: 22.597% (5163/22848)\n",
            "Loss: 2.149 | Acc: 22.630% (5185/22912)\n",
            "Loss: 2.148 | Acc: 22.641% (5202/22976)\n",
            "Loss: 2.148 | Acc: 22.678% (5225/23040)\n",
            "Loss: 2.147 | Acc: 22.684% (5241/23104)\n",
            "Loss: 2.146 | Acc: 22.699% (5259/23168)\n",
            "Loss: 2.146 | Acc: 22.710% (5276/23232)\n",
            "Loss: 2.145 | Acc: 22.721% (5293/23296)\n",
            "Loss: 2.145 | Acc: 22.744% (5313/23360)\n",
            "Loss: 2.144 | Acc: 22.759% (5331/23424)\n",
            "Loss: 2.144 | Acc: 22.756% (5345/23488)\n",
            "Loss: 2.143 | Acc: 22.767% (5362/23552)\n",
            "Loss: 2.142 | Acc: 22.794% (5383/23616)\n",
            "Loss: 2.141 | Acc: 22.842% (5409/23680)\n",
            "Loss: 2.140 | Acc: 22.856% (5427/23744)\n",
            "Loss: 2.140 | Acc: 22.854% (5441/23808)\n",
            "Loss: 2.140 | Acc: 22.855% (5456/23872)\n",
            "Loss: 2.139 | Acc: 22.878% (5476/23936)\n",
            "Loss: 2.138 | Acc: 22.908% (5498/24000)\n",
            "Loss: 2.137 | Acc: 22.926% (5517/24064)\n",
            "Loss: 2.137 | Acc: 22.932% (5533/24128)\n",
            "Loss: 2.136 | Acc: 22.950% (5552/24192)\n",
            "Loss: 2.135 | Acc: 22.972% (5572/24256)\n",
            "Loss: 2.134 | Acc: 22.985% (5590/24320)\n",
            "Loss: 2.133 | Acc: 22.986% (5605/24384)\n",
            "Loss: 2.132 | Acc: 23.024% (5629/24448)\n",
            "Loss: 2.131 | Acc: 23.030% (5645/24512)\n",
            "Loss: 2.131 | Acc: 23.075% (5671/24576)\n",
            "Loss: 2.130 | Acc: 23.109% (5694/24640)\n",
            "Loss: 2.129 | Acc: 23.138% (5716/24704)\n",
            "Loss: 2.128 | Acc: 23.171% (5739/24768)\n",
            "Loss: 2.127 | Acc: 23.208% (5763/24832)\n",
            "Loss: 2.126 | Acc: 23.233% (5784/24896)\n",
            "Loss: 2.126 | Acc: 23.253% (5804/24960)\n",
            "Loss: 2.126 | Acc: 23.262% (5821/25024)\n",
            "Loss: 2.125 | Acc: 23.278% (5840/25088)\n",
            "Loss: 2.125 | Acc: 23.290% (5858/25152)\n",
            "Loss: 2.124 | Acc: 23.295% (5874/25216)\n",
            "Loss: 2.123 | Acc: 23.299% (5890/25280)\n",
            "Loss: 2.123 | Acc: 23.315% (5909/25344)\n",
            "Loss: 2.122 | Acc: 23.339% (5930/25408)\n",
            "Loss: 2.122 | Acc: 23.335% (5944/25472)\n",
            "Loss: 2.121 | Acc: 23.355% (5964/25536)\n",
            "Loss: 2.121 | Acc: 23.363% (5981/25600)\n",
            "Loss: 2.121 | Acc: 23.348% (5992/25664)\n",
            "Loss: 2.120 | Acc: 23.375% (6014/25728)\n",
            "Loss: 2.119 | Acc: 23.387% (6032/25792)\n",
            "Loss: 2.119 | Acc: 23.403% (6051/25856)\n",
            "Loss: 2.118 | Acc: 23.422% (6071/25920)\n",
            "Loss: 2.117 | Acc: 23.438% (6090/25984)\n",
            "Loss: 2.116 | Acc: 23.449% (6108/26048)\n",
            "Loss: 2.116 | Acc: 23.445% (6122/26112)\n",
            "Loss: 2.115 | Acc: 23.464% (6142/26176)\n",
            "Loss: 2.115 | Acc: 23.483% (6162/26240)\n",
            "Loss: 2.114 | Acc: 23.487% (6178/26304)\n",
            "Loss: 2.113 | Acc: 23.513% (6200/26368)\n",
            "Loss: 2.113 | Acc: 23.509% (6214/26432)\n",
            "Loss: 2.112 | Acc: 23.517% (6231/26496)\n",
            "Loss: 2.112 | Acc: 23.543% (6253/26560)\n",
            "Loss: 2.111 | Acc: 23.584% (6279/26624)\n",
            "Loss: 2.110 | Acc: 23.595% (6297/26688)\n",
            "Loss: 2.110 | Acc: 23.609% (6316/26752)\n",
            "Loss: 2.109 | Acc: 23.635% (6338/26816)\n",
            "Loss: 2.108 | Acc: 23.650% (6357/26880)\n",
            "Loss: 2.108 | Acc: 23.664% (6376/26944)\n",
            "Loss: 2.108 | Acc: 23.645% (6386/27008)\n",
            "Loss: 2.107 | Acc: 23.678% (6410/27072)\n",
            "Loss: 2.107 | Acc: 23.670% (6423/27136)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.107 | Acc: 23.676% (6440/27200)\n",
            "Loss: 2.106 | Acc: 23.669% (6453/27264)\n",
            "Loss: 2.106 | Acc: 23.701% (6477/27328)\n",
            "Loss: 2.105 | Acc: 23.708% (6494/27392)\n",
            "Loss: 2.105 | Acc: 23.736% (6517/27456)\n",
            "Loss: 2.105 | Acc: 23.721% (6528/27520)\n",
            "Loss: 2.104 | Acc: 23.731% (6546/27584)\n",
            "Loss: 2.103 | Acc: 23.781% (6575/27648)\n",
            "Loss: 2.103 | Acc: 23.777% (6589/27712)\n",
            "Loss: 2.102 | Acc: 23.794% (6609/27776)\n",
            "Loss: 2.102 | Acc: 23.822% (6632/27840)\n",
            "Loss: 2.101 | Acc: 23.853% (6656/27904)\n",
            "Loss: 2.101 | Acc: 23.849% (6670/27968)\n",
            "Loss: 2.101 | Acc: 23.862% (6689/28032)\n",
            "Loss: 2.100 | Acc: 23.872% (6707/28096)\n",
            "Loss: 2.100 | Acc: 23.874% (6723/28160)\n",
            "Loss: 2.099 | Acc: 23.887% (6742/28224)\n",
            "Loss: 2.099 | Acc: 23.883% (6756/28288)\n",
            "Loss: 2.099 | Acc: 23.889% (6773/28352)\n",
            "Loss: 2.098 | Acc: 23.906% (6793/28416)\n",
            "Loss: 2.097 | Acc: 23.922% (6813/28480)\n",
            "Loss: 2.097 | Acc: 23.900% (6822/28544)\n",
            "Loss: 2.097 | Acc: 23.899% (6837/28608)\n",
            "Loss: 2.096 | Acc: 23.926% (6860/28672)\n",
            "Loss: 2.096 | Acc: 23.942% (6880/28736)\n",
            "Loss: 2.095 | Acc: 23.948% (6897/28800)\n",
            "Loss: 2.095 | Acc: 23.971% (6919/28864)\n",
            "Loss: 2.094 | Acc: 24.008% (6945/28928)\n",
            "Loss: 2.094 | Acc: 24.014% (6962/28992)\n",
            "Loss: 2.093 | Acc: 24.036% (6984/29056)\n",
            "Loss: 2.093 | Acc: 24.028% (6997/29120)\n",
            "Loss: 2.092 | Acc: 24.058% (7021/29184)\n",
            "Loss: 2.091 | Acc: 24.101% (7049/29248)\n",
            "Loss: 2.091 | Acc: 24.123% (7071/29312)\n",
            "Loss: 2.091 | Acc: 24.122% (7086/29376)\n",
            "Loss: 2.090 | Acc: 24.130% (7104/29440)\n",
            "Loss: 2.090 | Acc: 24.132% (7120/29504)\n",
            "Loss: 2.089 | Acc: 24.144% (7139/29568)\n",
            "Loss: 2.089 | Acc: 24.153% (7157/29632)\n",
            "Loss: 2.089 | Acc: 24.155% (7173/29696)\n",
            "Loss: 2.088 | Acc: 24.157% (7189/29760)\n",
            "Loss: 2.088 | Acc: 24.165% (7207/29824)\n",
            "Loss: 2.088 | Acc: 24.174% (7225/29888)\n",
            "Loss: 2.087 | Acc: 24.172% (7240/29952)\n",
            "Loss: 2.087 | Acc: 24.187% (7260/30016)\n",
            "Loss: 2.086 | Acc: 24.202% (7280/30080)\n",
            "Loss: 2.085 | Acc: 24.214% (7299/30144)\n",
            "Loss: 2.085 | Acc: 24.235% (7321/30208)\n",
            "Loss: 2.085 | Acc: 24.253% (7342/30272)\n",
            "Loss: 2.084 | Acc: 24.271% (7363/30336)\n",
            "Loss: 2.083 | Acc: 24.299% (7387/30400)\n",
            "Loss: 2.083 | Acc: 24.317% (7408/30464)\n",
            "Loss: 2.082 | Acc: 24.332% (7428/30528)\n",
            "Loss: 2.081 | Acc: 24.363% (7453/30592)\n",
            "Loss: 2.080 | Acc: 24.387% (7476/30656)\n",
            "Loss: 2.080 | Acc: 24.417% (7501/30720)\n",
            "Loss: 2.079 | Acc: 24.432% (7521/30784)\n",
            "Loss: 2.079 | Acc: 24.446% (7541/30848)\n",
            "Loss: 2.078 | Acc: 24.457% (7560/30912)\n",
            "Loss: 2.077 | Acc: 24.474% (7581/30976)\n",
            "Loss: 2.077 | Acc: 24.507% (7607/31040)\n",
            "Loss: 2.076 | Acc: 24.511% (7624/31104)\n",
            "Loss: 2.076 | Acc: 24.509% (7639/31168)\n",
            "Loss: 2.075 | Acc: 24.545% (7666/31232)\n",
            "Loss: 2.075 | Acc: 24.559% (7686/31296)\n",
            "Loss: 2.074 | Acc: 24.557% (7701/31360)\n",
            "Loss: 2.074 | Acc: 24.570% (7721/31424)\n",
            "Loss: 2.073 | Acc: 24.593% (7744/31488)\n",
            "Loss: 2.072 | Acc: 24.610% (7765/31552)\n",
            "Loss: 2.072 | Acc: 24.608% (7780/31616)\n",
            "Loss: 2.072 | Acc: 24.609% (7796/31680)\n",
            "Loss: 2.071 | Acc: 24.638% (7821/31744)\n",
            "Loss: 2.070 | Acc: 24.670% (7847/31808)\n",
            "Loss: 2.070 | Acc: 24.686% (7868/31872)\n",
            "Loss: 2.069 | Acc: 24.706% (7890/31936)\n",
            "Loss: 2.069 | Acc: 24.716% (7909/32000)\n",
            "Loss: 2.069 | Acc: 24.729% (7929/32064)\n",
            "Loss: 2.068 | Acc: 24.720% (7942/32128)\n",
            "Loss: 2.068 | Acc: 24.742% (7965/32192)\n",
            "Loss: 2.068 | Acc: 24.743% (7981/32256)\n",
            "Loss: 2.067 | Acc: 24.790% (8012/32320)\n",
            "Loss: 2.067 | Acc: 24.790% (8028/32384)\n",
            "Loss: 2.066 | Acc: 24.809% (8050/32448)\n",
            "Loss: 2.066 | Acc: 24.831% (8073/32512)\n",
            "Loss: 2.065 | Acc: 24.825% (8087/32576)\n",
            "Loss: 2.065 | Acc: 24.822% (8102/32640)\n",
            "Loss: 2.065 | Acc: 24.838% (8123/32704)\n",
            "Loss: 2.065 | Acc: 24.838% (8139/32768)\n",
            "Loss: 2.065 | Acc: 24.842% (8156/32832)\n",
            "Loss: 2.064 | Acc: 24.854% (8176/32896)\n",
            "Loss: 2.064 | Acc: 24.882% (8201/32960)\n",
            "Loss: 2.064 | Acc: 24.882% (8217/33024)\n",
            "Loss: 2.063 | Acc: 24.897% (8238/33088)\n",
            "Loss: 2.063 | Acc: 24.906% (8257/33152)\n",
            "Loss: 2.063 | Acc: 24.907% (8273/33216)\n",
            "Loss: 2.063 | Acc: 24.898% (8286/33280)\n",
            "Loss: 2.062 | Acc: 24.919% (8309/33344)\n",
            "Loss: 2.062 | Acc: 24.952% (8336/33408)\n",
            "Loss: 2.061 | Acc: 24.973% (8359/33472)\n",
            "Loss: 2.061 | Acc: 24.985% (8379/33536)\n",
            "Loss: 2.061 | Acc: 24.991% (8397/33600)\n",
            "Loss: 2.060 | Acc: 25.009% (8419/33664)\n",
            "Loss: 2.060 | Acc: 25.021% (8439/33728)\n",
            "Loss: 2.060 | Acc: 25.021% (8455/33792)\n",
            "Loss: 2.060 | Acc: 25.015% (8469/33856)\n",
            "Loss: 2.059 | Acc: 25.021% (8487/33920)\n",
            "Loss: 2.059 | Acc: 25.029% (8506/33984)\n",
            "Loss: 2.058 | Acc: 25.038% (8525/34048)\n",
            "Loss: 2.058 | Acc: 25.056% (8547/34112)\n",
            "Loss: 2.057 | Acc: 25.067% (8567/34176)\n",
            "Loss: 2.057 | Acc: 25.079% (8587/34240)\n",
            "Loss: 2.056 | Acc: 25.105% (8612/34304)\n",
            "Loss: 2.055 | Acc: 25.131% (8637/34368)\n",
            "Loss: 2.055 | Acc: 25.137% (8655/34432)\n",
            "Loss: 2.055 | Acc: 25.136% (8671/34496)\n",
            "Loss: 2.054 | Acc: 25.139% (8688/34560)\n",
            "Loss: 2.054 | Acc: 25.170% (8715/34624)\n",
            "Loss: 2.053 | Acc: 25.196% (8740/34688)\n",
            "Loss: 2.053 | Acc: 25.210% (8761/34752)\n",
            "Loss: 2.052 | Acc: 25.218% (8780/34816)\n",
            "Loss: 2.053 | Acc: 25.215% (8795/34880)\n",
            "Loss: 2.052 | Acc: 25.223% (8814/34944)\n",
            "Loss: 2.052 | Acc: 25.229% (8832/35008)\n",
            "Loss: 2.051 | Acc: 25.240% (8852/35072)\n",
            "Loss: 2.051 | Acc: 25.231% (8865/35136)\n",
            "Loss: 2.051 | Acc: 25.244% (8886/35200)\n",
            "Loss: 2.050 | Acc: 25.244% (8902/35264)\n",
            "Loss: 2.050 | Acc: 25.252% (8921/35328)\n",
            "Loss: 2.050 | Acc: 25.266% (8942/35392)\n",
            "Loss: 2.050 | Acc: 25.262% (8957/35456)\n",
            "Loss: 2.049 | Acc: 25.276% (8978/35520)\n",
            "Loss: 2.049 | Acc: 25.306% (9005/35584)\n",
            "Loss: 2.048 | Acc: 25.317% (9025/35648)\n",
            "Loss: 2.048 | Acc: 25.333% (9047/35712)\n",
            "Loss: 2.047 | Acc: 25.335% (9064/35776)\n",
            "Loss: 2.047 | Acc: 25.335% (9080/35840)\n",
            "Loss: 2.047 | Acc: 25.343% (9099/35904)\n",
            "Loss: 2.046 | Acc: 25.353% (9119/35968)\n",
            "Loss: 2.046 | Acc: 25.369% (9141/36032)\n",
            "Loss: 2.045 | Acc: 25.382% (9162/36096)\n",
            "Loss: 2.045 | Acc: 25.407% (9187/36160)\n",
            "Loss: 2.044 | Acc: 25.428% (9211/36224)\n",
            "Loss: 2.044 | Acc: 25.444% (9233/36288)\n",
            "Loss: 2.043 | Acc: 25.462% (9256/36352)\n",
            "Loss: 2.043 | Acc: 25.467% (9274/36416)\n",
            "Loss: 2.042 | Acc: 25.515% (9308/36480)\n",
            "Loss: 2.042 | Acc: 25.539% (9333/36544)\n",
            "Loss: 2.041 | Acc: 25.555% (9355/36608)\n",
            "Loss: 2.041 | Acc: 25.564% (9375/36672)\n",
            "Loss: 2.040 | Acc: 25.585% (9399/36736)\n",
            "Loss: 2.040 | Acc: 25.609% (9424/36800)\n",
            "Loss: 2.039 | Acc: 25.616% (9443/36864)\n",
            "Loss: 2.038 | Acc: 25.628% (9464/36928)\n",
            "Loss: 2.038 | Acc: 25.649% (9488/36992)\n",
            "Loss: 2.038 | Acc: 25.653% (9506/37056)\n",
            "Loss: 2.037 | Acc: 25.657% (9524/37120)\n",
            "Loss: 2.037 | Acc: 25.664% (9543/37184)\n",
            "Loss: 2.037 | Acc: 25.652% (9555/37248)\n",
            "Loss: 2.037 | Acc: 25.651% (9571/37312)\n",
            "Loss: 2.037 | Acc: 25.650% (9587/37376)\n",
            "Loss: 2.036 | Acc: 25.654% (9605/37440)\n",
            "Loss: 2.036 | Acc: 25.661% (9624/37504)\n",
            "Loss: 2.035 | Acc: 25.679% (9647/37568)\n",
            "Loss: 2.035 | Acc: 25.694% (9669/37632)\n",
            "Loss: 2.035 | Acc: 25.708% (9691/37696)\n",
            "Loss: 2.034 | Acc: 25.731% (9716/37760)\n",
            "Loss: 2.033 | Acc: 25.748% (9739/37824)\n",
            "Loss: 2.033 | Acc: 25.750% (9756/37888)\n",
            "Loss: 2.033 | Acc: 25.756% (9775/37952)\n",
            "Loss: 2.033 | Acc: 25.765% (9795/38016)\n",
            "Loss: 2.032 | Acc: 25.777% (9816/38080)\n",
            "Loss: 2.032 | Acc: 25.800% (9841/38144)\n",
            "Loss: 2.032 | Acc: 25.819% (9865/38208)\n",
            "Loss: 2.031 | Acc: 25.841% (9890/38272)\n",
            "Loss: 2.031 | Acc: 25.837% (9905/38336)\n",
            "Loss: 2.031 | Acc: 25.844% (9924/38400)\n",
            "Loss: 2.031 | Acc: 25.853% (9944/38464)\n",
            "Loss: 2.030 | Acc: 25.862% (9964/38528)\n",
            "Loss: 2.030 | Acc: 25.878% (9987/38592)\n",
            "Loss: 2.030 | Acc: 25.887% (10007/38656)\n",
            "Loss: 2.029 | Acc: 25.896% (10027/38720)\n",
            "Loss: 2.029 | Acc: 25.900% (10045/38784)\n",
            "Loss: 2.029 | Acc: 25.911% (10066/38848)\n",
            "Loss: 2.029 | Acc: 25.928% (10089/38912)\n",
            "Loss: 2.028 | Acc: 25.949% (10114/38976)\n",
            "Loss: 2.028 | Acc: 25.958% (10134/39040)\n",
            "Loss: 2.028 | Acc: 25.974% (10157/39104)\n",
            "Loss: 2.027 | Acc: 26.001% (10184/39168)\n",
            "Loss: 2.027 | Acc: 25.999% (10200/39232)\n",
            "Loss: 2.027 | Acc: 26.015% (10223/39296)\n",
            "Loss: 2.026 | Acc: 26.029% (10245/39360)\n",
            "Loss: 2.026 | Acc: 26.043% (10267/39424)\n",
            "Loss: 2.025 | Acc: 26.059% (10290/39488)\n",
            "Loss: 2.025 | Acc: 26.067% (10310/39552)\n",
            "Loss: 2.025 | Acc: 26.073% (10329/39616)\n",
            "Loss: 2.025 | Acc: 26.051% (10337/39680)\n",
            "Loss: 2.024 | Acc: 26.059% (10357/39744)\n",
            "Loss: 2.024 | Acc: 26.073% (10379/39808)\n",
            "Loss: 2.023 | Acc: 26.078% (10398/39872)\n",
            "Loss: 2.023 | Acc: 26.084% (10417/39936)\n",
            "Loss: 2.023 | Acc: 26.093% (10437/40000)\n",
            "Loss: 2.023 | Acc: 26.083% (10450/40064)\n",
            "Loss: 2.022 | Acc: 26.084% (10467/40128)\n",
            "Loss: 2.022 | Acc: 26.090% (10486/40192)\n",
            "Loss: 2.022 | Acc: 26.088% (10502/40256)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.022 | Acc: 26.099% (10523/40320)\n",
            "Loss: 2.022 | Acc: 26.117% (10547/40384)\n",
            "Loss: 2.022 | Acc: 26.115% (10563/40448)\n",
            "Loss: 2.022 | Acc: 26.121% (10582/40512)\n",
            "Loss: 2.021 | Acc: 26.131% (10603/40576)\n",
            "Loss: 2.021 | Acc: 26.152% (10628/40640)\n",
            "Loss: 2.021 | Acc: 26.155% (10646/40704)\n",
            "Loss: 2.020 | Acc: 26.170% (10669/40768)\n",
            "Loss: 2.020 | Acc: 26.190% (10694/40832)\n",
            "Loss: 2.019 | Acc: 26.215% (10721/40896)\n",
            "Loss: 2.019 | Acc: 26.223% (10741/40960)\n",
            "Loss: 2.019 | Acc: 26.229% (10760/41024)\n",
            "Loss: 2.019 | Acc: 26.232% (10778/41088)\n",
            "Loss: 2.018 | Acc: 26.247% (10801/41152)\n",
            "Loss: 2.018 | Acc: 26.262% (10824/41216)\n",
            "Loss: 2.018 | Acc: 26.267% (10843/41280)\n",
            "Loss: 2.017 | Acc: 26.267% (10860/41344)\n",
            "Loss: 2.017 | Acc: 26.270% (10878/41408)\n",
            "Loss: 2.017 | Acc: 26.271% (10895/41472)\n",
            "Loss: 2.017 | Acc: 26.262% (10908/41536)\n",
            "Loss: 2.017 | Acc: 26.262% (10925/41600)\n",
            "Loss: 2.017 | Acc: 26.267% (10944/41664)\n",
            "Loss: 2.017 | Acc: 26.263% (10959/41728)\n",
            "Loss: 2.016 | Acc: 26.261% (10975/41792)\n",
            "Loss: 2.016 | Acc: 26.273% (10997/41856)\n",
            "Loss: 2.016 | Acc: 26.269% (11012/41920)\n",
            "Loss: 2.016 | Acc: 26.277% (11032/41984)\n",
            "Loss: 2.016 | Acc: 26.287% (11053/42048)\n",
            "Loss: 2.016 | Acc: 26.282% (11068/42112)\n",
            "Loss: 2.015 | Acc: 26.299% (11092/42176)\n",
            "Loss: 2.015 | Acc: 26.295% (11107/42240)\n",
            "Loss: 2.015 | Acc: 26.302% (11127/42304)\n",
            "Loss: 2.014 | Acc: 26.308% (11146/42368)\n",
            "Loss: 2.014 | Acc: 26.315% (11166/42432)\n",
            "Loss: 2.014 | Acc: 26.320% (11185/42496)\n",
            "Loss: 2.014 | Acc: 26.309% (11197/42560)\n",
            "Loss: 2.013 | Acc: 26.330% (11223/42624)\n",
            "Loss: 2.012 | Acc: 26.347% (11247/42688)\n",
            "Loss: 2.012 | Acc: 26.361% (11270/42752)\n",
            "Loss: 2.012 | Acc: 26.380% (11295/42816)\n",
            "Loss: 2.012 | Acc: 26.381% (11312/42880)\n",
            "Loss: 2.011 | Acc: 26.388% (11332/42944)\n",
            "Loss: 2.011 | Acc: 26.386% (11348/43008)\n",
            "Loss: 2.011 | Acc: 26.400% (11371/43072)\n",
            "Loss: 2.011 | Acc: 26.409% (11392/43136)\n",
            "Loss: 2.011 | Acc: 26.410% (11409/43200)\n",
            "Loss: 2.010 | Acc: 26.417% (11429/43264)\n",
            "Loss: 2.010 | Acc: 26.424% (11449/43328)\n",
            "Loss: 2.010 | Acc: 26.438% (11472/43392)\n",
            "Loss: 2.009 | Acc: 26.452% (11495/43456)\n",
            "Loss: 2.009 | Acc: 26.464% (11517/43520)\n",
            "Loss: 2.009 | Acc: 26.478% (11540/43584)\n",
            "Loss: 2.009 | Acc: 26.491% (11563/43648)\n",
            "Loss: 2.008 | Acc: 26.498% (11583/43712)\n",
            "Loss: 2.008 | Acc: 26.510% (11605/43776)\n",
            "Loss: 2.008 | Acc: 26.494% (11615/43840)\n",
            "Loss: 2.008 | Acc: 26.499% (11634/43904)\n",
            "Loss: 2.008 | Acc: 26.499% (11651/43968)\n",
            "Loss: 2.008 | Acc: 26.506% (11671/44032)\n",
            "Loss: 2.008 | Acc: 26.508% (11689/44096)\n",
            "Loss: 2.007 | Acc: 26.517% (11710/44160)\n",
            "Loss: 2.007 | Acc: 26.538% (11736/44224)\n",
            "Loss: 2.007 | Acc: 26.553% (11760/44288)\n",
            "Loss: 2.006 | Acc: 26.558% (11779/44352)\n",
            "Loss: 2.006 | Acc: 26.572% (11802/44416)\n",
            "Loss: 2.006 | Acc: 26.589% (11827/44480)\n",
            "Loss: 2.005 | Acc: 26.603% (11850/44544)\n",
            "Loss: 2.005 | Acc: 26.621% (11875/44608)\n",
            "Loss: 2.005 | Acc: 26.641% (11901/44672)\n",
            "Loss: 2.004 | Acc: 26.641% (11918/44736)\n",
            "Loss: 2.004 | Acc: 26.650% (11939/44800)\n",
            "Loss: 2.004 | Acc: 26.658% (11960/44864)\n",
            "Loss: 2.004 | Acc: 26.669% (11982/44928)\n",
            "Loss: 2.004 | Acc: 26.683% (12005/44992)\n",
            "Loss: 2.003 | Acc: 26.707% (12033/45056)\n",
            "Loss: 2.003 | Acc: 26.707% (12050/45120)\n",
            "Loss: 2.003 | Acc: 26.717% (12072/45184)\n",
            "Loss: 2.002 | Acc: 26.730% (12095/45248)\n",
            "Loss: 2.002 | Acc: 26.743% (12118/45312)\n",
            "Loss: 2.002 | Acc: 26.734% (12131/45376)\n",
            "Loss: 2.002 | Acc: 26.739% (12150/45440)\n",
            "Loss: 2.002 | Acc: 26.727% (12162/45504)\n",
            "Loss: 2.001 | Acc: 26.740% (12185/45568)\n",
            "Loss: 2.001 | Acc: 26.736% (12200/45632)\n",
            "Loss: 2.001 | Acc: 26.746% (12222/45696)\n",
            "Loss: 2.001 | Acc: 26.746% (12239/45760)\n",
            "Loss: 2.000 | Acc: 26.763% (12264/45824)\n",
            "Loss: 2.000 | Acc: 26.780% (12289/45888)\n",
            "Loss: 1.999 | Acc: 26.795% (12313/45952)\n",
            "Loss: 1.999 | Acc: 26.817% (12340/46016)\n",
            "Loss: 1.998 | Acc: 26.836% (12366/46080)\n",
            "Loss: 1.998 | Acc: 26.855% (12392/46144)\n",
            "Loss: 1.997 | Acc: 26.876% (12419/46208)\n",
            "Loss: 1.997 | Acc: 26.895% (12445/46272)\n",
            "Loss: 1.997 | Acc: 26.888% (12459/46336)\n",
            "Loss: 1.996 | Acc: 26.888% (12476/46400)\n",
            "Loss: 1.996 | Acc: 26.907% (12502/46464)\n",
            "Loss: 1.996 | Acc: 26.909% (12520/46528)\n",
            "Loss: 1.995 | Acc: 26.919% (12542/46592)\n",
            "Loss: 1.995 | Acc: 26.920% (12560/46656)\n",
            "Loss: 1.995 | Acc: 26.926% (12580/46720)\n",
            "Loss: 1.994 | Acc: 26.939% (12603/46784)\n",
            "Loss: 1.994 | Acc: 26.953% (12627/46848)\n",
            "Loss: 1.994 | Acc: 26.959% (12647/46912)\n",
            "Loss: 1.993 | Acc: 26.978% (12673/46976)\n",
            "Loss: 1.993 | Acc: 26.990% (12696/47040)\n",
            "Loss: 1.993 | Acc: 26.993% (12715/47104)\n",
            "Loss: 1.993 | Acc: 26.999% (12735/47168)\n",
            "Loss: 1.993 | Acc: 27.011% (12758/47232)\n",
            "Loss: 1.992 | Acc: 27.028% (12783/47296)\n",
            "Loss: 1.992 | Acc: 27.042% (12807/47360)\n",
            "Loss: 1.992 | Acc: 27.035% (12821/47424)\n",
            "Loss: 1.991 | Acc: 27.053% (12847/47488)\n",
            "Loss: 1.991 | Acc: 27.059% (12867/47552)\n",
            "Loss: 1.991 | Acc: 27.081% (12895/47616)\n",
            "Loss: 1.990 | Acc: 27.091% (12917/47680)\n",
            "Loss: 1.990 | Acc: 27.084% (12931/47744)\n",
            "Loss: 1.990 | Acc: 27.094% (12953/47808)\n",
            "Loss: 1.990 | Acc: 27.116% (12981/47872)\n",
            "Loss: 1.989 | Acc: 27.124% (13002/47936)\n",
            "Loss: 1.989 | Acc: 27.127% (13021/48000)\n",
            "Loss: 1.989 | Acc: 27.147% (13048/48064)\n",
            "Loss: 1.989 | Acc: 27.144% (13064/48128)\n",
            "Loss: 1.989 | Acc: 27.162% (13090/48192)\n",
            "Loss: 1.988 | Acc: 27.161% (13107/48256)\n",
            "Loss: 1.988 | Acc: 27.167% (13127/48320)\n",
            "Loss: 1.988 | Acc: 27.172% (13147/48384)\n",
            "Loss: 1.988 | Acc: 27.184% (13170/48448)\n",
            "Loss: 1.988 | Acc: 27.199% (13195/48512)\n",
            "Loss: 1.988 | Acc: 27.205% (13215/48576)\n",
            "Loss: 1.988 | Acc: 27.206% (13233/48640)\n",
            "Loss: 1.988 | Acc: 27.203% (13249/48704)\n",
            "Loss: 1.988 | Acc: 27.210% (13270/48768)\n",
            "Loss: 1.987 | Acc: 27.222% (13293/48832)\n",
            "Loss: 1.987 | Acc: 27.223% (13311/48896)\n",
            "Loss: 1.987 | Acc: 27.241% (13337/48960)\n",
            "Loss: 1.987 | Acc: 27.245% (13350/49000)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 27.244897959183675\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.759 | Acc: 35.938% (23/64)\n",
            "Loss: 1.751 | Acc: 35.156% (45/128)\n",
            "Loss: 1.761 | Acc: 34.375% (66/192)\n",
            "Loss: 1.812 | Acc: 32.812% (84/256)\n",
            "Loss: 1.783 | Acc: 33.438% (107/320)\n",
            "Loss: 1.801 | Acc: 32.812% (126/384)\n",
            "Loss: 1.834 | Acc: 30.804% (138/448)\n",
            "Loss: 1.833 | Acc: 30.664% (157/512)\n",
            "Loss: 1.824 | Acc: 31.250% (180/576)\n",
            "Loss: 1.811 | Acc: 32.188% (206/640)\n",
            "Loss: 1.819 | Acc: 31.392% (221/704)\n",
            "Loss: 1.819 | Acc: 31.120% (239/768)\n",
            "Loss: 1.825 | Acc: 30.649% (255/832)\n",
            "Loss: 1.821 | Acc: 30.915% (277/896)\n",
            "Loss: 1.814 | Acc: 31.354% (301/960)\n",
            "Loss: 1.807 | Acc: 32.129% (329/1024)\n",
            "Loss: 1.809 | Acc: 32.721% (356/1088)\n",
            "Loss: 1.809 | Acc: 33.160% (382/1152)\n",
            "Loss: 1.805 | Acc: 33.635% (409/1216)\n",
            "Loss: 1.809 | Acc: 33.125% (424/1280)\n",
            "Loss: 1.805 | Acc: 33.259% (447/1344)\n",
            "Loss: 1.799 | Acc: 33.452% (471/1408)\n",
            "Loss: 1.794 | Acc: 33.356% (491/1472)\n",
            "Loss: 1.793 | Acc: 33.268% (511/1536)\n",
            "Loss: 1.791 | Acc: 33.375% (534/1600)\n",
            "Loss: 1.792 | Acc: 33.293% (554/1664)\n",
            "Loss: 1.793 | Acc: 33.333% (576/1728)\n",
            "Loss: 1.791 | Acc: 33.315% (597/1792)\n",
            "Loss: 1.791 | Acc: 33.513% (622/1856)\n",
            "Loss: 1.792 | Acc: 33.490% (643/1920)\n",
            "Loss: 1.788 | Acc: 33.569% (666/1984)\n",
            "Loss: 1.791 | Acc: 33.350% (683/2048)\n",
            "Loss: 1.790 | Acc: 33.381% (705/2112)\n",
            "Loss: 1.791 | Acc: 33.364% (726/2176)\n",
            "Loss: 1.792 | Acc: 33.438% (749/2240)\n",
            "Loss: 1.791 | Acc: 33.464% (771/2304)\n",
            "Loss: 1.791 | Acc: 33.319% (789/2368)\n",
            "Loss: 1.791 | Acc: 33.347% (811/2432)\n",
            "Loss: 1.788 | Acc: 33.574% (838/2496)\n",
            "Loss: 1.791 | Acc: 33.398% (855/2560)\n",
            "Loss: 1.790 | Acc: 33.537% (880/2624)\n",
            "Loss: 1.790 | Acc: 33.668% (905/2688)\n",
            "Loss: 1.789 | Acc: 33.576% (924/2752)\n",
            "Loss: 1.792 | Acc: 33.381% (940/2816)\n",
            "Loss: 1.793 | Acc: 33.368% (961/2880)\n",
            "Loss: 1.791 | Acc: 33.458% (985/2944)\n",
            "Loss: 1.792 | Acc: 33.211% (999/3008)\n",
            "Loss: 1.791 | Acc: 33.203% (1020/3072)\n",
            "Loss: 1.790 | Acc: 33.355% (1046/3136)\n",
            "Loss: 1.785 | Acc: 33.594% (1075/3200)\n",
            "Loss: 1.786 | Acc: 33.456% (1092/3264)\n",
            "Loss: 1.785 | Acc: 33.383% (1111/3328)\n",
            "Loss: 1.785 | Acc: 33.284% (1129/3392)\n",
            "Loss: 1.786 | Acc: 33.189% (1147/3456)\n",
            "Loss: 1.788 | Acc: 33.068% (1164/3520)\n",
            "Loss: 1.785 | Acc: 33.259% (1192/3584)\n",
            "Loss: 1.785 | Acc: 33.251% (1213/3648)\n",
            "Loss: 1.783 | Acc: 33.244% (1234/3712)\n",
            "Loss: 1.783 | Acc: 33.342% (1259/3776)\n",
            "Loss: 1.782 | Acc: 33.333% (1280/3840)\n",
            "Loss: 1.780 | Acc: 33.402% (1304/3904)\n",
            "Loss: 1.779 | Acc: 33.468% (1328/3968)\n",
            "Loss: 1.781 | Acc: 33.433% (1348/4032)\n",
            "Loss: 1.784 | Acc: 33.276% (1363/4096)\n",
            "Loss: 1.788 | Acc: 33.101% (1377/4160)\n",
            "Loss: 1.786 | Acc: 33.239% (1404/4224)\n",
            "Loss: 1.787 | Acc: 33.186% (1423/4288)\n",
            "Loss: 1.785 | Acc: 33.318% (1450/4352)\n",
            "Loss: 1.784 | Acc: 33.424% (1476/4416)\n",
            "Loss: 1.781 | Acc: 33.549% (1503/4480)\n",
            "Loss: 1.782 | Acc: 33.561% (1525/4544)\n",
            "Loss: 1.783 | Acc: 33.659% (1551/4608)\n",
            "Loss: 1.783 | Acc: 33.583% (1569/4672)\n",
            "Loss: 1.782 | Acc: 33.699% (1596/4736)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.782 | Acc: 33.729% (1619/4800)\n",
            "Loss: 1.780 | Acc: 33.758% (1642/4864)\n",
            "Loss: 1.780 | Acc: 33.787% (1665/4928)\n",
            "Loss: 1.779 | Acc: 33.734% (1684/4992)\n",
            "Loss: 1.779 | Acc: 33.683% (1703/5056)\n",
            "Loss: 1.780 | Acc: 33.711% (1726/5120)\n",
            "Loss: 1.780 | Acc: 33.758% (1750/5184)\n",
            "Loss: 1.781 | Acc: 33.689% (1768/5248)\n",
            "Loss: 1.780 | Acc: 33.735% (1792/5312)\n",
            "Loss: 1.782 | Acc: 33.575% (1805/5376)\n",
            "Loss: 1.783 | Acc: 33.621% (1829/5440)\n",
            "Loss: 1.783 | Acc: 33.576% (1848/5504)\n",
            "Loss: 1.784 | Acc: 33.495% (1865/5568)\n",
            "Loss: 1.785 | Acc: 33.452% (1884/5632)\n",
            "Loss: 1.786 | Acc: 33.357% (1900/5696)\n",
            "Loss: 1.784 | Acc: 33.490% (1929/5760)\n",
            "Loss: 1.783 | Acc: 33.448% (1948/5824)\n",
            "Loss: 1.784 | Acc: 33.390% (1966/5888)\n",
            "Loss: 1.783 | Acc: 33.468% (1992/5952)\n",
            "Loss: 1.783 | Acc: 33.527% (2017/6016)\n",
            "Loss: 1.784 | Acc: 33.503% (2037/6080)\n",
            "Loss: 1.784 | Acc: 33.512% (2059/6144)\n",
            "Loss: 1.784 | Acc: 33.505% (2080/6208)\n",
            "Loss: 1.786 | Acc: 33.482% (2100/6272)\n",
            "Loss: 1.786 | Acc: 33.475% (2121/6336)\n",
            "Loss: 1.786 | Acc: 33.484% (2143/6400)\n",
            "Loss: 1.787 | Acc: 33.478% (2164/6464)\n",
            "Loss: 1.787 | Acc: 33.487% (2186/6528)\n",
            "Loss: 1.788 | Acc: 33.419% (2203/6592)\n",
            "Loss: 1.788 | Acc: 33.383% (2222/6656)\n",
            "Loss: 1.789 | Acc: 33.348% (2241/6720)\n",
            "Loss: 1.789 | Acc: 33.240% (2255/6784)\n",
            "Loss: 1.786 | Acc: 33.309% (2281/6848)\n",
            "Loss: 1.788 | Acc: 33.218% (2296/6912)\n",
            "Loss: 1.789 | Acc: 33.142% (2312/6976)\n",
            "Loss: 1.791 | Acc: 33.040% (2326/7040)\n",
            "Loss: 1.791 | Acc: 33.010% (2345/7104)\n",
            "Loss: 1.792 | Acc: 33.036% (2368/7168)\n",
            "Loss: 1.793 | Acc: 32.965% (2384/7232)\n",
            "Loss: 1.792 | Acc: 32.991% (2407/7296)\n",
            "Loss: 1.791 | Acc: 33.071% (2434/7360)\n",
            "Loss: 1.791 | Acc: 33.109% (2458/7424)\n",
            "Loss: 1.790 | Acc: 33.160% (2483/7488)\n",
            "Loss: 1.790 | Acc: 33.197% (2507/7552)\n",
            "Loss: 1.791 | Acc: 33.154% (2525/7616)\n",
            "Loss: 1.790 | Acc: 33.190% (2549/7680)\n",
            "Loss: 1.789 | Acc: 33.264% (2576/7744)\n",
            "Loss: 1.789 | Acc: 33.299% (2600/7808)\n",
            "Loss: 1.789 | Acc: 33.270% (2619/7872)\n",
            "Loss: 1.788 | Acc: 33.291% (2642/7936)\n",
            "Loss: 1.788 | Acc: 33.263% (2661/8000)\n",
            "Loss: 1.788 | Acc: 33.296% (2685/8064)\n",
            "Loss: 1.788 | Acc: 33.268% (2704/8128)\n",
            "Loss: 1.787 | Acc: 33.240% (2723/8192)\n",
            "Loss: 1.788 | Acc: 33.236% (2744/8256)\n",
            "Loss: 1.789 | Acc: 33.161% (2759/8320)\n",
            "Loss: 1.789 | Acc: 33.194% (2783/8384)\n",
            "Loss: 1.789 | Acc: 33.215% (2806/8448)\n",
            "Loss: 1.790 | Acc: 33.165% (2823/8512)\n",
            "Loss: 1.790 | Acc: 33.197% (2847/8576)\n",
            "Loss: 1.790 | Acc: 33.252% (2873/8640)\n",
            "Loss: 1.790 | Acc: 33.261% (2895/8704)\n",
            "Loss: 1.790 | Acc: 33.223% (2913/8768)\n",
            "Loss: 1.790 | Acc: 33.277% (2939/8832)\n",
            "Loss: 1.789 | Acc: 33.296% (2962/8896)\n",
            "Loss: 1.788 | Acc: 33.371% (2990/8960)\n",
            "Loss: 1.789 | Acc: 33.344% (3009/9024)\n",
            "Loss: 1.789 | Acc: 33.319% (3028/9088)\n",
            "Loss: 1.789 | Acc: 33.239% (3042/9152)\n",
            "Loss: 1.788 | Acc: 33.257% (3065/9216)\n",
            "Loss: 1.788 | Acc: 33.265% (3087/9280)\n",
            "Loss: 1.789 | Acc: 33.241% (3106/9344)\n",
            "Loss: 1.790 | Acc: 33.206% (3124/9408)\n",
            "Loss: 1.791 | Acc: 33.150% (3140/9472)\n",
            "Loss: 1.790 | Acc: 33.180% (3164/9536)\n",
            "Loss: 1.790 | Acc: 33.250% (3192/9600)\n",
            "Loss: 1.789 | Acc: 33.299% (3218/9664)\n",
            "Loss: 1.789 | Acc: 33.296% (3239/9728)\n",
            "Loss: 1.790 | Acc: 33.211% (3252/9792)\n",
            "Loss: 1.789 | Acc: 33.208% (3273/9856)\n",
            "Loss: 1.789 | Acc: 33.185% (3292/9920)\n",
            "Loss: 1.789 | Acc: 33.163% (3311/9984)\n",
            "Loss: 1.790 | Acc: 33.120% (3312/10000)\n",
            "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 33.12\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 1.833 | Acc: 29.688% (19/64)\n",
            "Loss: 1.759 | Acc: 35.938% (46/128)\n",
            "Loss: 1.806 | Acc: 32.812% (63/192)\n",
            "Loss: 1.798 | Acc: 31.641% (81/256)\n",
            "Loss: 1.819 | Acc: 30.938% (99/320)\n",
            "Loss: 1.824 | Acc: 30.469% (117/384)\n",
            "Loss: 1.849 | Acc: 30.580% (137/448)\n",
            "Loss: 1.847 | Acc: 31.250% (160/512)\n",
            "Loss: 1.849 | Acc: 31.424% (181/576)\n",
            "Loss: 1.853 | Acc: 30.781% (197/640)\n",
            "Loss: 1.865 | Acc: 30.824% (217/704)\n",
            "Loss: 1.847 | Acc: 30.990% (238/768)\n",
            "Loss: 1.841 | Acc: 31.250% (260/832)\n",
            "Loss: 1.839 | Acc: 30.915% (277/896)\n",
            "Loss: 1.834 | Acc: 30.833% (296/960)\n",
            "Loss: 1.832 | Acc: 30.957% (317/1024)\n",
            "Loss: 1.834 | Acc: 31.342% (341/1088)\n",
            "Loss: 1.825 | Acc: 31.337% (361/1152)\n",
            "Loss: 1.824 | Acc: 31.250% (380/1216)\n",
            "Loss: 1.827 | Acc: 31.094% (398/1280)\n",
            "Loss: 1.827 | Acc: 31.696% (426/1344)\n",
            "Loss: 1.820 | Acc: 32.173% (453/1408)\n",
            "Loss: 1.824 | Acc: 31.997% (471/1472)\n",
            "Loss: 1.819 | Acc: 32.161% (494/1536)\n",
            "Loss: 1.825 | Acc: 31.875% (510/1600)\n",
            "Loss: 1.824 | Acc: 31.791% (529/1664)\n",
            "Loss: 1.819 | Acc: 31.944% (552/1728)\n",
            "Loss: 1.815 | Acc: 31.920% (572/1792)\n",
            "Loss: 1.821 | Acc: 31.735% (589/1856)\n",
            "Loss: 1.816 | Acc: 32.031% (615/1920)\n",
            "Loss: 1.816 | Acc: 32.208% (639/1984)\n",
            "Loss: 1.815 | Acc: 32.324% (662/2048)\n",
            "Loss: 1.816 | Acc: 32.434% (685/2112)\n",
            "Loss: 1.813 | Acc: 32.537% (708/2176)\n",
            "Loss: 1.814 | Acc: 32.545% (729/2240)\n",
            "Loss: 1.810 | Acc: 32.812% (756/2304)\n",
            "Loss: 1.805 | Acc: 32.770% (776/2368)\n",
            "Loss: 1.806 | Acc: 32.771% (797/2432)\n",
            "Loss: 1.812 | Acc: 32.732% (817/2496)\n",
            "Loss: 1.811 | Acc: 32.695% (837/2560)\n",
            "Loss: 1.812 | Acc: 32.736% (859/2624)\n",
            "Loss: 1.815 | Acc: 32.626% (877/2688)\n",
            "Loss: 1.813 | Acc: 32.849% (904/2752)\n",
            "Loss: 1.815 | Acc: 32.812% (924/2816)\n",
            "Loss: 1.813 | Acc: 32.882% (947/2880)\n",
            "Loss: 1.812 | Acc: 32.846% (967/2944)\n",
            "Loss: 1.813 | Acc: 32.746% (985/3008)\n",
            "Loss: 1.812 | Acc: 32.617% (1002/3072)\n",
            "Loss: 1.817 | Acc: 32.589% (1022/3136)\n",
            "Loss: 1.815 | Acc: 32.625% (1044/3200)\n",
            "Loss: 1.814 | Acc: 32.659% (1066/3264)\n",
            "Loss: 1.815 | Acc: 32.512% (1082/3328)\n",
            "Loss: 1.817 | Acc: 32.459% (1101/3392)\n",
            "Loss: 1.819 | Acc: 32.494% (1123/3456)\n",
            "Loss: 1.819 | Acc: 32.670% (1150/3520)\n",
            "Loss: 1.819 | Acc: 32.701% (1172/3584)\n",
            "Loss: 1.819 | Acc: 32.785% (1196/3648)\n",
            "Loss: 1.820 | Acc: 32.759% (1216/3712)\n",
            "Loss: 1.819 | Acc: 32.733% (1236/3776)\n",
            "Loss: 1.821 | Acc: 32.604% (1252/3840)\n",
            "Loss: 1.822 | Acc: 32.403% (1265/3904)\n",
            "Loss: 1.822 | Acc: 32.334% (1283/3968)\n",
            "Loss: 1.821 | Acc: 32.391% (1306/4032)\n",
            "Loss: 1.822 | Acc: 32.349% (1325/4096)\n",
            "Loss: 1.821 | Acc: 32.356% (1346/4160)\n",
            "Loss: 1.822 | Acc: 32.292% (1364/4224)\n",
            "Loss: 1.820 | Acc: 32.369% (1388/4288)\n",
            "Loss: 1.823 | Acc: 32.123% (1398/4352)\n",
            "Loss: 1.823 | Acc: 32.043% (1415/4416)\n",
            "Loss: 1.822 | Acc: 32.054% (1436/4480)\n",
            "Loss: 1.821 | Acc: 32.108% (1459/4544)\n",
            "Loss: 1.820 | Acc: 32.140% (1481/4608)\n",
            "Loss: 1.820 | Acc: 32.235% (1506/4672)\n",
            "Loss: 1.818 | Acc: 32.348% (1532/4736)\n",
            "Loss: 1.820 | Acc: 32.188% (1545/4800)\n",
            "Loss: 1.820 | Acc: 32.155% (1564/4864)\n",
            "Loss: 1.817 | Acc: 32.244% (1589/4928)\n",
            "Loss: 1.816 | Acc: 32.272% (1611/4992)\n",
            "Loss: 1.815 | Acc: 32.417% (1639/5056)\n",
            "Loss: 1.815 | Acc: 32.324% (1655/5120)\n",
            "Loss: 1.816 | Acc: 32.311% (1675/5184)\n",
            "Loss: 1.814 | Acc: 32.355% (1698/5248)\n",
            "Loss: 1.814 | Acc: 32.304% (1716/5312)\n",
            "Loss: 1.813 | Acc: 32.347% (1739/5376)\n",
            "Loss: 1.812 | Acc: 32.371% (1761/5440)\n",
            "Loss: 1.813 | Acc: 32.249% (1775/5504)\n",
            "Loss: 1.813 | Acc: 32.256% (1796/5568)\n",
            "Loss: 1.812 | Acc: 32.404% (1825/5632)\n",
            "Loss: 1.811 | Acc: 32.356% (1843/5696)\n",
            "Loss: 1.810 | Acc: 32.378% (1865/5760)\n",
            "Loss: 1.812 | Acc: 32.315% (1882/5824)\n",
            "Loss: 1.811 | Acc: 32.371% (1906/5888)\n",
            "Loss: 1.812 | Acc: 32.342% (1925/5952)\n",
            "Loss: 1.812 | Acc: 32.281% (1942/6016)\n",
            "Loss: 1.813 | Acc: 32.270% (1962/6080)\n",
            "Loss: 1.811 | Acc: 32.324% (1986/6144)\n",
            "Loss: 1.812 | Acc: 32.313% (2006/6208)\n",
            "Loss: 1.812 | Acc: 32.334% (2028/6272)\n",
            "Loss: 1.812 | Acc: 32.339% (2049/6336)\n",
            "Loss: 1.811 | Acc: 32.297% (2067/6400)\n",
            "Loss: 1.810 | Acc: 32.457% (2098/6464)\n",
            "Loss: 1.810 | Acc: 32.460% (2119/6528)\n",
            "Loss: 1.809 | Acc: 32.585% (2148/6592)\n",
            "Loss: 1.808 | Acc: 32.617% (2171/6656)\n",
            "Loss: 1.807 | Acc: 32.693% (2197/6720)\n",
            "Loss: 1.807 | Acc: 32.709% (2219/6784)\n",
            "Loss: 1.806 | Acc: 32.710% (2240/6848)\n",
            "Loss: 1.805 | Acc: 32.711% (2261/6912)\n",
            "Loss: 1.807 | Acc: 32.683% (2280/6976)\n",
            "Loss: 1.809 | Acc: 32.614% (2296/7040)\n",
            "Loss: 1.806 | Acc: 32.700% (2323/7104)\n",
            "Loss: 1.805 | Acc: 32.743% (2347/7168)\n",
            "Loss: 1.806 | Acc: 32.702% (2365/7232)\n",
            "Loss: 1.804 | Acc: 32.648% (2382/7296)\n",
            "Loss: 1.805 | Acc: 32.663% (2404/7360)\n",
            "Loss: 1.805 | Acc: 32.691% (2427/7424)\n",
            "Loss: 1.805 | Acc: 32.692% (2448/7488)\n",
            "Loss: 1.805 | Acc: 32.654% (2466/7552)\n",
            "Loss: 1.804 | Acc: 32.681% (2489/7616)\n",
            "Loss: 1.804 | Acc: 32.708% (2512/7680)\n",
            "Loss: 1.802 | Acc: 32.774% (2538/7744)\n",
            "Loss: 1.803 | Acc: 32.761% (2558/7808)\n",
            "Loss: 1.803 | Acc: 32.774% (2580/7872)\n",
            "Loss: 1.803 | Acc: 32.775% (2601/7936)\n",
            "Loss: 1.803 | Acc: 32.725% (2618/8000)\n",
            "Loss: 1.802 | Acc: 32.688% (2636/8064)\n",
            "Loss: 1.803 | Acc: 32.665% (2655/8128)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.804 | Acc: 32.654% (2675/8192)\n",
            "Loss: 1.803 | Acc: 32.728% (2702/8256)\n",
            "Loss: 1.804 | Acc: 32.692% (2720/8320)\n",
            "Loss: 1.803 | Acc: 32.717% (2743/8384)\n",
            "Loss: 1.802 | Acc: 32.801% (2771/8448)\n",
            "Loss: 1.802 | Acc: 32.777% (2790/8512)\n",
            "Loss: 1.802 | Acc: 32.743% (2808/8576)\n",
            "Loss: 1.801 | Acc: 32.731% (2828/8640)\n",
            "Loss: 1.801 | Acc: 32.663% (2843/8704)\n",
            "Loss: 1.801 | Acc: 32.619% (2860/8768)\n",
            "Loss: 1.802 | Acc: 32.597% (2879/8832)\n",
            "Loss: 1.800 | Acc: 32.621% (2902/8896)\n",
            "Loss: 1.800 | Acc: 32.634% (2924/8960)\n",
            "Loss: 1.802 | Acc: 32.646% (2946/9024)\n",
            "Loss: 1.800 | Acc: 32.669% (2969/9088)\n",
            "Loss: 1.799 | Acc: 32.692% (2992/9152)\n",
            "Loss: 1.800 | Acc: 32.661% (3010/9216)\n",
            "Loss: 1.801 | Acc: 32.597% (3025/9280)\n",
            "Loss: 1.801 | Acc: 32.577% (3044/9344)\n",
            "Loss: 1.800 | Acc: 32.547% (3062/9408)\n",
            "Loss: 1.800 | Acc: 32.538% (3082/9472)\n",
            "Loss: 1.800 | Acc: 32.508% (3100/9536)\n",
            "Loss: 1.800 | Acc: 32.552% (3125/9600)\n",
            "Loss: 1.800 | Acc: 32.554% (3146/9664)\n",
            "Loss: 1.799 | Acc: 32.627% (3174/9728)\n",
            "Loss: 1.799 | Acc: 32.690% (3201/9792)\n",
            "Loss: 1.800 | Acc: 32.691% (3222/9856)\n",
            "Loss: 1.799 | Acc: 32.702% (3244/9920)\n",
            "Loss: 1.799 | Acc: 32.692% (3264/9984)\n",
            "Loss: 1.799 | Acc: 32.703% (3286/10048)\n",
            "Loss: 1.799 | Acc: 32.753% (3312/10112)\n",
            "Loss: 1.799 | Acc: 32.724% (3330/10176)\n",
            "Loss: 1.798 | Acc: 32.715% (3350/10240)\n",
            "Loss: 1.797 | Acc: 32.783% (3378/10304)\n",
            "Loss: 1.797 | Acc: 32.812% (3402/10368)\n",
            "Loss: 1.797 | Acc: 32.793% (3421/10432)\n",
            "Loss: 1.797 | Acc: 32.812% (3444/10496)\n",
            "Loss: 1.797 | Acc: 32.822% (3466/10560)\n",
            "Loss: 1.796 | Acc: 32.841% (3489/10624)\n",
            "Loss: 1.796 | Acc: 32.887% (3515/10688)\n",
            "Loss: 1.796 | Acc: 32.822% (3529/10752)\n",
            "Loss: 1.796 | Acc: 32.803% (3548/10816)\n",
            "Loss: 1.794 | Acc: 32.877% (3577/10880)\n",
            "Loss: 1.792 | Acc: 32.913% (3602/10944)\n",
            "Loss: 1.791 | Acc: 32.949% (3627/11008)\n",
            "Loss: 1.792 | Acc: 32.975% (3651/11072)\n",
            "Loss: 1.791 | Acc: 33.010% (3676/11136)\n",
            "Loss: 1.793 | Acc: 32.946% (3690/11200)\n",
            "Loss: 1.793 | Acc: 32.999% (3717/11264)\n",
            "Loss: 1.793 | Acc: 32.998% (3738/11328)\n",
            "Loss: 1.793 | Acc: 33.014% (3761/11392)\n",
            "Loss: 1.792 | Acc: 33.092% (3791/11456)\n",
            "Loss: 1.793 | Acc: 33.073% (3810/11520)\n",
            "Loss: 1.793 | Acc: 33.097% (3834/11584)\n",
            "Loss: 1.792 | Acc: 33.104% (3856/11648)\n",
            "Loss: 1.792 | Acc: 33.094% (3876/11712)\n",
            "Loss: 1.791 | Acc: 33.093% (3897/11776)\n",
            "Loss: 1.791 | Acc: 33.091% (3918/11840)\n",
            "Loss: 1.790 | Acc: 33.140% (3945/11904)\n",
            "Loss: 1.791 | Acc: 33.138% (3966/11968)\n",
            "Loss: 1.791 | Acc: 33.137% (3987/12032)\n",
            "Loss: 1.791 | Acc: 33.135% (4008/12096)\n",
            "Loss: 1.791 | Acc: 33.150% (4031/12160)\n",
            "Loss: 1.792 | Acc: 33.148% (4052/12224)\n",
            "Loss: 1.791 | Acc: 33.236% (4084/12288)\n",
            "Loss: 1.790 | Acc: 33.233% (4105/12352)\n",
            "Loss: 1.790 | Acc: 33.223% (4125/12416)\n",
            "Loss: 1.790 | Acc: 33.221% (4146/12480)\n",
            "Loss: 1.789 | Acc: 33.243% (4170/12544)\n",
            "Loss: 1.790 | Acc: 33.280% (4196/12608)\n",
            "Loss: 1.790 | Acc: 33.325% (4223/12672)\n",
            "Loss: 1.789 | Acc: 33.339% (4246/12736)\n",
            "Loss: 1.789 | Acc: 33.312% (4264/12800)\n",
            "Loss: 1.788 | Acc: 33.326% (4287/12864)\n",
            "Loss: 1.789 | Acc: 33.269% (4301/12928)\n",
            "Loss: 1.789 | Acc: 33.282% (4324/12992)\n",
            "Loss: 1.790 | Acc: 33.257% (4342/13056)\n",
            "Loss: 1.789 | Acc: 33.247% (4362/13120)\n",
            "Loss: 1.788 | Acc: 33.252% (4384/13184)\n",
            "Loss: 1.788 | Acc: 33.243% (4404/13248)\n",
            "Loss: 1.788 | Acc: 33.286% (4431/13312)\n",
            "Loss: 1.787 | Acc: 33.351% (4461/13376)\n",
            "Loss: 1.787 | Acc: 33.393% (4488/13440)\n",
            "Loss: 1.786 | Acc: 33.442% (4516/13504)\n",
            "Loss: 1.785 | Acc: 33.432% (4536/13568)\n",
            "Loss: 1.785 | Acc: 33.465% (4562/13632)\n",
            "Loss: 1.784 | Acc: 33.477% (4585/13696)\n",
            "Loss: 1.784 | Acc: 33.488% (4608/13760)\n",
            "Loss: 1.784 | Acc: 33.464% (4626/13824)\n",
            "Loss: 1.784 | Acc: 33.482% (4650/13888)\n",
            "Loss: 1.784 | Acc: 33.508% (4675/13952)\n",
            "Loss: 1.783 | Acc: 33.533% (4700/14016)\n",
            "Loss: 1.783 | Acc: 33.558% (4725/14080)\n",
            "Loss: 1.781 | Acc: 33.633% (4757/14144)\n",
            "Loss: 1.780 | Acc: 33.650% (4781/14208)\n",
            "Loss: 1.780 | Acc: 33.674% (4806/14272)\n",
            "Loss: 1.780 | Acc: 33.691% (4830/14336)\n",
            "Loss: 1.781 | Acc: 33.604% (4839/14400)\n",
            "Loss: 1.781 | Acc: 33.580% (4857/14464)\n",
            "Loss: 1.782 | Acc: 33.563% (4876/14528)\n",
            "Loss: 1.782 | Acc: 33.559% (4897/14592)\n",
            "Loss: 1.782 | Acc: 33.529% (4914/14656)\n",
            "Loss: 1.782 | Acc: 33.519% (4934/14720)\n",
            "Loss: 1.781 | Acc: 33.604% (4968/14784)\n",
            "Loss: 1.781 | Acc: 33.587% (4987/14848)\n",
            "Loss: 1.781 | Acc: 33.597% (5010/14912)\n",
            "Loss: 1.781 | Acc: 33.607% (5033/14976)\n",
            "Loss: 1.781 | Acc: 33.604% (5054/15040)\n",
            "Loss: 1.782 | Acc: 33.587% (5073/15104)\n",
            "Loss: 1.782 | Acc: 33.557% (5090/15168)\n",
            "Loss: 1.782 | Acc: 33.567% (5113/15232)\n",
            "Loss: 1.782 | Acc: 33.571% (5135/15296)\n",
            "Loss: 1.782 | Acc: 33.561% (5155/15360)\n",
            "Loss: 1.782 | Acc: 33.591% (5181/15424)\n",
            "Loss: 1.782 | Acc: 33.632% (5209/15488)\n",
            "Loss: 1.781 | Acc: 33.668% (5236/15552)\n",
            "Loss: 1.781 | Acc: 33.677% (5259/15616)\n",
            "Loss: 1.781 | Acc: 33.680% (5281/15680)\n",
            "Loss: 1.781 | Acc: 33.702% (5306/15744)\n",
            "Loss: 1.781 | Acc: 33.685% (5325/15808)\n",
            "Loss: 1.781 | Acc: 33.726% (5353/15872)\n",
            "Loss: 1.782 | Acc: 33.691% (5369/15936)\n",
            "Loss: 1.781 | Acc: 33.712% (5394/16000)\n",
            "Loss: 1.781 | Acc: 33.715% (5416/16064)\n",
            "Loss: 1.781 | Acc: 33.705% (5436/16128)\n",
            "Loss: 1.781 | Acc: 33.683% (5454/16192)\n",
            "Loss: 1.781 | Acc: 33.686% (5476/16256)\n",
            "Loss: 1.779 | Acc: 33.732% (5505/16320)\n",
            "Loss: 1.779 | Acc: 33.722% (5525/16384)\n",
            "Loss: 1.779 | Acc: 33.749% (5551/16448)\n",
            "Loss: 1.780 | Acc: 33.727% (5569/16512)\n",
            "Loss: 1.779 | Acc: 33.748% (5594/16576)\n",
            "Loss: 1.779 | Acc: 33.750% (5616/16640)\n",
            "Loss: 1.779 | Acc: 33.770% (5641/16704)\n",
            "Loss: 1.779 | Acc: 33.755% (5660/16768)\n",
            "Loss: 1.779 | Acc: 33.775% (5685/16832)\n",
            "Loss: 1.779 | Acc: 33.759% (5704/16896)\n",
            "Loss: 1.779 | Acc: 33.744% (5723/16960)\n",
            "Loss: 1.779 | Acc: 33.735% (5743/17024)\n",
            "Loss: 1.779 | Acc: 33.714% (5761/17088)\n",
            "Loss: 1.779 | Acc: 33.699% (5780/17152)\n",
            "Loss: 1.779 | Acc: 33.730% (5807/17216)\n",
            "Loss: 1.779 | Acc: 33.721% (5827/17280)\n",
            "Loss: 1.779 | Acc: 33.706% (5846/17344)\n",
            "Loss: 1.779 | Acc: 33.703% (5867/17408)\n",
            "Loss: 1.779 | Acc: 33.705% (5889/17472)\n",
            "Loss: 1.780 | Acc: 33.691% (5908/17536)\n",
            "Loss: 1.779 | Acc: 33.682% (5928/17600)\n",
            "Loss: 1.779 | Acc: 33.684% (5950/17664)\n",
            "Loss: 1.779 | Acc: 33.676% (5970/17728)\n",
            "Loss: 1.779 | Acc: 33.695% (5995/17792)\n",
            "Loss: 1.779 | Acc: 33.692% (6016/17856)\n",
            "Loss: 1.779 | Acc: 33.694% (6038/17920)\n",
            "Loss: 1.778 | Acc: 33.702% (6061/17984)\n",
            "Loss: 1.779 | Acc: 33.699% (6082/18048)\n",
            "Loss: 1.779 | Acc: 33.735% (6110/18112)\n",
            "Loss: 1.779 | Acc: 33.742% (6133/18176)\n",
            "Loss: 1.778 | Acc: 33.755% (6157/18240)\n",
            "Loss: 1.779 | Acc: 33.774% (6182/18304)\n",
            "Loss: 1.778 | Acc: 33.776% (6204/18368)\n",
            "Loss: 1.778 | Acc: 33.794% (6229/18432)\n",
            "Loss: 1.778 | Acc: 33.769% (6246/18496)\n",
            "Loss: 1.778 | Acc: 33.739% (6262/18560)\n",
            "Loss: 1.778 | Acc: 33.731% (6282/18624)\n",
            "Loss: 1.779 | Acc: 33.717% (6301/18688)\n",
            "Loss: 1.779 | Acc: 33.735% (6326/18752)\n",
            "Loss: 1.779 | Acc: 33.753% (6351/18816)\n",
            "Loss: 1.779 | Acc: 33.713% (6365/18880)\n",
            "Loss: 1.779 | Acc: 33.757% (6395/18944)\n",
            "Loss: 1.778 | Acc: 33.781% (6421/19008)\n",
            "Loss: 1.778 | Acc: 33.756% (6438/19072)\n",
            "Loss: 1.778 | Acc: 33.790% (6466/19136)\n",
            "Loss: 1.778 | Acc: 33.776% (6485/19200)\n",
            "Loss: 1.779 | Acc: 33.773% (6506/19264)\n",
            "Loss: 1.779 | Acc: 33.749% (6523/19328)\n",
            "Loss: 1.779 | Acc: 33.751% (6545/19392)\n",
            "Loss: 1.778 | Acc: 33.784% (6573/19456)\n",
            "Loss: 1.778 | Acc: 33.806% (6599/19520)\n",
            "Loss: 1.778 | Acc: 33.788% (6617/19584)\n",
            "Loss: 1.778 | Acc: 33.769% (6635/19648)\n",
            "Loss: 1.778 | Acc: 33.776% (6658/19712)\n",
            "Loss: 1.778 | Acc: 33.804% (6685/19776)\n",
            "Loss: 1.778 | Acc: 33.805% (6707/19840)\n",
            "Loss: 1.778 | Acc: 33.792% (6726/19904)\n",
            "Loss: 1.778 | Acc: 33.774% (6744/19968)\n",
            "Loss: 1.779 | Acc: 33.761% (6763/20032)\n",
            "Loss: 1.778 | Acc: 33.778% (6788/20096)\n",
            "Loss: 1.778 | Acc: 33.810% (6816/20160)\n",
            "Loss: 1.777 | Acc: 33.811% (6838/20224)\n",
            "Loss: 1.778 | Acc: 33.793% (6856/20288)\n",
            "Loss: 1.778 | Acc: 33.805% (6880/20352)\n",
            "Loss: 1.779 | Acc: 33.792% (6899/20416)\n",
            "Loss: 1.779 | Acc: 33.779% (6918/20480)\n",
            "Loss: 1.779 | Acc: 33.752% (6934/20544)\n",
            "Loss: 1.780 | Acc: 33.739% (6953/20608)\n",
            "Loss: 1.780 | Acc: 33.717% (6970/20672)\n",
            "Loss: 1.780 | Acc: 33.748% (6998/20736)\n",
            "Loss: 1.781 | Acc: 33.716% (7013/20800)\n",
            "Loss: 1.781 | Acc: 33.718% (7035/20864)\n",
            "Loss: 1.781 | Acc: 33.701% (7053/20928)\n",
            "Loss: 1.782 | Acc: 33.713% (7077/20992)\n",
            "Loss: 1.782 | Acc: 33.691% (7094/21056)\n",
            "Loss: 1.782 | Acc: 33.665% (7110/21120)\n",
            "Loss: 1.782 | Acc: 33.676% (7134/21184)\n",
            "Loss: 1.783 | Acc: 33.655% (7151/21248)\n",
            "Loss: 1.783 | Acc: 33.671% (7176/21312)\n",
            "Loss: 1.783 | Acc: 33.669% (7197/21376)\n",
            "Loss: 1.783 | Acc: 33.647% (7214/21440)\n",
            "Loss: 1.783 | Acc: 33.650% (7236/21504)\n",
            "Loss: 1.782 | Acc: 33.680% (7264/21568)\n",
            "Loss: 1.781 | Acc: 33.691% (7288/21632)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.781 | Acc: 33.684% (7308/21696)\n",
            "Loss: 1.781 | Acc: 33.718% (7337/21760)\n",
            "Loss: 1.781 | Acc: 33.711% (7357/21824)\n",
            "Loss: 1.781 | Acc: 33.690% (7374/21888)\n",
            "Loss: 1.780 | Acc: 33.719% (7402/21952)\n",
            "Loss: 1.780 | Acc: 33.725% (7425/22016)\n",
            "Loss: 1.779 | Acc: 33.750% (7452/22080)\n",
            "Loss: 1.779 | Acc: 33.752% (7474/22144)\n",
            "Loss: 1.779 | Acc: 33.758% (7497/22208)\n",
            "Loss: 1.779 | Acc: 33.773% (7522/22272)\n",
            "Loss: 1.778 | Acc: 33.797% (7549/22336)\n",
            "Loss: 1.778 | Acc: 33.826% (7577/22400)\n",
            "Loss: 1.778 | Acc: 33.810% (7595/22464)\n",
            "Loss: 1.777 | Acc: 33.847% (7625/22528)\n",
            "Loss: 1.776 | Acc: 33.870% (7652/22592)\n",
            "Loss: 1.776 | Acc: 33.894% (7679/22656)\n",
            "Loss: 1.776 | Acc: 33.895% (7701/22720)\n",
            "Loss: 1.776 | Acc: 33.901% (7724/22784)\n",
            "Loss: 1.776 | Acc: 33.915% (7749/22848)\n",
            "Loss: 1.776 | Acc: 33.917% (7771/22912)\n",
            "Loss: 1.776 | Acc: 33.940% (7798/22976)\n",
            "Loss: 1.775 | Acc: 33.963% (7825/23040)\n",
            "Loss: 1.775 | Acc: 33.947% (7843/23104)\n",
            "Loss: 1.775 | Acc: 33.974% (7871/23168)\n",
            "Loss: 1.775 | Acc: 33.996% (7898/23232)\n",
            "Loss: 1.774 | Acc: 34.002% (7921/23296)\n",
            "Loss: 1.775 | Acc: 33.985% (7939/23360)\n",
            "Loss: 1.774 | Acc: 34.025% (7970/23424)\n",
            "Loss: 1.774 | Acc: 34.051% (7998/23488)\n",
            "Loss: 1.774 | Acc: 34.061% (8022/23552)\n",
            "Loss: 1.773 | Acc: 34.062% (8044/23616)\n",
            "Loss: 1.773 | Acc: 34.084% (8071/23680)\n",
            "Loss: 1.773 | Acc: 34.080% (8092/23744)\n",
            "Loss: 1.774 | Acc: 34.081% (8114/23808)\n",
            "Loss: 1.774 | Acc: 34.069% (8133/23872)\n",
            "Loss: 1.774 | Acc: 34.074% (8156/23936)\n",
            "Loss: 1.774 | Acc: 34.067% (8176/24000)\n",
            "Loss: 1.774 | Acc: 34.072% (8199/24064)\n",
            "Loss: 1.775 | Acc: 34.056% (8217/24128)\n",
            "Loss: 1.775 | Acc: 34.057% (8239/24192)\n",
            "Loss: 1.775 | Acc: 34.053% (8260/24256)\n",
            "Loss: 1.775 | Acc: 34.042% (8279/24320)\n",
            "Loss: 1.774 | Acc: 34.047% (8302/24384)\n",
            "Loss: 1.775 | Acc: 34.023% (8318/24448)\n",
            "Loss: 1.775 | Acc: 34.016% (8338/24512)\n",
            "Loss: 1.775 | Acc: 34.029% (8363/24576)\n",
            "Loss: 1.775 | Acc: 34.026% (8384/24640)\n",
            "Loss: 1.775 | Acc: 34.015% (8403/24704)\n",
            "Loss: 1.776 | Acc: 34.000% (8421/24768)\n",
            "Loss: 1.775 | Acc: 34.013% (8446/24832)\n",
            "Loss: 1.775 | Acc: 34.038% (8474/24896)\n",
            "Loss: 1.775 | Acc: 34.038% (8496/24960)\n",
            "Loss: 1.775 | Acc: 34.031% (8516/25024)\n",
            "Loss: 1.775 | Acc: 34.004% (8531/25088)\n",
            "Loss: 1.775 | Acc: 34.009% (8554/25152)\n",
            "Loss: 1.776 | Acc: 34.006% (8575/25216)\n",
            "Loss: 1.776 | Acc: 34.015% (8599/25280)\n",
            "Loss: 1.775 | Acc: 34.036% (8626/25344)\n",
            "Loss: 1.776 | Acc: 34.017% (8643/25408)\n",
            "Loss: 1.776 | Acc: 33.994% (8659/25472)\n",
            "Loss: 1.776 | Acc: 34.034% (8691/25536)\n",
            "Loss: 1.776 | Acc: 34.020% (8709/25600)\n",
            "Loss: 1.776 | Acc: 34.024% (8732/25664)\n",
            "Loss: 1.776 | Acc: 34.010% (8750/25728)\n",
            "Loss: 1.776 | Acc: 34.030% (8777/25792)\n",
            "Loss: 1.776 | Acc: 34.019% (8796/25856)\n",
            "Loss: 1.776 | Acc: 33.997% (8812/25920)\n",
            "Loss: 1.776 | Acc: 34.009% (8837/25984)\n",
            "Loss: 1.776 | Acc: 34.003% (8857/26048)\n",
            "Loss: 1.776 | Acc: 34.011% (8881/26112)\n",
            "Loss: 1.776 | Acc: 34.012% (8903/26176)\n",
            "Loss: 1.776 | Acc: 34.032% (8930/26240)\n",
            "Loss: 1.776 | Acc: 34.006% (8945/26304)\n",
            "Loss: 1.776 | Acc: 33.996% (8964/26368)\n",
            "Loss: 1.775 | Acc: 34.004% (8988/26432)\n",
            "Loss: 1.775 | Acc: 33.986% (9005/26496)\n",
            "Loss: 1.775 | Acc: 33.972% (9023/26560)\n",
            "Loss: 1.775 | Acc: 33.954% (9040/26624)\n",
            "Loss: 1.775 | Acc: 33.955% (9062/26688)\n",
            "Loss: 1.775 | Acc: 33.979% (9090/26752)\n",
            "Loss: 1.774 | Acc: 33.983% (9113/26816)\n",
            "Loss: 1.775 | Acc: 33.977% (9133/26880)\n",
            "Loss: 1.774 | Acc: 34.000% (9161/26944)\n",
            "Loss: 1.774 | Acc: 33.997% (9182/27008)\n",
            "Loss: 1.774 | Acc: 34.013% (9208/27072)\n",
            "Loss: 1.774 | Acc: 34.003% (9227/27136)\n",
            "Loss: 1.774 | Acc: 34.004% (9249/27200)\n",
            "Loss: 1.774 | Acc: 34.008% (9272/27264)\n",
            "Loss: 1.773 | Acc: 34.020% (9297/27328)\n",
            "Loss: 1.774 | Acc: 33.988% (9310/27392)\n",
            "Loss: 1.774 | Acc: 33.978% (9329/27456)\n",
            "Loss: 1.773 | Acc: 33.979% (9351/27520)\n",
            "Loss: 1.773 | Acc: 33.973% (9371/27584)\n",
            "Loss: 1.773 | Acc: 33.974% (9393/27648)\n",
            "Loss: 1.773 | Acc: 33.996% (9421/27712)\n",
            "Loss: 1.773 | Acc: 33.975% (9437/27776)\n",
            "Loss: 1.773 | Acc: 33.980% (9460/27840)\n",
            "Loss: 1.773 | Acc: 33.959% (9476/27904)\n",
            "Loss: 1.773 | Acc: 33.967% (9500/27968)\n",
            "Loss: 1.773 | Acc: 33.958% (9519/28032)\n",
            "Loss: 1.773 | Acc: 33.944% (9537/28096)\n",
            "Loss: 1.773 | Acc: 33.945% (9559/28160)\n",
            "Loss: 1.773 | Acc: 33.968% (9587/28224)\n",
            "Loss: 1.773 | Acc: 33.954% (9605/28288)\n",
            "Loss: 1.773 | Acc: 33.966% (9630/28352)\n",
            "Loss: 1.773 | Acc: 33.984% (9657/28416)\n",
            "Loss: 1.773 | Acc: 33.968% (9674/28480)\n",
            "Loss: 1.773 | Acc: 33.965% (9695/28544)\n",
            "Loss: 1.773 | Acc: 33.980% (9721/28608)\n",
            "Loss: 1.772 | Acc: 34.005% (9750/28672)\n",
            "Loss: 1.772 | Acc: 34.010% (9773/28736)\n",
            "Loss: 1.772 | Acc: 34.014% (9796/28800)\n",
            "Loss: 1.772 | Acc: 34.011% (9817/28864)\n",
            "Loss: 1.772 | Acc: 34.022% (9842/28928)\n",
            "Loss: 1.771 | Acc: 34.075% (9879/28992)\n",
            "Loss: 1.771 | Acc: 34.089% (9905/29056)\n",
            "Loss: 1.771 | Acc: 34.083% (9925/29120)\n",
            "Loss: 1.771 | Acc: 34.094% (9950/29184)\n",
            "Loss: 1.770 | Acc: 34.108% (9976/29248)\n",
            "Loss: 1.770 | Acc: 34.095% (9994/29312)\n",
            "Loss: 1.770 | Acc: 34.109% (10020/29376)\n",
            "Loss: 1.769 | Acc: 34.130% (10048/29440)\n",
            "Loss: 1.769 | Acc: 34.134% (10071/29504)\n",
            "Loss: 1.769 | Acc: 34.131% (10092/29568)\n",
            "Loss: 1.769 | Acc: 34.156% (10121/29632)\n",
            "Loss: 1.768 | Acc: 34.156% (10143/29696)\n",
            "Loss: 1.769 | Acc: 34.147% (10162/29760)\n",
            "Loss: 1.768 | Acc: 34.174% (10192/29824)\n",
            "Loss: 1.768 | Acc: 34.184% (10217/29888)\n",
            "Loss: 1.768 | Acc: 34.195% (10242/29952)\n",
            "Loss: 1.768 | Acc: 34.198% (10265/30016)\n",
            "Loss: 1.767 | Acc: 34.192% (10285/30080)\n",
            "Loss: 1.767 | Acc: 34.206% (10311/30144)\n",
            "Loss: 1.767 | Acc: 34.223% (10338/30208)\n",
            "Loss: 1.767 | Acc: 34.223% (10360/30272)\n",
            "Loss: 1.767 | Acc: 34.233% (10385/30336)\n",
            "Loss: 1.767 | Acc: 34.237% (10408/30400)\n",
            "Loss: 1.768 | Acc: 34.214% (10423/30464)\n",
            "Loss: 1.768 | Acc: 34.201% (10441/30528)\n",
            "Loss: 1.768 | Acc: 34.198% (10462/30592)\n",
            "Loss: 1.768 | Acc: 34.205% (10486/30656)\n",
            "Loss: 1.768 | Acc: 34.206% (10508/30720)\n",
            "Loss: 1.768 | Acc: 34.206% (10530/30784)\n",
            "Loss: 1.767 | Acc: 34.206% (10552/30848)\n",
            "Loss: 1.767 | Acc: 34.213% (10576/30912)\n",
            "Loss: 1.767 | Acc: 34.214% (10598/30976)\n",
            "Loss: 1.767 | Acc: 34.211% (10619/31040)\n",
            "Loss: 1.767 | Acc: 34.195% (10636/31104)\n",
            "Loss: 1.767 | Acc: 34.202% (10660/31168)\n",
            "Loss: 1.767 | Acc: 34.228% (10690/31232)\n",
            "Loss: 1.766 | Acc: 34.241% (10716/31296)\n",
            "Loss: 1.766 | Acc: 34.251% (10741/31360)\n",
            "Loss: 1.766 | Acc: 34.270% (10769/31424)\n",
            "Loss: 1.766 | Acc: 34.280% (10794/31488)\n",
            "Loss: 1.766 | Acc: 34.302% (10823/31552)\n",
            "Loss: 1.766 | Acc: 34.315% (10849/31616)\n",
            "Loss: 1.766 | Acc: 34.312% (10870/31680)\n",
            "Loss: 1.765 | Acc: 34.331% (10898/31744)\n",
            "Loss: 1.766 | Acc: 34.325% (10918/31808)\n",
            "Loss: 1.766 | Acc: 34.319% (10938/31872)\n",
            "Loss: 1.766 | Acc: 34.331% (10964/31936)\n",
            "Loss: 1.766 | Acc: 34.316% (10981/32000)\n",
            "Loss: 1.765 | Acc: 34.325% (11006/32064)\n",
            "Loss: 1.765 | Acc: 34.331% (11030/32128)\n",
            "Loss: 1.765 | Acc: 34.328% (11051/32192)\n",
            "Loss: 1.765 | Acc: 34.319% (11070/32256)\n",
            "Loss: 1.765 | Acc: 34.325% (11094/32320)\n",
            "Loss: 1.764 | Acc: 34.326% (11116/32384)\n",
            "Loss: 1.765 | Acc: 34.320% (11136/32448)\n",
            "Loss: 1.765 | Acc: 34.313% (11156/32512)\n",
            "Loss: 1.765 | Acc: 34.317% (11179/32576)\n",
            "Loss: 1.765 | Acc: 34.320% (11202/32640)\n",
            "Loss: 1.765 | Acc: 34.317% (11223/32704)\n",
            "Loss: 1.765 | Acc: 34.305% (11241/32768)\n",
            "Loss: 1.765 | Acc: 34.317% (11267/32832)\n",
            "Loss: 1.765 | Acc: 34.326% (11292/32896)\n",
            "Loss: 1.764 | Acc: 34.357% (11324/32960)\n",
            "Loss: 1.764 | Acc: 34.354% (11345/33024)\n",
            "Loss: 1.765 | Acc: 34.336% (11361/33088)\n",
            "Loss: 1.764 | Acc: 34.336% (11383/33152)\n",
            "Loss: 1.764 | Acc: 34.336% (11405/33216)\n",
            "Loss: 1.764 | Acc: 34.339% (11428/33280)\n",
            "Loss: 1.764 | Acc: 34.351% (11454/33344)\n",
            "Loss: 1.764 | Acc: 34.369% (11482/33408)\n",
            "Loss: 1.764 | Acc: 34.366% (11503/33472)\n",
            "Loss: 1.765 | Acc: 34.360% (11523/33536)\n",
            "Loss: 1.765 | Acc: 34.357% (11544/33600)\n",
            "Loss: 1.764 | Acc: 34.354% (11565/33664)\n",
            "Loss: 1.764 | Acc: 34.357% (11588/33728)\n",
            "Loss: 1.764 | Acc: 34.372% (11615/33792)\n",
            "Loss: 1.764 | Acc: 34.381% (11640/33856)\n",
            "Loss: 1.764 | Acc: 34.390% (11665/33920)\n",
            "Loss: 1.764 | Acc: 34.387% (11686/33984)\n",
            "Loss: 1.764 | Acc: 34.384% (11707/34048)\n",
            "Loss: 1.764 | Acc: 34.381% (11728/34112)\n",
            "Loss: 1.764 | Acc: 34.381% (11750/34176)\n",
            "Loss: 1.764 | Acc: 34.407% (11781/34240)\n",
            "Loss: 1.763 | Acc: 34.416% (11806/34304)\n",
            "Loss: 1.763 | Acc: 34.407% (11825/34368)\n",
            "Loss: 1.763 | Acc: 34.401% (11845/34432)\n",
            "Loss: 1.763 | Acc: 34.389% (11863/34496)\n",
            "Loss: 1.763 | Acc: 34.392% (11886/34560)\n",
            "Loss: 1.763 | Acc: 34.421% (11918/34624)\n",
            "Loss: 1.762 | Acc: 34.421% (11940/34688)\n",
            "Loss: 1.762 | Acc: 34.435% (11967/34752)\n",
            "Loss: 1.762 | Acc: 34.421% (11984/34816)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.762 | Acc: 34.429% (12009/34880)\n",
            "Loss: 1.762 | Acc: 34.427% (12030/34944)\n",
            "Loss: 1.762 | Acc: 34.421% (12050/35008)\n",
            "Loss: 1.762 | Acc: 34.429% (12075/35072)\n",
            "Loss: 1.762 | Acc: 34.446% (12103/35136)\n",
            "Loss: 1.762 | Acc: 34.455% (12128/35200)\n",
            "Loss: 1.762 | Acc: 34.454% (12150/35264)\n",
            "Loss: 1.762 | Acc: 34.451% (12171/35328)\n",
            "Loss: 1.762 | Acc: 34.448% (12192/35392)\n",
            "Loss: 1.762 | Acc: 34.423% (12205/35456)\n",
            "Loss: 1.762 | Acc: 34.440% (12233/35520)\n",
            "Loss: 1.762 | Acc: 34.448% (12258/35584)\n",
            "Loss: 1.762 | Acc: 34.462% (12285/35648)\n",
            "Loss: 1.762 | Acc: 34.470% (12310/35712)\n",
            "Loss: 1.762 | Acc: 34.501% (12343/35776)\n",
            "Loss: 1.761 | Acc: 34.515% (12370/35840)\n",
            "Loss: 1.761 | Acc: 34.523% (12395/35904)\n",
            "Loss: 1.761 | Acc: 34.536% (12422/35968)\n",
            "Loss: 1.761 | Acc: 34.533% (12443/36032)\n",
            "Loss: 1.761 | Acc: 34.544% (12469/36096)\n",
            "Loss: 1.761 | Acc: 34.560% (12497/36160)\n",
            "Loss: 1.760 | Acc: 34.557% (12518/36224)\n",
            "Loss: 1.760 | Acc: 34.573% (12546/36288)\n",
            "Loss: 1.760 | Acc: 34.584% (12572/36352)\n",
            "Loss: 1.760 | Acc: 34.597% (12599/36416)\n",
            "Loss: 1.759 | Acc: 34.597% (12621/36480)\n",
            "Loss: 1.759 | Acc: 34.594% (12642/36544)\n",
            "Loss: 1.759 | Acc: 34.583% (12660/36608)\n",
            "Loss: 1.759 | Acc: 34.588% (12684/36672)\n",
            "Loss: 1.759 | Acc: 34.593% (12708/36736)\n",
            "Loss: 1.759 | Acc: 34.590% (12729/36800)\n",
            "Loss: 1.759 | Acc: 34.578% (12747/36864)\n",
            "Loss: 1.759 | Acc: 34.594% (12775/36928)\n",
            "Loss: 1.759 | Acc: 34.591% (12796/36992)\n",
            "Loss: 1.759 | Acc: 34.585% (12816/37056)\n",
            "Loss: 1.759 | Acc: 34.585% (12838/37120)\n",
            "Loss: 1.759 | Acc: 34.614% (12871/37184)\n",
            "Loss: 1.759 | Acc: 34.611% (12892/37248)\n",
            "Loss: 1.759 | Acc: 34.611% (12914/37312)\n",
            "Loss: 1.759 | Acc: 34.626% (12942/37376)\n",
            "Loss: 1.759 | Acc: 34.634% (12967/37440)\n",
            "Loss: 1.759 | Acc: 34.631% (12988/37504)\n",
            "Loss: 1.759 | Acc: 34.625% (13008/37568)\n",
            "Loss: 1.759 | Acc: 34.614% (13026/37632)\n",
            "Loss: 1.759 | Acc: 34.624% (13052/37696)\n",
            "Loss: 1.759 | Acc: 34.632% (13077/37760)\n",
            "Loss: 1.759 | Acc: 34.626% (13097/37824)\n",
            "Loss: 1.759 | Acc: 34.634% (13122/37888)\n",
            "Loss: 1.759 | Acc: 34.638% (13146/37952)\n",
            "Loss: 1.758 | Acc: 34.649% (13172/38016)\n",
            "Loss: 1.759 | Acc: 34.643% (13192/38080)\n",
            "Loss: 1.759 | Acc: 34.640% (13213/38144)\n",
            "Loss: 1.758 | Acc: 34.642% (13236/38208)\n",
            "Loss: 1.758 | Acc: 34.647% (13260/38272)\n",
            "Loss: 1.758 | Acc: 34.646% (13282/38336)\n",
            "Loss: 1.758 | Acc: 34.654% (13307/38400)\n",
            "Loss: 1.758 | Acc: 34.674% (13337/38464)\n",
            "Loss: 1.758 | Acc: 34.658% (13353/38528)\n",
            "Loss: 1.758 | Acc: 34.657% (13375/38592)\n",
            "Loss: 1.758 | Acc: 34.665% (13400/38656)\n",
            "Loss: 1.758 | Acc: 34.667% (13423/38720)\n",
            "Loss: 1.758 | Acc: 34.682% (13451/38784)\n",
            "Loss: 1.758 | Acc: 34.681% (13473/38848)\n",
            "Loss: 1.758 | Acc: 34.681% (13495/38912)\n",
            "Loss: 1.758 | Acc: 34.675% (13515/38976)\n",
            "Loss: 1.758 | Acc: 34.682% (13540/39040)\n",
            "Loss: 1.759 | Acc: 34.664% (13555/39104)\n",
            "Loss: 1.759 | Acc: 34.674% (13581/39168)\n",
            "Loss: 1.759 | Acc: 34.663% (13599/39232)\n",
            "Loss: 1.759 | Acc: 34.652% (13617/39296)\n",
            "Loss: 1.759 | Acc: 34.639% (13634/39360)\n",
            "Loss: 1.760 | Acc: 34.626% (13651/39424)\n",
            "Loss: 1.760 | Acc: 34.626% (13673/39488)\n",
            "Loss: 1.760 | Acc: 34.635% (13699/39552)\n",
            "Loss: 1.760 | Acc: 34.638% (13722/39616)\n",
            "Loss: 1.759 | Acc: 34.640% (13745/39680)\n",
            "Loss: 1.760 | Acc: 34.637% (13766/39744)\n",
            "Loss: 1.760 | Acc: 34.629% (13785/39808)\n",
            "Loss: 1.760 | Acc: 34.628% (13807/39872)\n",
            "Loss: 1.761 | Acc: 34.608% (13821/39936)\n",
            "Loss: 1.761 | Acc: 34.590% (13836/40000)\n",
            "Loss: 1.761 | Acc: 34.580% (13854/40064)\n",
            "Loss: 1.761 | Acc: 34.577% (13875/40128)\n",
            "Loss: 1.761 | Acc: 34.564% (13892/40192)\n",
            "Loss: 1.762 | Acc: 34.551% (13909/40256)\n",
            "Loss: 1.762 | Acc: 34.556% (13933/40320)\n",
            "Loss: 1.762 | Acc: 34.541% (13949/40384)\n",
            "Loss: 1.762 | Acc: 34.551% (13975/40448)\n",
            "Loss: 1.762 | Acc: 34.548% (13996/40512)\n",
            "Loss: 1.762 | Acc: 34.555% (14021/40576)\n",
            "Loss: 1.761 | Acc: 34.572% (14050/40640)\n",
            "Loss: 1.761 | Acc: 34.572% (14072/40704)\n",
            "Loss: 1.761 | Acc: 34.576% (14096/40768)\n",
            "Loss: 1.761 | Acc: 34.591% (14124/40832)\n",
            "Loss: 1.760 | Acc: 34.612% (14155/40896)\n",
            "Loss: 1.760 | Acc: 34.607% (14175/40960)\n",
            "Loss: 1.761 | Acc: 34.604% (14196/41024)\n",
            "Loss: 1.760 | Acc: 34.618% (14224/41088)\n",
            "Loss: 1.760 | Acc: 34.620% (14247/41152)\n",
            "Loss: 1.760 | Acc: 34.637% (14276/41216)\n",
            "Loss: 1.760 | Acc: 34.634% (14297/41280)\n",
            "Loss: 1.760 | Acc: 34.643% (14323/41344)\n",
            "Loss: 1.760 | Acc: 34.626% (14338/41408)\n",
            "Loss: 1.761 | Acc: 34.616% (14356/41472)\n",
            "Loss: 1.760 | Acc: 34.628% (14383/41536)\n",
            "Loss: 1.761 | Acc: 34.623% (14403/41600)\n",
            "Loss: 1.760 | Acc: 34.639% (14432/41664)\n",
            "Loss: 1.760 | Acc: 34.646% (14457/41728)\n",
            "Loss: 1.760 | Acc: 34.633% (14474/41792)\n",
            "Loss: 1.760 | Acc: 34.635% (14497/41856)\n",
            "Loss: 1.760 | Acc: 34.649% (14525/41920)\n",
            "Loss: 1.760 | Acc: 34.670% (14556/41984)\n",
            "Loss: 1.760 | Acc: 34.668% (14577/42048)\n",
            "Loss: 1.760 | Acc: 34.684% (14606/42112)\n",
            "Loss: 1.760 | Acc: 34.686% (14629/42176)\n",
            "Loss: 1.760 | Acc: 34.695% (14655/42240)\n",
            "Loss: 1.760 | Acc: 34.704% (14681/42304)\n",
            "Loss: 1.759 | Acc: 34.710% (14706/42368)\n",
            "Loss: 1.759 | Acc: 34.710% (14728/42432)\n",
            "Loss: 1.759 | Acc: 34.730% (14759/42496)\n",
            "Loss: 1.759 | Acc: 34.732% (14782/42560)\n",
            "Loss: 1.759 | Acc: 34.750% (14812/42624)\n",
            "Loss: 1.759 | Acc: 34.757% (14837/42688)\n",
            "Loss: 1.759 | Acc: 34.749% (14856/42752)\n",
            "Loss: 1.759 | Acc: 34.751% (14879/42816)\n",
            "Loss: 1.759 | Acc: 34.748% (14900/42880)\n",
            "Loss: 1.759 | Acc: 34.743% (14920/42944)\n",
            "Loss: 1.759 | Acc: 34.749% (14945/43008)\n",
            "Loss: 1.759 | Acc: 34.735% (14961/43072)\n",
            "Loss: 1.759 | Acc: 34.751% (14990/43136)\n",
            "Loss: 1.759 | Acc: 34.741% (15008/43200)\n",
            "Loss: 1.759 | Acc: 34.738% (15029/43264)\n",
            "Loss: 1.759 | Acc: 34.749% (15056/43328)\n",
            "Loss: 1.759 | Acc: 34.755% (15081/43392)\n",
            "Loss: 1.759 | Acc: 34.752% (15102/43456)\n",
            "Loss: 1.759 | Acc: 34.747% (15122/43520)\n",
            "Loss: 1.758 | Acc: 34.751% (15146/43584)\n",
            "Loss: 1.758 | Acc: 34.753% (15169/43648)\n",
            "Loss: 1.758 | Acc: 34.759% (15194/43712)\n",
            "Loss: 1.758 | Acc: 34.756% (15215/43776)\n",
            "Loss: 1.758 | Acc: 34.770% (15243/43840)\n",
            "Loss: 1.758 | Acc: 34.778% (15269/43904)\n",
            "Loss: 1.758 | Acc: 34.771% (15288/43968)\n",
            "Loss: 1.758 | Acc: 34.779% (15314/44032)\n",
            "Loss: 1.758 | Acc: 34.783% (15338/44096)\n",
            "Loss: 1.758 | Acc: 34.794% (15365/44160)\n",
            "Loss: 1.758 | Acc: 34.796% (15388/44224)\n",
            "Loss: 1.758 | Acc: 34.793% (15409/44288)\n",
            "Loss: 1.758 | Acc: 34.797% (15433/44352)\n",
            "Loss: 1.758 | Acc: 34.801% (15457/44416)\n",
            "Loss: 1.758 | Acc: 34.800% (15479/44480)\n",
            "Loss: 1.758 | Acc: 34.786% (15495/44544)\n",
            "Loss: 1.758 | Acc: 34.770% (15510/44608)\n",
            "Loss: 1.758 | Acc: 34.776% (15535/44672)\n",
            "Loss: 1.758 | Acc: 34.782% (15560/44736)\n",
            "Loss: 1.758 | Acc: 34.768% (15576/44800)\n",
            "Loss: 1.758 | Acc: 34.765% (15597/44864)\n",
            "Loss: 1.758 | Acc: 34.760% (15617/44928)\n",
            "Loss: 1.759 | Acc: 34.757% (15638/44992)\n",
            "Loss: 1.758 | Acc: 34.759% (15661/45056)\n",
            "Loss: 1.759 | Acc: 34.754% (15681/45120)\n",
            "Loss: 1.759 | Acc: 34.762% (15707/45184)\n",
            "Loss: 1.759 | Acc: 34.753% (15725/45248)\n",
            "Loss: 1.759 | Acc: 34.761% (15751/45312)\n",
            "Loss: 1.759 | Acc: 34.756% (15771/45376)\n",
            "Loss: 1.759 | Acc: 34.758% (15794/45440)\n",
            "Loss: 1.759 | Acc: 34.760% (15817/45504)\n",
            "Loss: 1.759 | Acc: 34.768% (15843/45568)\n",
            "Loss: 1.759 | Acc: 34.776% (15869/45632)\n",
            "Loss: 1.759 | Acc: 34.775% (15891/45696)\n",
            "Loss: 1.759 | Acc: 34.768% (15910/45760)\n",
            "Loss: 1.759 | Acc: 34.779% (15937/45824)\n",
            "Loss: 1.758 | Acc: 34.800% (15969/45888)\n",
            "Loss: 1.758 | Acc: 34.797% (15990/45952)\n",
            "Loss: 1.758 | Acc: 34.812% (16019/46016)\n",
            "Loss: 1.758 | Acc: 34.822% (16046/46080)\n",
            "Loss: 1.758 | Acc: 34.828% (16071/46144)\n",
            "Loss: 1.758 | Acc: 34.840% (16099/46208)\n",
            "Loss: 1.758 | Acc: 34.837% (16120/46272)\n",
            "Loss: 1.758 | Acc: 34.841% (16144/46336)\n",
            "Loss: 1.758 | Acc: 34.838% (16165/46400)\n",
            "Loss: 1.758 | Acc: 34.833% (16185/46464)\n",
            "Loss: 1.758 | Acc: 34.820% (16201/46528)\n",
            "Loss: 1.758 | Acc: 34.839% (16232/46592)\n",
            "Loss: 1.758 | Acc: 34.834% (16252/46656)\n",
            "Loss: 1.758 | Acc: 34.850% (16282/46720)\n",
            "Loss: 1.758 | Acc: 34.856% (16307/46784)\n",
            "Loss: 1.758 | Acc: 34.866% (16334/46848)\n",
            "Loss: 1.758 | Acc: 34.867% (16357/46912)\n",
            "Loss: 1.758 | Acc: 34.884% (16387/46976)\n",
            "Loss: 1.758 | Acc: 34.889% (16412/47040)\n",
            "Loss: 1.757 | Acc: 34.899% (16439/47104)\n",
            "Loss: 1.757 | Acc: 34.905% (16464/47168)\n",
            "Loss: 1.757 | Acc: 34.902% (16485/47232)\n",
            "Loss: 1.757 | Acc: 34.906% (16509/47296)\n",
            "Loss: 1.757 | Acc: 34.916% (16536/47360)\n",
            "Loss: 1.757 | Acc: 34.919% (16560/47424)\n",
            "Loss: 1.757 | Acc: 34.914% (16580/47488)\n",
            "Loss: 1.757 | Acc: 34.920% (16605/47552)\n",
            "Loss: 1.757 | Acc: 34.934% (16634/47616)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.757 | Acc: 34.933% (16656/47680)\n",
            "Loss: 1.757 | Acc: 34.911% (16668/47744)\n",
            "Loss: 1.757 | Acc: 34.902% (16686/47808)\n",
            "Loss: 1.757 | Acc: 34.916% (16715/47872)\n",
            "Loss: 1.757 | Acc: 34.913% (16736/47936)\n",
            "Loss: 1.757 | Acc: 34.913% (16758/48000)\n",
            "Loss: 1.757 | Acc: 34.920% (16784/48064)\n",
            "Loss: 1.757 | Acc: 34.928% (16810/48128)\n",
            "Loss: 1.757 | Acc: 34.931% (16834/48192)\n",
            "Loss: 1.757 | Acc: 34.939% (16860/48256)\n",
            "Loss: 1.757 | Acc: 34.942% (16884/48320)\n",
            "Loss: 1.757 | Acc: 34.941% (16906/48384)\n",
            "Loss: 1.757 | Acc: 34.947% (16931/48448)\n",
            "Loss: 1.756 | Acc: 34.960% (16960/48512)\n",
            "Loss: 1.757 | Acc: 34.951% (16978/48576)\n",
            "Loss: 1.756 | Acc: 34.959% (17004/48640)\n",
            "Loss: 1.757 | Acc: 34.958% (17026/48704)\n",
            "Loss: 1.756 | Acc: 34.964% (17051/48768)\n",
            "Loss: 1.756 | Acc: 34.969% (17076/48832)\n",
            "Loss: 1.756 | Acc: 34.970% (17099/48896)\n",
            "Loss: 1.756 | Acc: 34.967% (17120/48960)\n",
            "Loss: 1.756 | Acc: 34.965% (17133/49000)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 34.96530612244898\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.672 | Acc: 37.500% (24/64)\n",
            "Loss: 1.644 | Acc: 36.719% (47/128)\n",
            "Loss: 1.649 | Acc: 38.021% (73/192)\n",
            "Loss: 1.720 | Acc: 36.719% (94/256)\n",
            "Loss: 1.670 | Acc: 38.750% (124/320)\n",
            "Loss: 1.655 | Acc: 39.583% (152/384)\n",
            "Loss: 1.701 | Acc: 37.277% (167/448)\n",
            "Loss: 1.686 | Acc: 37.891% (194/512)\n",
            "Loss: 1.654 | Acc: 39.583% (228/576)\n",
            "Loss: 1.659 | Acc: 39.219% (251/640)\n",
            "Loss: 1.661 | Acc: 38.920% (274/704)\n",
            "Loss: 1.660 | Acc: 39.974% (307/768)\n",
            "Loss: 1.659 | Acc: 39.663% (330/832)\n",
            "Loss: 1.656 | Acc: 39.844% (357/896)\n",
            "Loss: 1.656 | Acc: 39.583% (380/960)\n",
            "Loss: 1.650 | Acc: 40.332% (413/1024)\n",
            "Loss: 1.649 | Acc: 40.901% (445/1088)\n",
            "Loss: 1.654 | Acc: 40.972% (472/1152)\n",
            "Loss: 1.657 | Acc: 41.201% (501/1216)\n",
            "Loss: 1.664 | Acc: 40.781% (522/1280)\n",
            "Loss: 1.662 | Acc: 40.327% (542/1344)\n",
            "Loss: 1.659 | Acc: 40.270% (567/1408)\n",
            "Loss: 1.651 | Acc: 40.217% (592/1472)\n",
            "Loss: 1.650 | Acc: 40.365% (620/1536)\n",
            "Loss: 1.652 | Acc: 40.250% (644/1600)\n",
            "Loss: 1.652 | Acc: 40.325% (671/1664)\n",
            "Loss: 1.654 | Acc: 40.509% (700/1728)\n",
            "Loss: 1.649 | Acc: 40.402% (724/1792)\n",
            "Loss: 1.653 | Acc: 40.356% (749/1856)\n",
            "Loss: 1.652 | Acc: 40.156% (771/1920)\n",
            "Loss: 1.650 | Acc: 40.272% (799/1984)\n",
            "Loss: 1.653 | Acc: 40.137% (822/2048)\n",
            "Loss: 1.649 | Acc: 40.436% (854/2112)\n",
            "Loss: 1.644 | Acc: 40.763% (887/2176)\n",
            "Loss: 1.647 | Acc: 40.357% (904/2240)\n",
            "Loss: 1.648 | Acc: 40.321% (929/2304)\n",
            "Loss: 1.649 | Acc: 40.329% (955/2368)\n",
            "Loss: 1.647 | Acc: 40.502% (985/2432)\n",
            "Loss: 1.646 | Acc: 40.665% (1015/2496)\n",
            "Loss: 1.652 | Acc: 40.391% (1034/2560)\n",
            "Loss: 1.655 | Acc: 40.053% (1051/2624)\n",
            "Loss: 1.656 | Acc: 39.993% (1075/2688)\n",
            "Loss: 1.656 | Acc: 39.935% (1099/2752)\n",
            "Loss: 1.656 | Acc: 39.773% (1120/2816)\n",
            "Loss: 1.660 | Acc: 39.688% (1143/2880)\n",
            "Loss: 1.657 | Acc: 39.810% (1172/2944)\n",
            "Loss: 1.661 | Acc: 39.628% (1192/3008)\n",
            "Loss: 1.658 | Acc: 39.811% (1223/3072)\n",
            "Loss: 1.655 | Acc: 39.732% (1246/3136)\n",
            "Loss: 1.651 | Acc: 39.906% (1277/3200)\n",
            "Loss: 1.653 | Acc: 39.798% (1299/3264)\n",
            "Loss: 1.654 | Acc: 39.724% (1322/3328)\n",
            "Loss: 1.653 | Acc: 39.800% (1350/3392)\n",
            "Loss: 1.657 | Acc: 39.641% (1370/3456)\n",
            "Loss: 1.658 | Acc: 39.602% (1394/3520)\n",
            "Loss: 1.657 | Acc: 39.704% (1423/3584)\n",
            "Loss: 1.657 | Acc: 39.583% (1444/3648)\n",
            "Loss: 1.654 | Acc: 39.655% (1472/3712)\n",
            "Loss: 1.655 | Acc: 39.539% (1493/3776)\n",
            "Loss: 1.656 | Acc: 39.479% (1516/3840)\n",
            "Loss: 1.655 | Acc: 39.524% (1543/3904)\n",
            "Loss: 1.655 | Acc: 39.516% (1568/3968)\n",
            "Loss: 1.655 | Acc: 39.583% (1596/4032)\n",
            "Loss: 1.657 | Acc: 39.526% (1619/4096)\n",
            "Loss: 1.663 | Acc: 39.447% (1641/4160)\n",
            "Loss: 1.660 | Acc: 39.583% (1672/4224)\n",
            "Loss: 1.661 | Acc: 39.552% (1696/4288)\n",
            "Loss: 1.660 | Acc: 39.522% (1720/4352)\n",
            "Loss: 1.658 | Acc: 39.583% (1748/4416)\n",
            "Loss: 1.657 | Acc: 39.554% (1772/4480)\n",
            "Loss: 1.659 | Acc: 39.503% (1795/4544)\n",
            "Loss: 1.661 | Acc: 39.497% (1820/4608)\n",
            "Loss: 1.662 | Acc: 39.512% (1846/4672)\n",
            "Loss: 1.661 | Acc: 39.358% (1864/4736)\n",
            "Loss: 1.665 | Acc: 39.312% (1887/4800)\n",
            "Loss: 1.661 | Acc: 39.494% (1921/4864)\n",
            "Loss: 1.659 | Acc: 39.590% (1951/4928)\n",
            "Loss: 1.659 | Acc: 39.583% (1976/4992)\n",
            "Loss: 1.659 | Acc: 39.537% (1999/5056)\n",
            "Loss: 1.659 | Acc: 39.551% (2025/5120)\n",
            "Loss: 1.660 | Acc: 39.525% (2049/5184)\n",
            "Loss: 1.662 | Acc: 39.367% (2066/5248)\n",
            "Loss: 1.662 | Acc: 39.383% (2092/5312)\n",
            "Loss: 1.663 | Acc: 39.379% (2117/5376)\n",
            "Loss: 1.663 | Acc: 39.393% (2143/5440)\n",
            "Loss: 1.662 | Acc: 39.462% (2172/5504)\n",
            "Loss: 1.663 | Acc: 39.314% (2189/5568)\n",
            "Loss: 1.664 | Acc: 39.364% (2217/5632)\n",
            "Loss: 1.665 | Acc: 39.414% (2245/5696)\n",
            "Loss: 1.664 | Acc: 39.375% (2268/5760)\n",
            "Loss: 1.661 | Acc: 39.509% (2301/5824)\n",
            "Loss: 1.662 | Acc: 39.453% (2323/5888)\n",
            "Loss: 1.663 | Acc: 39.382% (2344/5952)\n",
            "Loss: 1.663 | Acc: 39.395% (2370/6016)\n",
            "Loss: 1.664 | Acc: 39.309% (2390/6080)\n",
            "Loss: 1.666 | Acc: 39.258% (2412/6144)\n",
            "Loss: 1.667 | Acc: 39.224% (2435/6208)\n",
            "Loss: 1.668 | Acc: 39.222% (2460/6272)\n",
            "Loss: 1.669 | Acc: 39.236% (2486/6336)\n",
            "Loss: 1.669 | Acc: 39.234% (2511/6400)\n",
            "Loss: 1.670 | Acc: 39.186% (2533/6464)\n",
            "Loss: 1.669 | Acc: 39.262% (2563/6528)\n",
            "Loss: 1.671 | Acc: 39.214% (2585/6592)\n",
            "Loss: 1.670 | Acc: 39.273% (2614/6656)\n",
            "Loss: 1.671 | Acc: 39.211% (2635/6720)\n",
            "Loss: 1.670 | Acc: 39.166% (2657/6784)\n",
            "Loss: 1.668 | Acc: 39.238% (2687/6848)\n",
            "Loss: 1.668 | Acc: 39.207% (2710/6912)\n",
            "Loss: 1.669 | Acc: 39.177% (2733/6976)\n",
            "Loss: 1.669 | Acc: 39.134% (2755/7040)\n",
            "Loss: 1.670 | Acc: 39.091% (2777/7104)\n",
            "Loss: 1.670 | Acc: 39.146% (2806/7168)\n",
            "Loss: 1.670 | Acc: 39.132% (2830/7232)\n",
            "Loss: 1.668 | Acc: 39.186% (2859/7296)\n",
            "Loss: 1.666 | Acc: 39.185% (2884/7360)\n",
            "Loss: 1.667 | Acc: 39.157% (2907/7424)\n",
            "Loss: 1.664 | Acc: 39.249% (2939/7488)\n",
            "Loss: 1.666 | Acc: 39.221% (2962/7552)\n",
            "Loss: 1.666 | Acc: 39.194% (2985/7616)\n",
            "Loss: 1.666 | Acc: 39.154% (3007/7680)\n",
            "Loss: 1.665 | Acc: 39.166% (3033/7744)\n",
            "Loss: 1.664 | Acc: 39.216% (3062/7808)\n",
            "Loss: 1.665 | Acc: 39.151% (3082/7872)\n",
            "Loss: 1.664 | Acc: 39.163% (3108/7936)\n",
            "Loss: 1.664 | Acc: 39.100% (3128/8000)\n",
            "Loss: 1.663 | Acc: 39.174% (3159/8064)\n",
            "Loss: 1.662 | Acc: 39.210% (3187/8128)\n",
            "Loss: 1.662 | Acc: 39.258% (3216/8192)\n",
            "Loss: 1.662 | Acc: 39.220% (3238/8256)\n",
            "Loss: 1.665 | Acc: 39.087% (3252/8320)\n",
            "Loss: 1.665 | Acc: 39.074% (3276/8384)\n",
            "Loss: 1.664 | Acc: 39.098% (3303/8448)\n",
            "Loss: 1.666 | Acc: 39.039% (3323/8512)\n",
            "Loss: 1.667 | Acc: 39.028% (3347/8576)\n",
            "Loss: 1.667 | Acc: 39.109% (3379/8640)\n",
            "Loss: 1.668 | Acc: 39.051% (3399/8704)\n",
            "Loss: 1.668 | Acc: 39.017% (3421/8768)\n",
            "Loss: 1.668 | Acc: 39.017% (3446/8832)\n",
            "Loss: 1.667 | Acc: 39.018% (3471/8896)\n",
            "Loss: 1.666 | Acc: 39.152% (3508/8960)\n",
            "Loss: 1.665 | Acc: 39.229% (3540/9024)\n",
            "Loss: 1.666 | Acc: 39.239% (3566/9088)\n",
            "Loss: 1.667 | Acc: 39.161% (3584/9152)\n",
            "Loss: 1.665 | Acc: 39.236% (3616/9216)\n",
            "Loss: 1.664 | Acc: 39.278% (3645/9280)\n",
            "Loss: 1.664 | Acc: 39.287% (3671/9344)\n",
            "Loss: 1.666 | Acc: 39.296% (3697/9408)\n",
            "Loss: 1.667 | Acc: 39.263% (3719/9472)\n",
            "Loss: 1.667 | Acc: 39.283% (3746/9536)\n",
            "Loss: 1.666 | Acc: 39.312% (3774/9600)\n",
            "Loss: 1.666 | Acc: 39.352% (3803/9664)\n",
            "Loss: 1.665 | Acc: 39.371% (3830/9728)\n",
            "Loss: 1.666 | Acc: 39.338% (3852/9792)\n",
            "Loss: 1.665 | Acc: 39.306% (3874/9856)\n",
            "Loss: 1.666 | Acc: 39.304% (3899/9920)\n",
            "Loss: 1.666 | Acc: 39.273% (3921/9984)\n",
            "Loss: 1.664 | Acc: 39.260% (3926/10000)\n",
            "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 39.26\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 1.555 | Acc: 45.312% (29/64)\n",
            "Loss: 1.643 | Acc: 35.938% (46/128)\n",
            "Loss: 1.630 | Acc: 39.062% (75/192)\n",
            "Loss: 1.624 | Acc: 39.844% (102/256)\n",
            "Loss: 1.713 | Acc: 40.000% (128/320)\n",
            "Loss: 1.700 | Acc: 39.583% (152/384)\n",
            "Loss: 1.724 | Acc: 38.170% (171/448)\n",
            "Loss: 1.720 | Acc: 37.500% (192/512)\n",
            "Loss: 1.750 | Acc: 36.458% (210/576)\n",
            "Loss: 1.741 | Acc: 37.188% (238/640)\n",
            "Loss: 1.750 | Acc: 36.364% (256/704)\n",
            "Loss: 1.748 | Acc: 35.677% (274/768)\n",
            "Loss: 1.758 | Acc: 35.337% (294/832)\n",
            "Loss: 1.769 | Acc: 35.156% (315/896)\n",
            "Loss: 1.765 | Acc: 35.417% (340/960)\n",
            "Loss: 1.751 | Acc: 36.133% (370/1024)\n",
            "Loss: 1.758 | Acc: 35.570% (387/1088)\n",
            "Loss: 1.760 | Acc: 35.243% (406/1152)\n",
            "Loss: 1.761 | Acc: 35.197% (428/1216)\n",
            "Loss: 1.759 | Acc: 35.078% (449/1280)\n",
            "Loss: 1.753 | Acc: 35.268% (474/1344)\n",
            "Loss: 1.758 | Acc: 35.014% (493/1408)\n",
            "Loss: 1.764 | Acc: 34.647% (510/1472)\n",
            "Loss: 1.766 | Acc: 34.701% (533/1536)\n",
            "Loss: 1.759 | Acc: 34.938% (559/1600)\n",
            "Loss: 1.758 | Acc: 35.036% (583/1664)\n",
            "Loss: 1.758 | Acc: 34.896% (603/1728)\n",
            "Loss: 1.753 | Acc: 34.766% (623/1792)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.751 | Acc: 34.968% (649/1856)\n",
            "Loss: 1.752 | Acc: 34.948% (671/1920)\n",
            "Loss: 1.759 | Acc: 34.526% (685/1984)\n",
            "Loss: 1.760 | Acc: 34.521% (707/2048)\n",
            "Loss: 1.762 | Acc: 34.564% (730/2112)\n",
            "Loss: 1.768 | Acc: 34.467% (750/2176)\n",
            "Loss: 1.766 | Acc: 34.732% (778/2240)\n",
            "Loss: 1.764 | Acc: 34.896% (804/2304)\n",
            "Loss: 1.762 | Acc: 34.840% (825/2368)\n",
            "Loss: 1.763 | Acc: 34.827% (847/2432)\n",
            "Loss: 1.762 | Acc: 34.976% (873/2496)\n",
            "Loss: 1.758 | Acc: 35.078% (898/2560)\n",
            "Loss: 1.758 | Acc: 34.985% (918/2624)\n",
            "Loss: 1.758 | Acc: 35.082% (943/2688)\n",
            "Loss: 1.756 | Acc: 35.138% (967/2752)\n",
            "Loss: 1.757 | Acc: 34.908% (983/2816)\n",
            "Loss: 1.755 | Acc: 34.549% (995/2880)\n",
            "Loss: 1.759 | Acc: 34.341% (1011/2944)\n",
            "Loss: 1.753 | Acc: 34.475% (1037/3008)\n",
            "Loss: 1.753 | Acc: 34.505% (1060/3072)\n",
            "Loss: 1.755 | Acc: 34.534% (1083/3136)\n",
            "Loss: 1.754 | Acc: 34.438% (1102/3200)\n",
            "Loss: 1.750 | Acc: 34.344% (1121/3264)\n",
            "Loss: 1.750 | Acc: 34.345% (1143/3328)\n",
            "Loss: 1.749 | Acc: 34.257% (1162/3392)\n",
            "Loss: 1.750 | Acc: 34.288% (1185/3456)\n",
            "Loss: 1.744 | Acc: 34.403% (1211/3520)\n",
            "Loss: 1.743 | Acc: 34.542% (1238/3584)\n",
            "Loss: 1.743 | Acc: 34.594% (1262/3648)\n",
            "Loss: 1.742 | Acc: 34.591% (1284/3712)\n",
            "Loss: 1.744 | Acc: 34.454% (1301/3776)\n",
            "Loss: 1.741 | Acc: 34.583% (1328/3840)\n",
            "Loss: 1.743 | Acc: 34.452% (1345/3904)\n",
            "Loss: 1.745 | Acc: 34.451% (1367/3968)\n",
            "Loss: 1.745 | Acc: 34.375% (1386/4032)\n",
            "Loss: 1.745 | Acc: 34.424% (1410/4096)\n",
            "Loss: 1.744 | Acc: 34.495% (1435/4160)\n",
            "Loss: 1.744 | Acc: 34.446% (1455/4224)\n",
            "Loss: 1.744 | Acc: 34.445% (1477/4288)\n",
            "Loss: 1.744 | Acc: 34.605% (1506/4352)\n",
            "Loss: 1.745 | Acc: 34.556% (1526/4416)\n",
            "Loss: 1.745 | Acc: 34.621% (1551/4480)\n",
            "Loss: 1.743 | Acc: 34.683% (1576/4544)\n",
            "Loss: 1.744 | Acc: 34.657% (1597/4608)\n",
            "Loss: 1.743 | Acc: 34.803% (1626/4672)\n",
            "Loss: 1.744 | Acc: 34.818% (1649/4736)\n",
            "Loss: 1.744 | Acc: 34.750% (1668/4800)\n",
            "Loss: 1.749 | Acc: 34.581% (1682/4864)\n",
            "Loss: 1.750 | Acc: 34.456% (1698/4928)\n",
            "Loss: 1.750 | Acc: 34.535% (1724/4992)\n",
            "Loss: 1.749 | Acc: 34.553% (1747/5056)\n",
            "Loss: 1.748 | Acc: 34.551% (1769/5120)\n",
            "Loss: 1.750 | Acc: 34.491% (1788/5184)\n",
            "Loss: 1.750 | Acc: 34.527% (1812/5248)\n",
            "Loss: 1.750 | Acc: 34.639% (1840/5312)\n",
            "Loss: 1.748 | Acc: 34.617% (1861/5376)\n",
            "Loss: 1.747 | Acc: 34.651% (1885/5440)\n",
            "Loss: 1.746 | Acc: 34.702% (1910/5504)\n",
            "Loss: 1.745 | Acc: 34.806% (1938/5568)\n",
            "Loss: 1.744 | Acc: 34.837% (1962/5632)\n",
            "Loss: 1.741 | Acc: 34.954% (1991/5696)\n",
            "Loss: 1.737 | Acc: 35.052% (2019/5760)\n",
            "Loss: 1.737 | Acc: 34.993% (2038/5824)\n",
            "Loss: 1.738 | Acc: 35.020% (2062/5888)\n",
            "Loss: 1.740 | Acc: 35.114% (2090/5952)\n",
            "Loss: 1.739 | Acc: 35.173% (2116/6016)\n",
            "Loss: 1.738 | Acc: 35.197% (2140/6080)\n",
            "Loss: 1.737 | Acc: 35.221% (2164/6144)\n",
            "Loss: 1.737 | Acc: 35.245% (2188/6208)\n",
            "Loss: 1.736 | Acc: 35.204% (2208/6272)\n",
            "Loss: 1.738 | Acc: 35.164% (2228/6336)\n",
            "Loss: 1.739 | Acc: 35.125% (2248/6400)\n",
            "Loss: 1.739 | Acc: 35.102% (2269/6464)\n",
            "Loss: 1.741 | Acc: 35.141% (2294/6528)\n",
            "Loss: 1.741 | Acc: 35.209% (2321/6592)\n",
            "Loss: 1.742 | Acc: 35.141% (2339/6656)\n",
            "Loss: 1.743 | Acc: 35.089% (2358/6720)\n",
            "Loss: 1.744 | Acc: 35.009% (2375/6784)\n",
            "Loss: 1.745 | Acc: 35.003% (2397/6848)\n",
            "Loss: 1.746 | Acc: 35.012% (2420/6912)\n",
            "Loss: 1.748 | Acc: 34.891% (2434/6976)\n",
            "Loss: 1.748 | Acc: 34.957% (2461/7040)\n",
            "Loss: 1.748 | Acc: 34.938% (2482/7104)\n",
            "Loss: 1.747 | Acc: 35.031% (2511/7168)\n",
            "Loss: 1.747 | Acc: 34.983% (2530/7232)\n",
            "Loss: 1.747 | Acc: 35.005% (2554/7296)\n",
            "Loss: 1.747 | Acc: 35.041% (2579/7360)\n",
            "Loss: 1.746 | Acc: 35.035% (2601/7424)\n",
            "Loss: 1.744 | Acc: 35.083% (2627/7488)\n",
            "Loss: 1.743 | Acc: 35.090% (2650/7552)\n",
            "Loss: 1.742 | Acc: 35.084% (2672/7616)\n",
            "Loss: 1.740 | Acc: 35.130% (2698/7680)\n",
            "Loss: 1.739 | Acc: 35.201% (2726/7744)\n",
            "Loss: 1.738 | Acc: 35.233% (2751/7808)\n",
            "Loss: 1.738 | Acc: 35.290% (2778/7872)\n",
            "Loss: 1.737 | Acc: 35.219% (2795/7936)\n",
            "Loss: 1.738 | Acc: 35.225% (2818/8000)\n",
            "Loss: 1.736 | Acc: 35.255% (2843/8064)\n",
            "Loss: 1.738 | Acc: 35.162% (2858/8128)\n",
            "Loss: 1.737 | Acc: 35.193% (2883/8192)\n",
            "Loss: 1.737 | Acc: 35.247% (2910/8256)\n",
            "Loss: 1.738 | Acc: 35.252% (2933/8320)\n",
            "Loss: 1.736 | Acc: 35.341% (2963/8384)\n",
            "Loss: 1.735 | Acc: 35.369% (2988/8448)\n",
            "Loss: 1.735 | Acc: 35.362% (3010/8512)\n",
            "Loss: 1.735 | Acc: 35.378% (3034/8576)\n",
            "Loss: 1.734 | Acc: 35.405% (3059/8640)\n",
            "Loss: 1.733 | Acc: 35.420% (3083/8704)\n",
            "Loss: 1.733 | Acc: 35.390% (3103/8768)\n",
            "Loss: 1.733 | Acc: 35.405% (3127/8832)\n",
            "Loss: 1.733 | Acc: 35.398% (3149/8896)\n",
            "Loss: 1.734 | Acc: 35.446% (3176/8960)\n",
            "Loss: 1.734 | Acc: 35.406% (3195/9024)\n",
            "Loss: 1.733 | Acc: 35.464% (3223/9088)\n",
            "Loss: 1.733 | Acc: 35.468% (3246/9152)\n",
            "Loss: 1.733 | Acc: 35.514% (3273/9216)\n",
            "Loss: 1.734 | Acc: 35.496% (3294/9280)\n",
            "Loss: 1.733 | Acc: 35.509% (3318/9344)\n",
            "Loss: 1.732 | Acc: 35.544% (3344/9408)\n",
            "Loss: 1.733 | Acc: 35.505% (3363/9472)\n",
            "Loss: 1.732 | Acc: 35.497% (3385/9536)\n",
            "Loss: 1.732 | Acc: 35.479% (3406/9600)\n",
            "Loss: 1.732 | Acc: 35.493% (3430/9664)\n",
            "Loss: 1.733 | Acc: 35.475% (3451/9728)\n",
            "Loss: 1.734 | Acc: 35.427% (3469/9792)\n",
            "Loss: 1.734 | Acc: 35.461% (3495/9856)\n",
            "Loss: 1.735 | Acc: 35.444% (3516/9920)\n",
            "Loss: 1.735 | Acc: 35.447% (3539/9984)\n",
            "Loss: 1.735 | Acc: 35.460% (3563/10048)\n",
            "Loss: 1.733 | Acc: 35.512% (3591/10112)\n",
            "Loss: 1.733 | Acc: 35.584% (3621/10176)\n",
            "Loss: 1.732 | Acc: 35.596% (3645/10240)\n",
            "Loss: 1.733 | Acc: 35.578% (3666/10304)\n",
            "Loss: 1.733 | Acc: 35.581% (3689/10368)\n",
            "Loss: 1.732 | Acc: 35.535% (3707/10432)\n",
            "Loss: 1.733 | Acc: 35.499% (3726/10496)\n",
            "Loss: 1.733 | Acc: 35.511% (3750/10560)\n",
            "Loss: 1.732 | Acc: 35.505% (3772/10624)\n",
            "Loss: 1.733 | Acc: 35.442% (3788/10688)\n",
            "Loss: 1.733 | Acc: 35.491% (3816/10752)\n",
            "Loss: 1.732 | Acc: 35.549% (3845/10816)\n",
            "Loss: 1.732 | Acc: 35.588% (3872/10880)\n",
            "Loss: 1.731 | Acc: 35.581% (3894/10944)\n",
            "Loss: 1.732 | Acc: 35.556% (3914/11008)\n",
            "Loss: 1.733 | Acc: 35.567% (3938/11072)\n",
            "Loss: 1.733 | Acc: 35.515% (3955/11136)\n",
            "Loss: 1.732 | Acc: 35.571% (3984/11200)\n",
            "Loss: 1.733 | Acc: 35.538% (4003/11264)\n",
            "Loss: 1.733 | Acc: 35.584% (4031/11328)\n",
            "Loss: 1.733 | Acc: 35.551% (4050/11392)\n",
            "Loss: 1.734 | Acc: 35.536% (4071/11456)\n",
            "Loss: 1.733 | Acc: 35.564% (4097/11520)\n",
            "Loss: 1.733 | Acc: 35.523% (4115/11584)\n",
            "Loss: 1.732 | Acc: 35.525% (4138/11648)\n",
            "Loss: 1.733 | Acc: 35.494% (4157/11712)\n",
            "Loss: 1.734 | Acc: 35.462% (4176/11776)\n",
            "Loss: 1.734 | Acc: 35.456% (4198/11840)\n",
            "Loss: 1.735 | Acc: 35.400% (4214/11904)\n",
            "Loss: 1.735 | Acc: 35.403% (4237/11968)\n",
            "Loss: 1.734 | Acc: 35.389% (4258/12032)\n",
            "Loss: 1.735 | Acc: 35.359% (4277/12096)\n",
            "Loss: 1.735 | Acc: 35.345% (4298/12160)\n",
            "Loss: 1.736 | Acc: 35.373% (4324/12224)\n",
            "Loss: 1.736 | Acc: 35.311% (4339/12288)\n",
            "Loss: 1.737 | Acc: 35.322% (4363/12352)\n",
            "Loss: 1.736 | Acc: 35.366% (4391/12416)\n",
            "Loss: 1.737 | Acc: 35.369% (4414/12480)\n",
            "Loss: 1.736 | Acc: 35.411% (4442/12544)\n",
            "Loss: 1.736 | Acc: 35.414% (4465/12608)\n",
            "Loss: 1.737 | Acc: 35.440% (4491/12672)\n",
            "Loss: 1.737 | Acc: 35.427% (4512/12736)\n",
            "Loss: 1.737 | Acc: 35.391% (4530/12800)\n",
            "Loss: 1.738 | Acc: 35.409% (4555/12864)\n",
            "Loss: 1.739 | Acc: 35.365% (4572/12928)\n",
            "Loss: 1.740 | Acc: 35.353% (4593/12992)\n",
            "Loss: 1.740 | Acc: 35.363% (4617/13056)\n",
            "Loss: 1.740 | Acc: 35.381% (4642/13120)\n",
            "Loss: 1.741 | Acc: 35.369% (4663/13184)\n",
            "Loss: 1.742 | Acc: 35.341% (4682/13248)\n",
            "Loss: 1.742 | Acc: 35.299% (4699/13312)\n",
            "Loss: 1.741 | Acc: 35.317% (4724/13376)\n",
            "Loss: 1.741 | Acc: 35.305% (4745/13440)\n",
            "Loss: 1.741 | Acc: 35.330% (4771/13504)\n",
            "Loss: 1.741 | Acc: 35.333% (4794/13568)\n",
            "Loss: 1.741 | Acc: 35.321% (4815/13632)\n",
            "Loss: 1.741 | Acc: 35.331% (4839/13696)\n",
            "Loss: 1.742 | Acc: 35.298% (4857/13760)\n",
            "Loss: 1.741 | Acc: 35.279% (4877/13824)\n",
            "Loss: 1.743 | Acc: 35.239% (4894/13888)\n",
            "Loss: 1.742 | Acc: 35.278% (4922/13952)\n",
            "Loss: 1.742 | Acc: 35.274% (4944/14016)\n",
            "Loss: 1.742 | Acc: 35.270% (4966/14080)\n",
            "Loss: 1.742 | Acc: 35.238% (4984/14144)\n",
            "Loss: 1.742 | Acc: 35.255% (5009/14208)\n",
            "Loss: 1.742 | Acc: 35.251% (5031/14272)\n",
            "Loss: 1.742 | Acc: 35.254% (5054/14336)\n",
            "Loss: 1.742 | Acc: 35.222% (5072/14400)\n",
            "Loss: 1.742 | Acc: 35.218% (5094/14464)\n",
            "Loss: 1.743 | Acc: 35.229% (5118/14528)\n",
            "Loss: 1.744 | Acc: 35.184% (5134/14592)\n",
            "Loss: 1.743 | Acc: 35.180% (5156/14656)\n",
            "Loss: 1.743 | Acc: 35.177% (5178/14720)\n",
            "Loss: 1.745 | Acc: 35.146% (5196/14784)\n",
            "Loss: 1.744 | Acc: 35.156% (5220/14848)\n",
            "Loss: 1.744 | Acc: 35.153% (5242/14912)\n",
            "Loss: 1.745 | Acc: 35.130% (5261/14976)\n",
            "Loss: 1.745 | Acc: 35.120% (5282/15040)\n",
            "Loss: 1.745 | Acc: 35.123% (5305/15104)\n",
            "Loss: 1.745 | Acc: 35.166% (5334/15168)\n",
            "Loss: 1.744 | Acc: 35.196% (5361/15232)\n",
            "Loss: 1.744 | Acc: 35.212% (5386/15296)\n",
            "Loss: 1.744 | Acc: 35.228% (5411/15360)\n",
            "Loss: 1.744 | Acc: 35.270% (5440/15424)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.745 | Acc: 35.253% (5460/15488)\n",
            "Loss: 1.745 | Acc: 35.275% (5486/15552)\n",
            "Loss: 1.744 | Acc: 35.304% (5513/15616)\n",
            "Loss: 1.744 | Acc: 35.338% (5541/15680)\n",
            "Loss: 1.744 | Acc: 35.309% (5559/15744)\n",
            "Loss: 1.745 | Acc: 35.318% (5583/15808)\n",
            "Loss: 1.746 | Acc: 35.301% (5603/15872)\n",
            "Loss: 1.746 | Acc: 35.316% (5628/15936)\n",
            "Loss: 1.746 | Acc: 35.319% (5651/16000)\n",
            "Loss: 1.747 | Acc: 35.296% (5670/16064)\n",
            "Loss: 1.748 | Acc: 35.280% (5690/16128)\n",
            "Loss: 1.748 | Acc: 35.308% (5717/16192)\n",
            "Loss: 1.747 | Acc: 35.322% (5742/16256)\n",
            "Loss: 1.747 | Acc: 35.331% (5766/16320)\n",
            "Loss: 1.746 | Acc: 35.370% (5795/16384)\n",
            "Loss: 1.746 | Acc: 35.372% (5818/16448)\n",
            "Loss: 1.745 | Acc: 35.386% (5843/16512)\n",
            "Loss: 1.746 | Acc: 35.358% (5861/16576)\n",
            "Loss: 1.745 | Acc: 35.379% (5887/16640)\n",
            "Loss: 1.746 | Acc: 35.351% (5905/16704)\n",
            "Loss: 1.746 | Acc: 35.353% (5928/16768)\n",
            "Loss: 1.746 | Acc: 35.373% (5954/16832)\n",
            "Loss: 1.746 | Acc: 35.381% (5978/16896)\n",
            "Loss: 1.746 | Acc: 35.401% (6004/16960)\n",
            "Loss: 1.745 | Acc: 35.409% (6028/17024)\n",
            "Loss: 1.745 | Acc: 35.423% (6053/17088)\n",
            "Loss: 1.745 | Acc: 35.430% (6077/17152)\n",
            "Loss: 1.746 | Acc: 35.409% (6096/17216)\n",
            "Loss: 1.747 | Acc: 35.388% (6115/17280)\n",
            "Loss: 1.747 | Acc: 35.384% (6137/17344)\n",
            "Loss: 1.747 | Acc: 35.369% (6157/17408)\n",
            "Loss: 1.747 | Acc: 35.359% (6178/17472)\n",
            "Loss: 1.746 | Acc: 35.356% (6200/17536)\n",
            "Loss: 1.747 | Acc: 35.358% (6223/17600)\n",
            "Loss: 1.747 | Acc: 35.343% (6243/17664)\n",
            "Loss: 1.747 | Acc: 35.328% (6263/17728)\n",
            "Loss: 1.747 | Acc: 35.302% (6281/17792)\n",
            "Loss: 1.747 | Acc: 35.316% (6306/17856)\n",
            "Loss: 1.747 | Acc: 35.335% (6332/17920)\n",
            "Loss: 1.746 | Acc: 35.359% (6359/17984)\n",
            "Loss: 1.747 | Acc: 35.350% (6380/18048)\n",
            "Loss: 1.746 | Acc: 35.341% (6401/18112)\n",
            "Loss: 1.746 | Acc: 35.332% (6422/18176)\n",
            "Loss: 1.746 | Acc: 35.285% (6436/18240)\n",
            "Loss: 1.746 | Acc: 35.287% (6459/18304)\n",
            "Loss: 1.746 | Acc: 35.295% (6483/18368)\n",
            "Loss: 1.746 | Acc: 35.324% (6511/18432)\n",
            "Loss: 1.746 | Acc: 35.364% (6541/18496)\n",
            "Loss: 1.746 | Acc: 35.350% (6561/18560)\n",
            "Loss: 1.745 | Acc: 35.374% (6588/18624)\n",
            "Loss: 1.745 | Acc: 35.386% (6613/18688)\n",
            "Loss: 1.746 | Acc: 35.372% (6633/18752)\n",
            "Loss: 1.746 | Acc: 35.390% (6659/18816)\n",
            "Loss: 1.746 | Acc: 35.397% (6683/18880)\n",
            "Loss: 1.745 | Acc: 35.373% (6701/18944)\n",
            "Loss: 1.746 | Acc: 35.332% (6716/19008)\n",
            "Loss: 1.746 | Acc: 35.329% (6738/19072)\n",
            "Loss: 1.746 | Acc: 35.300% (6755/19136)\n",
            "Loss: 1.746 | Acc: 35.312% (6780/19200)\n",
            "Loss: 1.746 | Acc: 35.299% (6800/19264)\n",
            "Loss: 1.746 | Acc: 35.306% (6824/19328)\n",
            "Loss: 1.746 | Acc: 35.314% (6848/19392)\n",
            "Loss: 1.745 | Acc: 35.331% (6874/19456)\n",
            "Loss: 1.746 | Acc: 35.338% (6898/19520)\n",
            "Loss: 1.745 | Acc: 35.360% (6925/19584)\n",
            "Loss: 1.745 | Acc: 35.383% (6952/19648)\n",
            "Loss: 1.744 | Acc: 35.374% (6973/19712)\n",
            "Loss: 1.745 | Acc: 35.336% (6988/19776)\n",
            "Loss: 1.744 | Acc: 35.353% (7014/19840)\n",
            "Loss: 1.744 | Acc: 35.365% (7039/19904)\n",
            "Loss: 1.744 | Acc: 35.357% (7060/19968)\n",
            "Loss: 1.744 | Acc: 35.348% (7081/20032)\n",
            "Loss: 1.744 | Acc: 35.380% (7110/20096)\n",
            "Loss: 1.743 | Acc: 35.397% (7136/20160)\n",
            "Loss: 1.743 | Acc: 35.399% (7159/20224)\n",
            "Loss: 1.743 | Acc: 35.415% (7185/20288)\n",
            "Loss: 1.743 | Acc: 35.417% (7208/20352)\n",
            "Loss: 1.744 | Acc: 35.413% (7230/20416)\n",
            "Loss: 1.743 | Acc: 35.410% (7252/20480)\n",
            "Loss: 1.743 | Acc: 35.397% (7272/20544)\n",
            "Loss: 1.744 | Acc: 35.394% (7294/20608)\n",
            "Loss: 1.744 | Acc: 35.410% (7320/20672)\n",
            "Loss: 1.744 | Acc: 35.412% (7343/20736)\n",
            "Loss: 1.744 | Acc: 35.433% (7370/20800)\n",
            "Loss: 1.744 | Acc: 35.434% (7393/20864)\n",
            "Loss: 1.744 | Acc: 35.445% (7418/20928)\n",
            "Loss: 1.745 | Acc: 35.409% (7433/20992)\n",
            "Loss: 1.745 | Acc: 35.401% (7454/21056)\n",
            "Loss: 1.746 | Acc: 35.388% (7474/21120)\n",
            "Loss: 1.746 | Acc: 35.376% (7494/21184)\n",
            "Loss: 1.746 | Acc: 35.387% (7519/21248)\n",
            "Loss: 1.746 | Acc: 35.384% (7541/21312)\n",
            "Loss: 1.745 | Acc: 35.390% (7565/21376)\n",
            "Loss: 1.744 | Acc: 35.424% (7595/21440)\n",
            "Loss: 1.745 | Acc: 35.426% (7618/21504)\n",
            "Loss: 1.744 | Acc: 35.451% (7646/21568)\n",
            "Loss: 1.745 | Acc: 35.434% (7665/21632)\n",
            "Loss: 1.745 | Acc: 35.467% (7695/21696)\n",
            "Loss: 1.744 | Acc: 35.469% (7718/21760)\n",
            "Loss: 1.743 | Acc: 35.498% (7747/21824)\n",
            "Loss: 1.743 | Acc: 35.503% (7771/21888)\n",
            "Loss: 1.744 | Acc: 35.491% (7791/21952)\n",
            "Loss: 1.743 | Acc: 35.533% (7823/22016)\n",
            "Loss: 1.742 | Acc: 35.566% (7853/22080)\n",
            "Loss: 1.741 | Acc: 35.617% (7887/22144)\n",
            "Loss: 1.740 | Acc: 35.631% (7913/22208)\n",
            "Loss: 1.740 | Acc: 35.632% (7936/22272)\n",
            "Loss: 1.739 | Acc: 35.646% (7962/22336)\n",
            "Loss: 1.739 | Acc: 35.652% (7986/22400)\n",
            "Loss: 1.739 | Acc: 35.670% (8013/22464)\n",
            "Loss: 1.739 | Acc: 35.658% (8033/22528)\n",
            "Loss: 1.739 | Acc: 35.690% (8063/22592)\n",
            "Loss: 1.739 | Acc: 35.651% (8077/22656)\n",
            "Loss: 1.739 | Acc: 35.660% (8102/22720)\n",
            "Loss: 1.739 | Acc: 35.683% (8130/22784)\n",
            "Loss: 1.738 | Acc: 35.688% (8154/22848)\n",
            "Loss: 1.738 | Acc: 35.697% (8179/22912)\n",
            "Loss: 1.737 | Acc: 35.681% (8198/22976)\n",
            "Loss: 1.737 | Acc: 35.681% (8221/23040)\n",
            "Loss: 1.737 | Acc: 35.682% (8244/23104)\n",
            "Loss: 1.737 | Acc: 35.696% (8270/23168)\n",
            "Loss: 1.737 | Acc: 35.696% (8293/23232)\n",
            "Loss: 1.737 | Acc: 35.693% (8315/23296)\n",
            "Loss: 1.738 | Acc: 35.672% (8333/23360)\n",
            "Loss: 1.738 | Acc: 35.669% (8355/23424)\n",
            "Loss: 1.738 | Acc: 35.674% (8379/23488)\n",
            "Loss: 1.738 | Acc: 35.687% (8405/23552)\n",
            "Loss: 1.737 | Acc: 35.705% (8432/23616)\n",
            "Loss: 1.738 | Acc: 35.714% (8457/23680)\n",
            "Loss: 1.738 | Acc: 35.706% (8478/23744)\n",
            "Loss: 1.738 | Acc: 35.719% (8504/23808)\n",
            "Loss: 1.738 | Acc: 35.690% (8520/23872)\n",
            "Loss: 1.739 | Acc: 35.704% (8546/23936)\n",
            "Loss: 1.739 | Acc: 35.683% (8564/24000)\n",
            "Loss: 1.739 | Acc: 35.696% (8590/24064)\n",
            "Loss: 1.739 | Acc: 35.718% (8618/24128)\n",
            "Loss: 1.739 | Acc: 35.702% (8637/24192)\n",
            "Loss: 1.739 | Acc: 35.727% (8666/24256)\n",
            "Loss: 1.739 | Acc: 35.732% (8690/24320)\n",
            "Loss: 1.739 | Acc: 35.724% (8711/24384)\n",
            "Loss: 1.739 | Acc: 35.745% (8739/24448)\n",
            "Loss: 1.739 | Acc: 35.770% (8768/24512)\n",
            "Loss: 1.739 | Acc: 35.767% (8790/24576)\n",
            "Loss: 1.738 | Acc: 35.800% (8821/24640)\n",
            "Loss: 1.738 | Acc: 35.804% (8845/24704)\n",
            "Loss: 1.738 | Acc: 35.800% (8867/24768)\n",
            "Loss: 1.738 | Acc: 35.776% (8884/24832)\n",
            "Loss: 1.738 | Acc: 35.781% (8908/24896)\n",
            "Loss: 1.738 | Acc: 35.777% (8930/24960)\n",
            "Loss: 1.738 | Acc: 35.790% (8956/25024)\n",
            "Loss: 1.738 | Acc: 35.790% (8979/25088)\n",
            "Loss: 1.738 | Acc: 35.794% (9003/25152)\n",
            "Loss: 1.738 | Acc: 35.807% (9029/25216)\n",
            "Loss: 1.738 | Acc: 35.807% (9052/25280)\n",
            "Loss: 1.738 | Acc: 35.803% (9074/25344)\n",
            "Loss: 1.738 | Acc: 35.796% (9095/25408)\n",
            "Loss: 1.739 | Acc: 35.788% (9116/25472)\n",
            "Loss: 1.738 | Acc: 35.797% (9141/25536)\n",
            "Loss: 1.738 | Acc: 35.777% (9159/25600)\n",
            "Loss: 1.738 | Acc: 35.786% (9184/25664)\n",
            "Loss: 1.739 | Acc: 35.766% (9202/25728)\n",
            "Loss: 1.739 | Acc: 35.767% (9225/25792)\n",
            "Loss: 1.739 | Acc: 35.771% (9249/25856)\n",
            "Loss: 1.739 | Acc: 35.775% (9273/25920)\n",
            "Loss: 1.739 | Acc: 35.776% (9296/25984)\n",
            "Loss: 1.739 | Acc: 35.776% (9319/26048)\n",
            "Loss: 1.739 | Acc: 35.765% (9339/26112)\n",
            "Loss: 1.739 | Acc: 35.781% (9366/26176)\n",
            "Loss: 1.739 | Acc: 35.812% (9397/26240)\n",
            "Loss: 1.738 | Acc: 35.854% (9431/26304)\n",
            "Loss: 1.738 | Acc: 35.854% (9454/26368)\n",
            "Loss: 1.738 | Acc: 35.858% (9478/26432)\n",
            "Loss: 1.738 | Acc: 35.870% (9504/26496)\n",
            "Loss: 1.738 | Acc: 35.881% (9530/26560)\n",
            "Loss: 1.738 | Acc: 35.870% (9550/26624)\n",
            "Loss: 1.738 | Acc: 35.866% (9572/26688)\n",
            "Loss: 1.738 | Acc: 35.863% (9594/26752)\n",
            "Loss: 1.738 | Acc: 35.841% (9611/26816)\n",
            "Loss: 1.738 | Acc: 35.852% (9637/26880)\n",
            "Loss: 1.738 | Acc: 35.837% (9656/26944)\n",
            "Loss: 1.738 | Acc: 35.834% (9678/27008)\n",
            "Loss: 1.737 | Acc: 35.841% (9703/27072)\n",
            "Loss: 1.737 | Acc: 35.827% (9722/27136)\n",
            "Loss: 1.737 | Acc: 35.849% (9751/27200)\n",
            "Loss: 1.737 | Acc: 35.831% (9769/27264)\n",
            "Loss: 1.737 | Acc: 35.813% (9787/27328)\n",
            "Loss: 1.737 | Acc: 35.810% (9809/27392)\n",
            "Loss: 1.737 | Acc: 35.814% (9833/27456)\n",
            "Loss: 1.737 | Acc: 35.810% (9855/27520)\n",
            "Loss: 1.737 | Acc: 35.829% (9883/27584)\n",
            "Loss: 1.737 | Acc: 35.847% (9911/27648)\n",
            "Loss: 1.737 | Acc: 35.851% (9935/27712)\n",
            "Loss: 1.737 | Acc: 35.833% (9953/27776)\n",
            "Loss: 1.737 | Acc: 35.844% (9979/27840)\n",
            "Loss: 1.736 | Acc: 35.869% (10009/27904)\n",
            "Loss: 1.736 | Acc: 35.866% (10031/27968)\n",
            "Loss: 1.736 | Acc: 35.845% (10048/28032)\n",
            "Loss: 1.736 | Acc: 35.831% (10067/28096)\n",
            "Loss: 1.736 | Acc: 35.835% (10091/28160)\n",
            "Loss: 1.736 | Acc: 35.845% (10117/28224)\n",
            "Loss: 1.737 | Acc: 35.817% (10132/28288)\n",
            "Loss: 1.737 | Acc: 35.818% (10155/28352)\n",
            "Loss: 1.737 | Acc: 35.804% (10174/28416)\n",
            "Loss: 1.737 | Acc: 35.772% (10188/28480)\n",
            "Loss: 1.737 | Acc: 35.759% (10207/28544)\n",
            "Loss: 1.738 | Acc: 35.738% (10224/28608)\n",
            "Loss: 1.738 | Acc: 35.756% (10252/28672)\n",
            "Loss: 1.737 | Acc: 35.757% (10275/28736)\n",
            "Loss: 1.737 | Acc: 35.760% (10299/28800)\n",
            "Loss: 1.737 | Acc: 35.771% (10325/28864)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.737 | Acc: 35.778% (10350/28928)\n",
            "Loss: 1.737 | Acc: 35.755% (10366/28992)\n",
            "Loss: 1.737 | Acc: 35.769% (10393/29056)\n",
            "Loss: 1.737 | Acc: 35.749% (10410/29120)\n",
            "Loss: 1.737 | Acc: 35.746% (10432/29184)\n",
            "Loss: 1.737 | Acc: 35.726% (10449/29248)\n",
            "Loss: 1.737 | Acc: 35.743% (10477/29312)\n",
            "Loss: 1.736 | Acc: 35.778% (10510/29376)\n",
            "Loss: 1.736 | Acc: 35.788% (10536/29440)\n",
            "Loss: 1.736 | Acc: 35.782% (10557/29504)\n",
            "Loss: 1.736 | Acc: 35.789% (10582/29568)\n",
            "Loss: 1.736 | Acc: 35.786% (10604/29632)\n",
            "Loss: 1.736 | Acc: 35.766% (10621/29696)\n",
            "Loss: 1.736 | Acc: 35.746% (10638/29760)\n",
            "Loss: 1.736 | Acc: 35.740% (10659/29824)\n",
            "Loss: 1.736 | Acc: 35.740% (10682/29888)\n",
            "Loss: 1.737 | Acc: 35.710% (10696/29952)\n",
            "Loss: 1.737 | Acc: 35.714% (10720/30016)\n",
            "Loss: 1.737 | Acc: 35.695% (10737/30080)\n",
            "Loss: 1.737 | Acc: 35.679% (10755/30144)\n",
            "Loss: 1.737 | Acc: 35.683% (10779/30208)\n",
            "Loss: 1.737 | Acc: 35.683% (10802/30272)\n",
            "Loss: 1.737 | Acc: 35.694% (10828/30336)\n",
            "Loss: 1.737 | Acc: 35.704% (10854/30400)\n",
            "Loss: 1.737 | Acc: 35.714% (10880/30464)\n",
            "Loss: 1.737 | Acc: 35.702% (10899/30528)\n",
            "Loss: 1.737 | Acc: 35.692% (10919/30592)\n",
            "Loss: 1.737 | Acc: 35.693% (10942/30656)\n",
            "Loss: 1.737 | Acc: 35.690% (10964/30720)\n",
            "Loss: 1.737 | Acc: 35.658% (10977/30784)\n",
            "Loss: 1.737 | Acc: 35.655% (10999/30848)\n",
            "Loss: 1.737 | Acc: 35.672% (11027/30912)\n",
            "Loss: 1.737 | Acc: 35.676% (11051/30976)\n",
            "Loss: 1.737 | Acc: 35.660% (11069/31040)\n",
            "Loss: 1.737 | Acc: 35.674% (11096/31104)\n",
            "Loss: 1.737 | Acc: 35.678% (11120/31168)\n",
            "Loss: 1.737 | Acc: 35.656% (11136/31232)\n",
            "Loss: 1.737 | Acc: 35.660% (11160/31296)\n",
            "Loss: 1.738 | Acc: 35.644% (11178/31360)\n",
            "Loss: 1.737 | Acc: 35.651% (11203/31424)\n",
            "Loss: 1.738 | Acc: 35.639% (11222/31488)\n",
            "Loss: 1.738 | Acc: 35.624% (11240/31552)\n",
            "Loss: 1.738 | Acc: 35.624% (11263/31616)\n",
            "Loss: 1.738 | Acc: 35.616% (11283/31680)\n",
            "Loss: 1.738 | Acc: 35.604% (11302/31744)\n",
            "Loss: 1.738 | Acc: 35.601% (11324/31808)\n",
            "Loss: 1.738 | Acc: 35.589% (11343/31872)\n",
            "Loss: 1.738 | Acc: 35.593% (11367/31936)\n",
            "Loss: 1.738 | Acc: 35.612% (11396/32000)\n",
            "Loss: 1.738 | Acc: 35.594% (11413/32064)\n",
            "Loss: 1.739 | Acc: 35.586% (11433/32128)\n",
            "Loss: 1.739 | Acc: 35.593% (11458/32192)\n",
            "Loss: 1.738 | Acc: 35.584% (11478/32256)\n",
            "Loss: 1.739 | Acc: 35.566% (11495/32320)\n",
            "Loss: 1.739 | Acc: 35.570% (11519/32384)\n",
            "Loss: 1.738 | Acc: 35.586% (11547/32448)\n",
            "Loss: 1.739 | Acc: 35.584% (11569/32512)\n",
            "Loss: 1.739 | Acc: 35.578% (11590/32576)\n",
            "Loss: 1.739 | Acc: 35.573% (11611/32640)\n",
            "Loss: 1.740 | Acc: 35.577% (11635/32704)\n",
            "Loss: 1.740 | Acc: 35.568% (11655/32768)\n",
            "Loss: 1.740 | Acc: 35.572% (11679/32832)\n",
            "Loss: 1.740 | Acc: 35.570% (11701/32896)\n",
            "Loss: 1.740 | Acc: 35.583% (11728/32960)\n",
            "Loss: 1.740 | Acc: 35.574% (11748/33024)\n",
            "Loss: 1.740 | Acc: 35.572% (11770/33088)\n",
            "Loss: 1.740 | Acc: 35.560% (11789/33152)\n",
            "Loss: 1.740 | Acc: 35.555% (11810/33216)\n",
            "Loss: 1.740 | Acc: 35.529% (11824/33280)\n",
            "Loss: 1.740 | Acc: 35.524% (11845/33344)\n",
            "Loss: 1.740 | Acc: 35.524% (11868/33408)\n",
            "Loss: 1.740 | Acc: 35.510% (11886/33472)\n",
            "Loss: 1.741 | Acc: 35.499% (11905/33536)\n",
            "Loss: 1.741 | Acc: 35.479% (11921/33600)\n",
            "Loss: 1.741 | Acc: 35.447% (11933/33664)\n",
            "Loss: 1.741 | Acc: 35.451% (11957/33728)\n",
            "Loss: 1.741 | Acc: 35.461% (11983/33792)\n",
            "Loss: 1.741 | Acc: 35.471% (12009/33856)\n",
            "Loss: 1.741 | Acc: 35.483% (12036/33920)\n",
            "Loss: 1.741 | Acc: 35.508% (12067/33984)\n",
            "Loss: 1.741 | Acc: 35.515% (12092/34048)\n",
            "Loss: 1.741 | Acc: 35.504% (12111/34112)\n",
            "Loss: 1.741 | Acc: 35.496% (12131/34176)\n",
            "Loss: 1.741 | Acc: 35.496% (12154/34240)\n",
            "Loss: 1.741 | Acc: 35.491% (12175/34304)\n",
            "Loss: 1.741 | Acc: 35.504% (12202/34368)\n",
            "Loss: 1.741 | Acc: 35.493% (12221/34432)\n",
            "Loss: 1.741 | Acc: 35.517% (12252/34496)\n",
            "Loss: 1.741 | Acc: 35.530% (12279/34560)\n",
            "Loss: 1.741 | Acc: 35.542% (12306/34624)\n",
            "Loss: 1.741 | Acc: 35.551% (12332/34688)\n",
            "Loss: 1.741 | Acc: 35.555% (12356/34752)\n",
            "Loss: 1.741 | Acc: 35.538% (12373/34816)\n",
            "Loss: 1.742 | Acc: 35.528% (12392/34880)\n",
            "Loss: 1.742 | Acc: 35.543% (12420/34944)\n",
            "Loss: 1.741 | Acc: 35.549% (12445/35008)\n",
            "Loss: 1.741 | Acc: 35.550% (12468/35072)\n",
            "Loss: 1.741 | Acc: 35.570% (12498/35136)\n",
            "Loss: 1.741 | Acc: 35.594% (12529/35200)\n",
            "Loss: 1.740 | Acc: 35.606% (12556/35264)\n",
            "Loss: 1.740 | Acc: 35.603% (12578/35328)\n",
            "Loss: 1.740 | Acc: 35.604% (12601/35392)\n",
            "Loss: 1.740 | Acc: 35.596% (12621/35456)\n",
            "Loss: 1.740 | Acc: 35.597% (12644/35520)\n",
            "Loss: 1.741 | Acc: 35.586% (12663/35584)\n",
            "Loss: 1.741 | Acc: 35.587% (12686/35648)\n",
            "Loss: 1.740 | Acc: 35.585% (12708/35712)\n",
            "Loss: 1.741 | Acc: 35.588% (12732/35776)\n",
            "Loss: 1.741 | Acc: 35.583% (12753/35840)\n",
            "Loss: 1.741 | Acc: 35.581% (12775/35904)\n",
            "Loss: 1.741 | Acc: 35.584% (12799/35968)\n",
            "Loss: 1.741 | Acc: 35.610% (12831/36032)\n",
            "Loss: 1.740 | Acc: 35.611% (12854/36096)\n",
            "Loss: 1.740 | Acc: 35.614% (12878/36160)\n",
            "Loss: 1.741 | Acc: 35.609% (12899/36224)\n",
            "Loss: 1.740 | Acc: 35.607% (12921/36288)\n",
            "Loss: 1.741 | Acc: 35.602% (12942/36352)\n",
            "Loss: 1.741 | Acc: 35.594% (12962/36416)\n",
            "Loss: 1.740 | Acc: 35.614% (12992/36480)\n",
            "Loss: 1.740 | Acc: 35.617% (13016/36544)\n",
            "Loss: 1.740 | Acc: 35.612% (13037/36608)\n",
            "Loss: 1.740 | Acc: 35.608% (13058/36672)\n",
            "Loss: 1.740 | Acc: 35.605% (13080/36736)\n",
            "Loss: 1.740 | Acc: 35.614% (13106/36800)\n",
            "Loss: 1.740 | Acc: 35.639% (13138/36864)\n",
            "Loss: 1.740 | Acc: 35.634% (13159/36928)\n",
            "Loss: 1.740 | Acc: 35.640% (13184/36992)\n",
            "Loss: 1.740 | Acc: 35.646% (13209/37056)\n",
            "Loss: 1.740 | Acc: 35.657% (13236/37120)\n",
            "Loss: 1.740 | Acc: 35.671% (13264/37184)\n",
            "Loss: 1.740 | Acc: 35.658% (13282/37248)\n",
            "Loss: 1.740 | Acc: 35.664% (13307/37312)\n",
            "Loss: 1.740 | Acc: 35.683% (13337/37376)\n",
            "Loss: 1.740 | Acc: 35.684% (13360/37440)\n",
            "Loss: 1.740 | Acc: 35.698% (13388/37504)\n",
            "Loss: 1.740 | Acc: 35.709% (13415/37568)\n",
            "Loss: 1.740 | Acc: 35.712% (13439/37632)\n",
            "Loss: 1.740 | Acc: 35.723% (13466/37696)\n",
            "Loss: 1.740 | Acc: 35.734% (13493/37760)\n",
            "Loss: 1.740 | Acc: 35.731% (13515/37824)\n",
            "Loss: 1.740 | Acc: 35.726% (13536/37888)\n",
            "Loss: 1.739 | Acc: 35.740% (13564/37952)\n",
            "Loss: 1.740 | Acc: 35.730% (13583/38016)\n",
            "Loss: 1.740 | Acc: 35.733% (13607/38080)\n",
            "Loss: 1.739 | Acc: 35.736% (13631/38144)\n",
            "Loss: 1.739 | Acc: 35.741% (13656/38208)\n",
            "Loss: 1.739 | Acc: 35.739% (13678/38272)\n",
            "Loss: 1.740 | Acc: 35.729% (13697/38336)\n",
            "Loss: 1.739 | Acc: 35.750% (13728/38400)\n",
            "Loss: 1.739 | Acc: 35.756% (13753/38464)\n",
            "Loss: 1.739 | Acc: 35.756% (13776/38528)\n",
            "Loss: 1.739 | Acc: 35.764% (13802/38592)\n",
            "Loss: 1.739 | Acc: 35.751% (13820/38656)\n",
            "Loss: 1.739 | Acc: 35.754% (13844/38720)\n",
            "Loss: 1.739 | Acc: 35.752% (13866/38784)\n",
            "Loss: 1.739 | Acc: 35.747% (13887/38848)\n",
            "Loss: 1.739 | Acc: 35.752% (13912/38912)\n",
            "Loss: 1.739 | Acc: 35.748% (13933/38976)\n",
            "Loss: 1.739 | Acc: 35.751% (13957/39040)\n",
            "Loss: 1.739 | Acc: 35.756% (13982/39104)\n",
            "Loss: 1.739 | Acc: 35.766% (14009/39168)\n",
            "Loss: 1.739 | Acc: 35.757% (14028/39232)\n",
            "Loss: 1.739 | Acc: 35.762% (14053/39296)\n",
            "Loss: 1.739 | Acc: 35.749% (14071/39360)\n",
            "Loss: 1.739 | Acc: 35.765% (14100/39424)\n",
            "Loss: 1.739 | Acc: 35.760% (14121/39488)\n",
            "Loss: 1.740 | Acc: 35.750% (14140/39552)\n",
            "Loss: 1.739 | Acc: 35.768% (14170/39616)\n",
            "Loss: 1.739 | Acc: 35.784% (14199/39680)\n",
            "Loss: 1.739 | Acc: 35.782% (14221/39744)\n",
            "Loss: 1.739 | Acc: 35.792% (14248/39808)\n",
            "Loss: 1.739 | Acc: 35.790% (14270/39872)\n",
            "Loss: 1.739 | Acc: 35.785% (14291/39936)\n",
            "Loss: 1.739 | Acc: 35.790% (14316/40000)\n",
            "Loss: 1.739 | Acc: 35.803% (14344/40064)\n",
            "Loss: 1.738 | Acc: 35.810% (14370/40128)\n",
            "Loss: 1.738 | Acc: 35.811% (14393/40192)\n",
            "Loss: 1.738 | Acc: 35.823% (14421/40256)\n",
            "Loss: 1.738 | Acc: 35.831% (14447/40320)\n",
            "Loss: 1.739 | Acc: 35.826% (14468/40384)\n",
            "Loss: 1.739 | Acc: 35.816% (14487/40448)\n",
            "Loss: 1.738 | Acc: 35.824% (14513/40512)\n",
            "Loss: 1.738 | Acc: 35.812% (14531/40576)\n",
            "Loss: 1.739 | Acc: 35.810% (14553/40640)\n",
            "Loss: 1.739 | Acc: 35.795% (14570/40704)\n",
            "Loss: 1.739 | Acc: 35.783% (14588/40768)\n",
            "Loss: 1.739 | Acc: 35.771% (14606/40832)\n",
            "Loss: 1.739 | Acc: 35.774% (14630/40896)\n",
            "Loss: 1.739 | Acc: 35.779% (14655/40960)\n",
            "Loss: 1.739 | Acc: 35.794% (14684/41024)\n",
            "Loss: 1.739 | Acc: 35.804% (14711/41088)\n",
            "Loss: 1.740 | Acc: 35.797% (14731/41152)\n",
            "Loss: 1.739 | Acc: 35.806% (14758/41216)\n",
            "Loss: 1.739 | Acc: 35.790% (14774/41280)\n",
            "Loss: 1.739 | Acc: 35.790% (14797/41344)\n",
            "Loss: 1.739 | Acc: 35.776% (14814/41408)\n",
            "Loss: 1.739 | Acc: 35.788% (14842/41472)\n",
            "Loss: 1.739 | Acc: 35.779% (14861/41536)\n",
            "Loss: 1.739 | Acc: 35.776% (14883/41600)\n",
            "Loss: 1.739 | Acc: 35.801% (14916/41664)\n",
            "Loss: 1.739 | Acc: 35.801% (14939/41728)\n",
            "Loss: 1.739 | Acc: 35.804% (14963/41792)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.739 | Acc: 35.808% (14988/41856)\n",
            "Loss: 1.739 | Acc: 35.809% (15011/41920)\n",
            "Loss: 1.740 | Acc: 35.795% (15028/41984)\n",
            "Loss: 1.739 | Acc: 35.788% (15048/42048)\n",
            "Loss: 1.739 | Acc: 35.805% (15078/42112)\n",
            "Loss: 1.739 | Acc: 35.817% (15106/42176)\n",
            "Loss: 1.739 | Acc: 35.826% (15133/42240)\n",
            "Loss: 1.739 | Acc: 35.807% (15148/42304)\n",
            "Loss: 1.739 | Acc: 35.812% (15173/42368)\n",
            "Loss: 1.739 | Acc: 35.820% (15199/42432)\n",
            "Loss: 1.739 | Acc: 35.827% (15225/42496)\n",
            "Loss: 1.739 | Acc: 35.818% (15244/42560)\n",
            "Loss: 1.739 | Acc: 35.818% (15267/42624)\n",
            "Loss: 1.739 | Acc: 35.816% (15289/42688)\n",
            "Loss: 1.739 | Acc: 35.821% (15314/42752)\n",
            "Loss: 1.740 | Acc: 35.809% (15332/42816)\n",
            "Loss: 1.740 | Acc: 35.795% (15349/42880)\n",
            "Loss: 1.740 | Acc: 35.800% (15374/42944)\n",
            "Loss: 1.740 | Acc: 35.814% (15403/43008)\n",
            "Loss: 1.740 | Acc: 35.814% (15426/43072)\n",
            "Loss: 1.739 | Acc: 35.835% (15458/43136)\n",
            "Loss: 1.739 | Acc: 35.833% (15480/43200)\n",
            "Loss: 1.739 | Acc: 35.843% (15507/43264)\n",
            "Loss: 1.739 | Acc: 35.847% (15532/43328)\n",
            "Loss: 1.739 | Acc: 35.843% (15553/43392)\n",
            "Loss: 1.739 | Acc: 35.855% (15581/43456)\n",
            "Loss: 1.739 | Acc: 35.869% (15610/43520)\n",
            "Loss: 1.739 | Acc: 35.859% (15629/43584)\n",
            "Loss: 1.739 | Acc: 35.860% (15652/43648)\n",
            "Loss: 1.739 | Acc: 35.885% (15686/43712)\n",
            "Loss: 1.739 | Acc: 35.874% (15704/43776)\n",
            "Loss: 1.739 | Acc: 35.887% (15733/43840)\n",
            "Loss: 1.739 | Acc: 35.899% (15761/43904)\n",
            "Loss: 1.739 | Acc: 35.910% (15789/43968)\n",
            "Loss: 1.739 | Acc: 35.910% (15812/44032)\n",
            "Loss: 1.739 | Acc: 35.917% (15838/44096)\n",
            "Loss: 1.739 | Acc: 35.910% (15858/44160)\n",
            "Loss: 1.739 | Acc: 35.922% (15886/44224)\n",
            "Loss: 1.739 | Acc: 35.924% (15910/44288)\n",
            "Loss: 1.739 | Acc: 35.928% (15935/44352)\n",
            "Loss: 1.739 | Acc: 35.935% (15961/44416)\n",
            "Loss: 1.739 | Acc: 35.940% (15986/44480)\n",
            "Loss: 1.739 | Acc: 35.938% (16008/44544)\n",
            "Loss: 1.739 | Acc: 35.931% (16028/44608)\n",
            "Loss: 1.739 | Acc: 35.940% (16055/44672)\n",
            "Loss: 1.738 | Acc: 35.940% (16078/44736)\n",
            "Loss: 1.738 | Acc: 35.942% (16102/44800)\n",
            "Loss: 1.738 | Acc: 35.953% (16130/44864)\n",
            "Loss: 1.738 | Acc: 35.944% (16149/44928)\n",
            "Loss: 1.738 | Acc: 35.942% (16171/44992)\n",
            "Loss: 1.738 | Acc: 35.933% (16190/45056)\n",
            "Loss: 1.738 | Acc: 35.922% (16208/45120)\n",
            "Loss: 1.739 | Acc: 35.909% (16225/45184)\n",
            "Loss: 1.739 | Acc: 35.898% (16243/45248)\n",
            "Loss: 1.739 | Acc: 35.893% (16264/45312)\n",
            "Loss: 1.739 | Acc: 35.891% (16286/45376)\n",
            "Loss: 1.738 | Acc: 35.878% (16303/45440)\n",
            "Loss: 1.738 | Acc: 35.878% (16326/45504)\n",
            "Loss: 1.739 | Acc: 35.889% (16354/45568)\n",
            "Loss: 1.738 | Acc: 35.905% (16384/45632)\n",
            "Loss: 1.738 | Acc: 35.900% (16405/45696)\n",
            "Loss: 1.738 | Acc: 35.894% (16425/45760)\n",
            "Loss: 1.738 | Acc: 35.892% (16447/45824)\n",
            "Loss: 1.738 | Acc: 35.896% (16472/45888)\n",
            "Loss: 1.738 | Acc: 35.896% (16495/45952)\n",
            "Loss: 1.738 | Acc: 35.907% (16523/46016)\n",
            "Loss: 1.738 | Acc: 35.911% (16548/46080)\n",
            "Loss: 1.738 | Acc: 35.916% (16573/46144)\n",
            "Loss: 1.738 | Acc: 35.912% (16594/46208)\n",
            "Loss: 1.738 | Acc: 35.920% (16621/46272)\n",
            "Loss: 1.738 | Acc: 35.918% (16643/46336)\n",
            "Loss: 1.738 | Acc: 35.909% (16662/46400)\n",
            "Loss: 1.738 | Acc: 35.916% (16688/46464)\n",
            "Loss: 1.738 | Acc: 35.918% (16712/46528)\n",
            "Loss: 1.738 | Acc: 35.931% (16741/46592)\n",
            "Loss: 1.737 | Acc: 35.942% (16769/46656)\n",
            "Loss: 1.737 | Acc: 35.946% (16794/46720)\n",
            "Loss: 1.737 | Acc: 35.946% (16817/46784)\n",
            "Loss: 1.737 | Acc: 35.955% (16844/46848)\n",
            "Loss: 1.738 | Acc: 35.950% (16865/46912)\n",
            "Loss: 1.738 | Acc: 35.942% (16884/46976)\n",
            "Loss: 1.737 | Acc: 35.940% (16906/47040)\n",
            "Loss: 1.738 | Acc: 35.931% (16925/47104)\n",
            "Loss: 1.738 | Acc: 35.916% (16941/47168)\n",
            "Loss: 1.738 | Acc: 35.916% (16964/47232)\n",
            "Loss: 1.738 | Acc: 35.910% (16984/47296)\n",
            "Loss: 1.738 | Acc: 35.904% (17004/47360)\n",
            "Loss: 1.738 | Acc: 35.891% (17021/47424)\n",
            "Loss: 1.738 | Acc: 35.889% (17043/47488)\n",
            "Loss: 1.738 | Acc: 35.893% (17068/47552)\n",
            "Loss: 1.738 | Acc: 35.893% (17091/47616)\n",
            "Loss: 1.738 | Acc: 35.889% (17112/47680)\n",
            "Loss: 1.738 | Acc: 35.894% (17137/47744)\n",
            "Loss: 1.738 | Acc: 35.896% (17161/47808)\n",
            "Loss: 1.738 | Acc: 35.898% (17185/47872)\n",
            "Loss: 1.738 | Acc: 35.902% (17210/47936)\n",
            "Loss: 1.738 | Acc: 35.898% (17231/48000)\n",
            "Loss: 1.738 | Acc: 35.902% (17256/48064)\n",
            "Loss: 1.738 | Acc: 35.917% (17286/48128)\n",
            "Loss: 1.738 | Acc: 35.917% (17309/48192)\n",
            "Loss: 1.738 | Acc: 35.908% (17328/48256)\n",
            "Loss: 1.738 | Acc: 35.911% (17352/48320)\n",
            "Loss: 1.738 | Acc: 35.909% (17374/48384)\n",
            "Loss: 1.739 | Acc: 35.904% (17395/48448)\n",
            "Loss: 1.738 | Acc: 35.923% (17427/48512)\n",
            "Loss: 1.738 | Acc: 35.921% (17449/48576)\n",
            "Loss: 1.738 | Acc: 35.929% (17476/48640)\n",
            "Loss: 1.739 | Acc: 35.913% (17491/48704)\n",
            "Loss: 1.739 | Acc: 35.896% (17506/48768)\n",
            "Loss: 1.739 | Acc: 35.901% (17531/48832)\n",
            "Loss: 1.739 | Acc: 35.888% (17548/48896)\n",
            "Loss: 1.740 | Acc: 35.884% (17569/48960)\n",
            "Loss: 1.740 | Acc: 35.871% (17577/49000)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 35.871428571428574\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.835 | Acc: 37.500% (24/64)\n",
            "Loss: 1.771 | Acc: 36.719% (47/128)\n",
            "Loss: 1.760 | Acc: 37.500% (72/192)\n",
            "Loss: 1.794 | Acc: 37.500% (96/256)\n",
            "Loss: 1.762 | Acc: 37.500% (120/320)\n",
            "Loss: 1.765 | Acc: 37.760% (145/384)\n",
            "Loss: 1.782 | Acc: 36.607% (164/448)\n",
            "Loss: 1.779 | Acc: 37.109% (190/512)\n",
            "Loss: 1.775 | Acc: 37.847% (218/576)\n",
            "Loss: 1.768 | Acc: 38.281% (245/640)\n",
            "Loss: 1.785 | Acc: 37.216% (262/704)\n",
            "Loss: 1.787 | Acc: 37.500% (288/768)\n",
            "Loss: 1.789 | Acc: 37.380% (311/832)\n",
            "Loss: 1.788 | Acc: 37.500% (336/896)\n",
            "Loss: 1.779 | Acc: 38.021% (365/960)\n",
            "Loss: 1.779 | Acc: 37.695% (386/1024)\n",
            "Loss: 1.778 | Acc: 38.235% (416/1088)\n",
            "Loss: 1.774 | Acc: 37.934% (437/1152)\n",
            "Loss: 1.774 | Acc: 37.911% (461/1216)\n",
            "Loss: 1.777 | Acc: 37.578% (481/1280)\n",
            "Loss: 1.781 | Acc: 37.128% (499/1344)\n",
            "Loss: 1.779 | Acc: 37.074% (522/1408)\n",
            "Loss: 1.782 | Acc: 36.481% (537/1472)\n",
            "Loss: 1.783 | Acc: 36.068% (554/1536)\n",
            "Loss: 1.780 | Acc: 36.312% (581/1600)\n",
            "Loss: 1.780 | Acc: 35.877% (597/1664)\n",
            "Loss: 1.775 | Acc: 36.169% (625/1728)\n",
            "Loss: 1.768 | Acc: 36.328% (651/1792)\n",
            "Loss: 1.774 | Acc: 36.153% (671/1856)\n",
            "Loss: 1.773 | Acc: 36.250% (696/1920)\n",
            "Loss: 1.771 | Acc: 36.290% (720/1984)\n",
            "Loss: 1.773 | Acc: 36.426% (746/2048)\n",
            "Loss: 1.770 | Acc: 36.553% (772/2112)\n",
            "Loss: 1.774 | Acc: 36.351% (791/2176)\n",
            "Loss: 1.775 | Acc: 36.250% (812/2240)\n",
            "Loss: 1.773 | Acc: 36.502% (841/2304)\n",
            "Loss: 1.775 | Acc: 36.191% (857/2368)\n",
            "Loss: 1.774 | Acc: 36.513% (888/2432)\n",
            "Loss: 1.774 | Acc: 36.458% (910/2496)\n",
            "Loss: 1.780 | Acc: 36.250% (928/2560)\n",
            "Loss: 1.780 | Acc: 36.090% (947/2624)\n",
            "Loss: 1.782 | Acc: 36.086% (970/2688)\n",
            "Loss: 1.781 | Acc: 36.083% (993/2752)\n",
            "Loss: 1.781 | Acc: 35.902% (1011/2816)\n",
            "Loss: 1.779 | Acc: 36.007% (1037/2880)\n",
            "Loss: 1.782 | Acc: 35.768% (1053/2944)\n",
            "Loss: 1.783 | Acc: 35.638% (1072/3008)\n",
            "Loss: 1.780 | Acc: 35.579% (1093/3072)\n",
            "Loss: 1.781 | Acc: 35.555% (1115/3136)\n",
            "Loss: 1.781 | Acc: 35.656% (1141/3200)\n",
            "Loss: 1.781 | Acc: 35.631% (1163/3264)\n",
            "Loss: 1.781 | Acc: 35.607% (1185/3328)\n",
            "Loss: 1.780 | Acc: 35.731% (1212/3392)\n",
            "Loss: 1.782 | Acc: 35.764% (1236/3456)\n",
            "Loss: 1.783 | Acc: 35.653% (1255/3520)\n",
            "Loss: 1.782 | Acc: 35.714% (1280/3584)\n",
            "Loss: 1.782 | Acc: 35.526% (1296/3648)\n",
            "Loss: 1.781 | Acc: 35.533% (1319/3712)\n",
            "Loss: 1.782 | Acc: 35.408% (1337/3776)\n",
            "Loss: 1.782 | Acc: 35.417% (1360/3840)\n",
            "Loss: 1.782 | Acc: 35.400% (1382/3904)\n",
            "Loss: 1.783 | Acc: 35.408% (1405/3968)\n",
            "Loss: 1.785 | Acc: 35.342% (1425/4032)\n",
            "Loss: 1.788 | Acc: 35.156% (1440/4096)\n",
            "Loss: 1.790 | Acc: 35.000% (1456/4160)\n",
            "Loss: 1.789 | Acc: 35.085% (1482/4224)\n",
            "Loss: 1.788 | Acc: 35.191% (1509/4288)\n",
            "Loss: 1.788 | Acc: 35.179% (1531/4352)\n",
            "Loss: 1.786 | Acc: 35.236% (1556/4416)\n",
            "Loss: 1.785 | Acc: 35.268% (1580/4480)\n",
            "Loss: 1.785 | Acc: 35.145% (1597/4544)\n",
            "Loss: 1.786 | Acc: 35.048% (1615/4608)\n",
            "Loss: 1.787 | Acc: 35.081% (1639/4672)\n",
            "Loss: 1.786 | Acc: 35.093% (1662/4736)\n",
            "Loss: 1.785 | Acc: 35.146% (1687/4800)\n",
            "Loss: 1.784 | Acc: 35.115% (1708/4864)\n",
            "Loss: 1.782 | Acc: 35.187% (1734/4928)\n",
            "Loss: 1.782 | Acc: 35.216% (1758/4992)\n",
            "Loss: 1.783 | Acc: 35.146% (1777/5056)\n",
            "Loss: 1.784 | Acc: 35.137% (1799/5120)\n",
            "Loss: 1.783 | Acc: 35.127% (1821/5184)\n",
            "Loss: 1.783 | Acc: 35.099% (1842/5248)\n",
            "Loss: 1.782 | Acc: 35.147% (1867/5312)\n",
            "Loss: 1.782 | Acc: 35.138% (1889/5376)\n",
            "Loss: 1.782 | Acc: 35.074% (1908/5440)\n",
            "Loss: 1.783 | Acc: 35.102% (1932/5504)\n",
            "Loss: 1.783 | Acc: 35.075% (1953/5568)\n",
            "Loss: 1.782 | Acc: 35.156% (1980/5632)\n",
            "Loss: 1.782 | Acc: 35.165% (2003/5696)\n",
            "Loss: 1.781 | Acc: 35.174% (2026/5760)\n",
            "Loss: 1.781 | Acc: 35.234% (2052/5824)\n",
            "Loss: 1.781 | Acc: 35.173% (2071/5888)\n",
            "Loss: 1.783 | Acc: 35.064% (2087/5952)\n",
            "Loss: 1.782 | Acc: 35.057% (2109/6016)\n",
            "Loss: 1.782 | Acc: 35.082% (2133/6080)\n",
            "Loss: 1.783 | Acc: 35.091% (2156/6144)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.785 | Acc: 35.019% (2174/6208)\n",
            "Loss: 1.785 | Acc: 35.029% (2197/6272)\n",
            "Loss: 1.786 | Acc: 35.006% (2218/6336)\n",
            "Loss: 1.786 | Acc: 35.031% (2242/6400)\n",
            "Loss: 1.787 | Acc: 34.901% (2256/6464)\n",
            "Loss: 1.787 | Acc: 35.003% (2285/6528)\n",
            "Loss: 1.788 | Acc: 34.982% (2306/6592)\n",
            "Loss: 1.788 | Acc: 35.006% (2330/6656)\n",
            "Loss: 1.787 | Acc: 35.000% (2352/6720)\n",
            "Loss: 1.787 | Acc: 34.950% (2371/6784)\n",
            "Loss: 1.786 | Acc: 34.988% (2396/6848)\n",
            "Loss: 1.788 | Acc: 34.910% (2413/6912)\n",
            "Loss: 1.788 | Acc: 34.948% (2438/6976)\n",
            "Loss: 1.789 | Acc: 34.858% (2454/7040)\n",
            "Loss: 1.790 | Acc: 34.783% (2471/7104)\n",
            "Loss: 1.788 | Acc: 34.863% (2499/7168)\n",
            "Loss: 1.790 | Acc: 34.776% (2515/7232)\n",
            "Loss: 1.789 | Acc: 34.745% (2535/7296)\n",
            "Loss: 1.788 | Acc: 34.823% (2563/7360)\n",
            "Loss: 1.787 | Acc: 34.860% (2588/7424)\n",
            "Loss: 1.785 | Acc: 34.976% (2619/7488)\n",
            "Loss: 1.785 | Acc: 34.958% (2640/7552)\n",
            "Loss: 1.787 | Acc: 34.887% (2657/7616)\n",
            "Loss: 1.786 | Acc: 34.883% (2679/7680)\n",
            "Loss: 1.785 | Acc: 34.956% (2707/7744)\n",
            "Loss: 1.784 | Acc: 35.041% (2736/7808)\n",
            "Loss: 1.784 | Acc: 35.023% (2757/7872)\n",
            "Loss: 1.784 | Acc: 35.043% (2781/7936)\n",
            "Loss: 1.786 | Acc: 34.962% (2797/8000)\n",
            "Loss: 1.786 | Acc: 34.945% (2818/8064)\n",
            "Loss: 1.786 | Acc: 34.953% (2841/8128)\n",
            "Loss: 1.786 | Acc: 34.961% (2864/8192)\n",
            "Loss: 1.786 | Acc: 34.969% (2887/8256)\n",
            "Loss: 1.788 | Acc: 34.892% (2903/8320)\n",
            "Loss: 1.789 | Acc: 34.864% (2923/8384)\n",
            "Loss: 1.788 | Acc: 34.896% (2948/8448)\n",
            "Loss: 1.789 | Acc: 34.833% (2965/8512)\n",
            "Loss: 1.790 | Acc: 34.876% (2991/8576)\n",
            "Loss: 1.790 | Acc: 34.850% (3011/8640)\n",
            "Loss: 1.790 | Acc: 34.812% (3030/8704)\n",
            "Loss: 1.790 | Acc: 34.820% (3053/8768)\n",
            "Loss: 1.789 | Acc: 34.862% (3079/8832)\n",
            "Loss: 1.789 | Acc: 34.892% (3104/8896)\n",
            "Loss: 1.789 | Acc: 34.900% (3127/8960)\n",
            "Loss: 1.789 | Acc: 34.885% (3148/9024)\n",
            "Loss: 1.790 | Acc: 34.870% (3169/9088)\n",
            "Loss: 1.790 | Acc: 34.910% (3195/9152)\n",
            "Loss: 1.789 | Acc: 34.896% (3216/9216)\n",
            "Loss: 1.789 | Acc: 34.903% (3239/9280)\n",
            "Loss: 1.790 | Acc: 34.921% (3263/9344)\n",
            "Loss: 1.790 | Acc: 34.896% (3283/9408)\n",
            "Loss: 1.791 | Acc: 34.882% (3304/9472)\n",
            "Loss: 1.790 | Acc: 34.931% (3331/9536)\n",
            "Loss: 1.789 | Acc: 34.969% (3357/9600)\n",
            "Loss: 1.789 | Acc: 35.027% (3385/9664)\n",
            "Loss: 1.790 | Acc: 34.992% (3404/9728)\n",
            "Loss: 1.791 | Acc: 34.906% (3418/9792)\n",
            "Loss: 1.791 | Acc: 34.903% (3440/9856)\n",
            "Loss: 1.791 | Acc: 34.859% (3458/9920)\n",
            "Loss: 1.791 | Acc: 34.846% (3479/9984)\n",
            "Loss: 1.791 | Acc: 34.810% (3481/10000)\n",
            "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 34.81\n",
            "\n",
            "Final train set accuracy is 35.871428571428574\n",
            "Final test set accuracy is 34.81\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "regularization_val = 1e-6\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims=10\n",
        "num_trans_layers = 4\n",
        "num_heads=4\n",
        "image_k=32\n",
        "patch_k=4\n",
        "\n",
        "network = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "network = ViT(hidden_dims=hidden_dims,\n",
        "            input_dims=input_dims,\n",
        "            output_dims=output_dims,\n",
        "            num_trans_layers=num_trans_layers,\n",
        "            num_heads=num_heads,\n",
        "            image_k=image_k,\n",
        "            patch_k=patch_k,\n",
        "            bias=False).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0)\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE\n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "test_accs=[]\n",
        "for epoch in range(3):\n",
        "    tr_acc = train(network, optimizer, loader_train)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))\n",
        "\n",
        "    test_acc = evaluate(network, loader_test)\n",
        "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
        "              .format(epoch, test_acc))\n",
        "\n",
        "    tr_accs.append(tr_acc)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
      ],
      "id": "yY2Rz9JJCZDw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EeEctXdCZDw"
      },
      "outputs": [],
      "source": [],
      "id": "5EeEctXdCZDw"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}